


<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
    <responseDate>2025-09-09T13:47:24Z</responseDate>
    <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:12405550" metadataPrefix="pmc">https://pmc.ncbi.nlm.nih.gov/api/oai/v1/mh/</request>
    
    <GetRecord>
        <record>
    <header>
    <identifier>oai:pubmedcentral.nih.gov:12405550</identifier>
    <datestamp>2025-09-04</datestamp>
    
        
        <setSpec>scirep</setSpec>
        
    
        
        <setSpec>pmc-open</setSpec>
        
    
</header>
    <metadata>
        
        <article xmlns="https://jats.nlm.nih.gov/ns/archiving/1.4/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.4/ https://jats.nlm.nih.gov/archiving/1.4/xsd/JATS-archivearticle1-4.xsd" article-type="research-article" xml:lang="en" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-id journal-id-type="pmc-domain-id">1579</journal-id><journal-id journal-id-type="pmc-domain">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12405550</article-id><article-id pub-id-type="pmcid-ver">PMC12405550.1</article-id><article-id pub-id-type="pmcaid">12405550</article-id><article-id pub-id-type="pmcaiid">12405550</article-id><article-id pub-id-type="pmid">40897780</article-id><article-id pub-id-type="doi">10.1038/s41598-025-17188-7</article-id><article-id pub-id-type="publisher-id">17188</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Evaluating the ability of large Language models to predict human social decisions</article-title></title-group><contrib-group><contrib contrib-type="author"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0002-5704-6427</contrib-id><name name-style="western"><surname>XIAO</surname><given-names initials="F">Feng</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" corresp="yes"><contrib-id contrib-id-type="orcid" authenticated="false">http://orcid.org/0000-0003-1706-2446</contrib-id><name name-style="western"><surname>WANG</surname><given-names initials="XTX">XT XiaoTian</given-names></name><address><email>xtwang@cuhk.edu.cn</email></address><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/00t33hh48</institution-id><institution-id institution-id-type="GRID">grid.10784.3a</institution-id><institution-id institution-id-type="ISNI">0000 0004 1937 0482</institution-id><institution>Department of Applied Psychology, School of Humanities and Social Science, </institution><institution>The Chinese University of Hong Kong (Shenzhen), </institution></institution-wrap>2001 Longxiang Boulevard, 518172 Shenzhen, China </aff></contrib-group><pub-date pub-type="epub"><day>2</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type="pmc-issue-id">478255</issue-id><elocation-id>32290</elocation-id><history><date date-type="received"><day>11</day><month>3</month><year>2025</year></date><date date-type="accepted"><day>21</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-04 00:25:59.930"><day>04</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri content-type="pmc-pdf" xlink:href="41598_2025_Article_17188.pdf"/><abstract id="Abs1"><p id="Par8">Recent advances in large language models (LLMs) have highlighted their potential to predict human decisions. In two studies, we compared predictions by GPT-3.5 and GPT-4 across 51 scenarios (9,600 responses) against published data from 2,104 human participants within an evolutionary-psychology framework. We further examined our findings with GPT-4o across eight social-group and kinship conditions (1,600 responses). Our results revealed behavioral differences between humans and LLMs&#8217; predictions: Humans showed a greater sensitivity to kinship and group size than the LLMs when making life-death decisions. LLMs align closer with humans with a higher risk-seeking preference in financial domains. While human choices followed Prospect theory&#8217;s value function (risk-averse in gains, risk-seeking in losses), LLMs often predicted reversed patterns. GPT-3.5 matched the average level of human risk preference but showed reversed framing effects; GPT-4 was indiscriminately risk-averse across social contexts. While humans were more risk-seeking in small or kin groups than in large groups, GPT-4o made the opposite predictions. Our results suggest a set of criteria for a psychological version of the Turing Test reflected in framing effects and social context-dependent risk preference involving kinship, group size, social relations, sense of fairness, self-age awareness, public vs. personal properties, and social group-dependent aspiration levels.</p><sec><title>Supplementary Information</title><p>The online version contains supplementary material available at 10.1038/s41598-025-17188-7.</p></sec></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Generative AI</kwd><kwd>Social decision-making</kwd><kwd>Framing effects</kwd><kwd>Group size</kwd><kwd>Kinship</kwd><kwd>Group-dependent aspiration levels</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Evolution</kwd><kwd>Psychology</kwd></kwd-group><funding-group><award-group><funding-source><institution>the National Natural Science Foundation of Shenzhen, China</institution></funding-source><award-id>JCYJ20220530143803009</award-id><principal-award-recipient><name name-style="western"><surname>WANG</surname><given-names>XT XiaoTian</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>NSFC 31971025</award-id><principal-award-recipient><name name-style="western"><surname>WANG</surname><given-names>XT XiaoTian</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par9">Within a framework of evolutionary psychology, we address two fundamental questions: How do humans interact with their environments and others? What are some unique features of human social decision rationality, compared to other species? We evaluated large language models (LLMs) as another &#8220;species&#8221; against human benchmarks across 51 social scenarios. In particular, we evaluated the abilities of three LLMs (GPT-3.5, GPT-4, and GPT-4o) to predict human preference and choices.</p><p id="Par10">The rise of AI, particularly the advent of LLMs, signifies a new genesis of digital intellect. Can they pass diverse renditions of the Turing Test, ranging from the original thought experiment named imagination game to economic logic, theory of mind, and socially adaptive reasoning grounded in evolutionary principles? Social decision-making has evolved as an adaptation to solve recurrent and typical problems in human interaction and communication<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref></sup>. It becomes imminent and necessary to compare AI rationality with human rationality in social and moral decision-making.</p><p id="Par11">Several recent studies have begun examining LLMs&#8217; forecasting potential in social domains. For example, researchers used GPT-4 to predict human allocation decisions in dictator games, finding qualitative alignment with human patterns but systematic biases in the size of the offers<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. These findings highlight the promise of LLMs as a predictive tool while also revealing the need for broader, systematic comparisons across social contexts.</p><p id="Par12">To situate these forecasting assessments for AI, we draw on Herbert Simon&#8217;s concept of bounded rationality, emphasizing the role of cognitive limitations and task structure in shaping human choices<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Unlike an omniscient agent, human administrators operate with intuition and satisfice, settling for &#8220;good enough&#8221; rather than optimal decisions. From an evolutionary perspective, human rationality is also socially bound to reach the goals of survival and reproduction as measured by inclusive fitness<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>sensitive to social group structures, dynamics, and genetic relations<sup><xref ref-type="bibr" rid="CR1">1</xref>,<xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>.</p><p id="Par13">Moreover, linguistic cues and framing critically influence human social decisions. Recent meta-analytic evidence shows that subtle wording differences in one-shot games significantly alter human choices<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Verbal framing in risk communication, whether cast as gains or losses, reshapes perceived utilities and steers decision-making. Humans make decisions with cognitive heuristics, using minimal information and adapting our search strategies to the structure of the social environment<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. By contrast, although LLMs showcased remarkable potential in mimicking behavior and decision-making capabilities<sup><xref ref-type="bibr" rid="CR8">8</xref>&#8211;<xref ref-type="bibr" rid="CR11">11</xref></sup>, they do not engage in &#8220;reasoning&#8221; akin to humans<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. In reaction to the rapid development of LLMs, some scholars argue that language, as a tool optimized for communication, only reflects rather than gives rise to sophisticated human cognition<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>.</p><p id="Par14">Recent work has examined how well LLMs forecast specific human choices. LLM&#8217;s predictions in one-shot cooperation games had higher cooperation rates and equal resource splits than actual human behavior<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup>. In negotiation scenarios, the forecasts by GPT-4 changed in offers and rejection rates when prompted to prioritize fairness or selfishness<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>. However, without guided reasoning steps, AI&#8217;s coordination forecasts fall short<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>. In obedience and language comprehension tasks, LLMs overestimated human performance<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. In the moral judgment domain, GPT-3.5&#8217;s forecasts aligned well with human choices<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>,&#160;though they tended to be more rule-based, generous, and less sensitive to risk framing<sup><xref ref-type="bibr" rid="CR20">20</xref></sup>. Compared with simple algorithmic models, LLM forecasts diverged over repeated interactions<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. In trust-based resource allocations, GPT-4 predicted trust and distrust decisions effectively<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. Beyond social choices, fine-tuned LLMs accurately forecasted dietary intervention outcomes<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. Moreover, in the financial domain, arithmetic-trained models outperformed classical cognitive and general-purpose LLMs in predicting delay discounting and risk preferences<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>.</p><p id="Par15">Although current LLMs offer promise for decision support, their ability to forecast human social decision-making by integrating linguistic framing and social contextual cues remains underexplored. To bridge this gap, we ran two studies evaluating GPT-3.5 and GPT-4&#8217;s predictions against human decisions in 51 diverse socially and evolutionarily meaningful scenarios. We benchmarked their forecasts against data from 2,104 human participants collected in our previous empirical work<sup><xref ref-type="bibr" rid="CR25">25</xref>&#8211;<xref ref-type="bibr" rid="CR30">30</xref></sup>. We aim to determine whether general-purpose LLMs can accurately simulate human risk preference in different social contexts. To improve AI&#8217;s alignment with human decisions, we examine LLMs&#8217; social and linguistic sensitivity and identify social constructs, such as kinship, group size, age of decision recipients, and sense of fairness, with which LLMs may still be ignorant and need to improve in future AI model development.</p><p id="Par16">In Study 1, we conducted 36 independent forecasting tests per AI chatbot across five social contexts, encompassing human lives, kinship structure of social groups, human population versus the extraterrestrial (ET) population, public properties, and personal stock shares. We presented each scenario with positive and negative framing, yielding 18 framing pairs.</p><p id="Par17">From an evolutionary perspective and based on previous work<sup><xref ref-type="bibr" rid="CR25">25</xref>&#8211;<xref ref-type="bibr" rid="CR29">29</xref>,<xref ref-type="bibr" rid="CR31">31</xref></sup>, we argue that verbal framing is like the tone of voice and serves as a secondary communication cue. These secondary cues affect risk preference only when evolutionarily valid social cues such as kinship or group size are absent or in conflict<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>.</p><p id="Par18">Our previous human data<sup><xref ref-type="bibr" rid="CR25">25</xref>&#8211;<xref ref-type="bibr" rid="CR27">27</xref></sup> have shown that framing effects were evident in evolutionarily novel large-group contexts with either 600, 6000, or even 6&#160;billion anonymous lives at stake. Participants preferred the sure option of saving one-third of the group when framing the expected outcomes in terms of the number of lives saved and reversed the preference in favor of the gamble option when framing the same expected outcome in terms of mortality rate (e.g., &#8220;one-third of chance no one will die and two-thirds of chance all of them will die&#8221;). This irrational reversal in risk preference disappeared when the life-death problems were presented in kinship or small groups within the evolutionarily typical range of human groups (i.e., smaller than a two-digit number).</p><p id="Par19">Based on the above analysis, we generated five hypotheses for Study 1:</p><p id="Par20">1) Human participants exhibited a higher risk preference for the gamble option in small (six people) and particularly kinship (six kin) groups, where they had a higher minimum requirement for survival rate<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Saving one-third of the group failed to reach this decision reference point. We hypothesize that LLMs may be sensitive to the framing of expected life-death outcomes but would be less likely to predict human-like sensitivity to group size (Hypothesis 1).</p><p id="Par21">2) Humans are sensitive to not only group size but also group constituents<sup><xref ref-type="bibr" rid="CR7">7</xref>,<xref ref-type="bibr" rid="CR25">25</xref>&#8211;<xref ref-type="bibr" rid="CR29">29</xref></sup>. In a kin group of six, people were equally risk-taking in both framing conditions. However, when a group is a mixture of five strangers and one kin, the risk-neutral preference for strangers and risk-seeking preference for kin would collide, leading to an ambivalent condition where decision-makers may seek verbal framing cues to help solve the problem<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>, and thus, the framing effect would occur. We predict that this mixed group structure would be too subtle for LLMs to consider (Hypothesis 2).</p><p id="Par22">3) To test an argument regarding group-size-dependent framing effects, we compared the life-death problems involving the human population versus the ET population. We argue that humans become more sensitive to verbal framing in large-group contexts, not due to indifference to anonymous lives but indecisiveness in such evolutionarily novel conditions. As a result, decision-makers would rely more on secondary communicational cues such as verbal framing<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Replacing humans with ETs would retain the large-group context but induce a lower level of caring about ET lives. We predict that framing effects would be evident in the context of the human population at stake but disappear in the context of the ET population. In contrast, AI chatbots would treat human lives and ET lives similarly (Hypothesis 3).</p><p id="Par23">4) The direction of framing effects (risk-averse under positive framing and risk-seeking under negative framing) has proven robust<sup><xref ref-type="bibr" rid="CR32">32</xref></sup> since initially reported by Tversky and Kahneman<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. However, how LLMs react to risky choice framing in life-death and financial problems remains to be tested. We predict that LLMs will be sensitive to verbal framing and show more human-like framing effects in the economic domain (public properties and personal investment) than in the life domain and adjust their risk preference as a function of monetary values, given the assumption that monetary utilities are easier to compute than social utilities (Hypothesis 4).</p><p id="Par24">5) In monetary choices, such as public properties or personal investments, the expected value (EV) plays a more critical role. We predict that LLMs will better align with humans and be more consistent than humans in these scenarios, where utility maximization principles can readily guide risky choices (Hypothesis 5).</p><p id="Par25">In Study 2, we conducted 15 independent forecasting tests per AI chatbot across three decision tasks designed to examine the effects of sense of fairness, age dynamics, and group-dependent aspiration levels. We generated four hypotheses for Study 2:</p><p id="Par26">6) Regarding the sense of fairness, if the sure option of saving two lives out of six were pre-selected, depriving everyone of a fair shot, the gamble option would be more favorable. Such preference for the gamble option is evident in an in-group context but diminishes in a large anonymous group situation<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>. We predict that, unlike humans, LLMs have not yet acquired an in-group-based perception of fairness and thus would favor the fair option equally irrespective of group contexts (Hypothesis 6).</p><p id="Par27">7) With self-age awareness, humans automatically use their age as a reference when making decisions. For young human adults (<italic toggle="yes">M</italic>
<sub>age</sub> = 20.2 years) in a previous study<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, &#8221;older kin&#8221; implied their parents, and &#8220;younger kin&#8221; implied their siblings; both of them have compatible reproductive fitness values. In contrast, for middle-aged participants (<italic toggle="yes">M</italic>
<sub>age</sub> = 41.4 years), &#8220;younger kin&#8221; was more likely to imply their children and &#8220;older kin&#8221; of their parents. The elderly parents, on average, have lower reproductive fitness than teenage children. As a result, the sure option of saving two older kin would be less favorable than the sure option of saving two younger kin compared to the gamble option<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. Due to the lack of self-age awareness, we predict that LLMs would react to the same choice problems independent of the age of their decision recipients. We predict that LLMs will be inclined more towards the &#8220;saving young&#8221; than the &#8220;saving old&#8221; option due to a higher survival potential of young over old (Hypothesis 7).</p><p id="Par28">8) Based on previous studies<sup><xref ref-type="bibr" rid="CR25">25</xref>,<xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>, we estimated that for more than half of the human participants, the minimum requirement for saving kin is higher than two-thirds of the group. In contrast, the minimum requirement for saving strangers in a large group of 600 people is lower than one-third of the group. Humans were willing to choose the gamble option with a lower EV when the sure option failed to reach the goal setting or the minimum requirement for group survival<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. These social group-dependent aspiration levels reflect individuals&#8217; tendency to set higher minimum survival requirements for &#8220;we&#8221; groups, such as kin, than for more distant or anonymous groups. Due to their lack of implicit group-dependent minimum requirement, we predict that LLMs would be more EV-driven and favor the superior (dominant) sure option across the three group situations (Hypothesis 8).</p><p id="Par29">9) Given the group-dependent aspirational levels, changing EV of choice outcomes from saving one-third to two-thirds of the group resulted in different risk preferences<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>. Such an increase in the EV of outcomes would not significantly change human risk preference in a group of six kin since both are below their minimum requirement (higher than two-thirds). For a similar token, increasing the EV of outcomes from one-third to two-thirds would not significantly change risk preference in the large-group context since both are above the minimum requirement setting (lower than one-third). Only in the small group context did the change in EV induce a significant change in risk preference. More human participants switched to the sure option of saving two-thirds of the group since an EV of two-thirds satisfied their minimum requirement (between one-third and two-thirds). In contrast, LLMs are unlikely to have such implicit group-dependent goal settings and will fail to predict group-dependent risk preference (Hypothesis 9).</p></sec><sec id="Sec2"><title>Results</title><p id="Par30">Table&#160;<xref rid="Tab1" ref-type="table">1</xref> summarizes the statistical results of human and LLM choice preference data in different framing conditions and social contexts in Studies 1 and 2. Table&#160;<xref rid="Tab2" ref-type="table">2</xref> summarizes the statistical results of comparisons between humans and LLMs and between the two LLMs in Studies 1 and 2.</p><p id="Par31">
<table-wrap id="Tab1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Statistical summary of choice prediction patterns for humans, GPT-3.5, and GPT-4 in studies 1 and 2.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Study 1: Social contexts</th><th align="left" colspan="1" rowspan="1">Humans</th><th align="left" colspan="1" rowspan="1">GPT-3.5</th><th align="left" colspan="1" rowspan="1">GPT-4</th></tr></thead><tbody><tr><td align="left" colspan="4" rowspan="1">
<bold>Human lives: 6/60/600/6000 lives</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size (Pos):</td><td align="left" colspan="1" rowspan="1">50/40/50/44</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size (Neg):</td><td align="left" colspan="1" rowspan="1">50/40/50/44</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Risk-seeking (Pos vs. Neg):</td><td align="left" colspan="1" rowspan="1">ND/ND/Neg<sup>*</sup>/ND</td><td align="left" colspan="1" rowspan="1">Pos<sup>*</sup>/ND/ND/ND</td><td align="left" colspan="1" rowspan="1">ND/ND/ND/ND</td></tr><tr><td align="left" colspan="4" rowspan="1">
<bold>Kinship constituents: 1/2/3/6 kin</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size (Pos):</td><td align="left" colspan="1" rowspan="1">50/50/50/50</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size (Neg):</td><td align="left" colspan="1" rowspan="1">50/50/50/50</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Risk-seeking (Pos vs. Neg):</td><td align="left" colspan="1" rowspan="1">Neg<sup>*</sup>/Neg<sup>*</sup>/ND/ND</td><td align="left" colspan="1" rowspan="1">Pos<sup>*</sup>/ND/Pos<sup>**</sup>/ND</td><td align="left" colspan="1" rowspan="1">ND/ND/ND/Pos<sup>**</sup></td></tr><tr><td align="left" colspan="4" rowspan="1">
<bold>Human vs. ET populations: 6&#160;billion humans/ETs</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size (Pos):</td><td align="left" colspan="1" rowspan="1">50/50</td><td align="left" colspan="1" rowspan="1">100/100</td><td align="left" colspan="1" rowspan="1">100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size (Neg):</td><td align="left" colspan="1" rowspan="1">50/50</td><td align="left" colspan="1" rowspan="1">100/100</td><td align="left" colspan="1" rowspan="1">100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Risk-seeking (Pos vs. Neg):</td><td align="left" colspan="1" rowspan="1">Neg<sup>*</sup>/ND</td><td align="left" colspan="1" rowspan="1">Neg<sup>*</sup>/ND</td><td align="left" colspan="1" rowspan="1">ND/ND</td></tr><tr><td align="left" colspan="4" rowspan="1">
<bold>Public properties: 6/60/600/6000 paintings</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size (Pos):</td><td align="left" colspan="1" rowspan="1">45/45/39/38</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size (Neg):</td><td align="left" colspan="1" rowspan="1">41/40/41/38</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Risk-seeking (Pos vs. Neg):</td><td align="left" colspan="1" rowspan="1">ND/Neg<sup>*</sup>/ND/ND</td><td align="left" colspan="1" rowspan="1">Pos<sup>***</sup>/Pos<sup>***</sup>/Neg<sup>***</sup>/ND</td><td align="left" colspan="1" rowspan="1">ND/Neg<sup>*</sup>/ND/Neg<sup>**</sup></td></tr><tr><td align="left" colspan="4" rowspan="1">
<bold>Personal stock shares: 6/60/600/6000 dollars</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size (Pos):</td><td align="left" colspan="1" rowspan="1">31/35/32/36</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size (Neg):</td><td align="left" colspan="1" rowspan="1">31/33/31/30</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td><td align="left" colspan="1" rowspan="1">100/100/100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Risk-seeking (Pos vs. Neg):</td><td align="left" colspan="1" rowspan="1">ND/ND/ND/Neg<sup>*</sup></td><td align="left" colspan="1" rowspan="1">Pos<sup>***</sup>/ND/ND/Neg<sup>***</sup></td><td align="left" colspan="1" rowspan="1">Neg<sup>**</sup>/Neg<sup>*</sup>/Neg<sup>*</sup>/ND</td></tr><tr><td align="left" colspan="1" rowspan="1">
<bold>Study 2: Decision premises</bold>
</td><td align="left" colspan="1" rowspan="1">
<bold>Humans</bold>
</td><td align="left" colspan="1" rowspan="1">
<bold>GPT-3.5</bold>
</td><td align="left" colspan="1" rowspan="1">
<bold>GPT-4</bold>
</td></tr><tr><td align="left" colspan="4" rowspan="1">
<bold>Sense of fairness: 6/600 lives</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size:</td><td align="left" colspan="1" rowspan="1">40/40</td><td align="left" colspan="1" rowspan="1">100/100</td><td align="left" colspan="1" rowspan="1">100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Choice (Fair-shot vs. Selective)</td><td align="left" colspan="1" rowspan="1">Fair-shot<sup>***</sup>/ND</td><td align="left" colspan="1" rowspan="1">Selective<sup>***</sup>/Selective<sup>*</sup></td><td align="left" colspan="1" rowspan="1">Fair-shot<sup>***</sup>/Fair-shot<sup>***</sup></td></tr><tr><td align="left" colspan="4" rowspan="1">
<bold>Self-age awareness: YSO-6 kin/YSY-6 kin</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size:</td><td align="left" colspan="1" rowspan="1">40/42</td><td align="left" colspan="1" rowspan="1">100/100</td><td align="left" colspan="1" rowspan="1">100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Choice (Sure-thing vs. Gamble):</td><td align="left" colspan="1" rowspan="1">Gamble<sup>***</sup>/Gamble<sup>***</sup></td><td align="left" colspan="1" rowspan="1">Gamble<sup>***</sup>/ND</td><td align="left" colspan="1" rowspan="1">Gamble<sup>***</sup>/ND</td></tr><tr><td align="left" colspan="4" rowspan="1">
<bold>Self-age awareness: MSO-6 kin/MSY-6 kin</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size:</td><td align="left" colspan="1" rowspan="1">30/36</td><td align="left" colspan="1" rowspan="1">100/100</td><td align="left" colspan="1" rowspan="1">100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Choice (Sure-thing vs. Gamble):</td><td align="left" colspan="1" rowspan="1">Gamble<sup>***</sup>/ND</td><td align="left" colspan="1" rowspan="1">ND/ND</td><td align="left" colspan="1" rowspan="1">Sure-thing<sup>***</sup>/Sure-thing<sup>***</sup></td></tr><tr><td align="left" colspan="4" rowspan="1">
<bold>Group-dependent goal settings with unequal EVs: 6 kin/6 lives/600 lives</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size:</td><td align="left" colspan="1" rowspan="1">40/40/40</td><td align="left" colspan="1" rowspan="1">100/100/100</td><td align="left" colspan="1" rowspan="1">100/100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Choice (Sure-thing vs. Gamble):</td><td align="left" colspan="1" rowspan="1">ND/Sure-thing <sup>***</sup>/Sure-thing <sup>***</sup></td><td align="left" colspan="1" rowspan="1">Gamble<sup>**</sup>/Gamble<sup>***</sup>/ND</td><td align="left" colspan="1" rowspan="1">Sure-thing <sup>***</sup>/Sure-thing <sup>***</sup>/Sure-thing <sup>***</sup></td></tr><tr><td align="left" colspan="4" rowspan="1">
<bold>Group-dependent goal settings with equal EVs: 6 kin/6 lives/600 lives</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size (One-third):</td><td align="left" colspan="1" rowspan="1">44/30/31</td><td align="left" colspan="1" rowspan="1">100/100/100</td><td align="left" colspan="1" rowspan="1">100/100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Sample size (Two-thirds):</td><td align="left" colspan="1" rowspan="1">31/32/34</td><td align="left" colspan="1" rowspan="1">100/100/100</td><td align="left" colspan="1" rowspan="1">100/100/100</td></tr><tr><td align="left" colspan="1" rowspan="1">Risk-seeking (One-third vs. Two-third)</td><td align="left" colspan="1" rowspan="1">ND/ND/ND</td><td align="left" colspan="1" rowspan="1">Two-thirds<sup>**</sup>/Two-thirds<sup>***</sup>/Two-thirds<sup>***</sup></td><td align="left" colspan="1" rowspan="1">One-third<sup>***</sup>/Two-thirds<sup>***</sup>/ND</td></tr></tbody></table><table-wrap-foot><p>Note. Pos, Positive framing; Neg, Negative framing; ND, No differences; Gamble, More risk-seeking; Sure-thing, More risk-averse; Fair-shot, Favor the fair option; Selective, Favor the unfair option; One-third, More risk-seeking in the lower EV scenario; Two-thirds, More risk-seeking in the higher EV scenario; <sup>*</sup>, <italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.050; <sup>**</sup>, <italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.010; <sup>***</sup>, <italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.001. We summarize the choice preference patterns for each agent, including humans, GPT-3.5, and GPT-4, across 51 decision scenarios varied in different social contexts and framing conditions (Study 1) and decision premises (Study 2).</p></table-wrap-foot></table-wrap>
</p><p id="Par32">
<table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Statistical differences in choice preference between AI chatbots and humans in studies 1 and 2.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1"/><th align="left" colspan="1" rowspan="1">GPT-3.5 vs. Humans</th><th align="left" colspan="1" rowspan="1">GPT-4 vs. Humans</th><th align="left" colspan="1" rowspan="1">GPT-4 vs. GPT-3.5</th></tr><tr><th align="left" colspan="1" rowspan="1">Study 1: Social contexts</th><th align="left" colspan="3" rowspan="1">Which decision agent was more risk-seeking in corresponding scenarios</th></tr></thead><tbody><tr><td align="left" colspan="4" rowspan="1">
<bold>Human lives:</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">Pos: 6/60/600/6000 lives</td><td align="left" colspan="1" rowspan="1">ND/ND/ND/ND</td><td align="left" colspan="1" rowspan="1">HM<sup>***</sup>/HM<sup>***</sup>/HM<sup>***</sup>/HM<sup>***</sup></td><td align="left" colspan="1" rowspan="1">GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup></td></tr><tr><td align="left" colspan="1" rowspan="1">Neg: 6/60/600/6000 lives</td><td align="left" colspan="1" rowspan="1">ND/ND/ND/ND</td><td align="left" colspan="1" rowspan="1">HM<sup>***</sup>/HM<sup>***</sup>/HM<sup>***</sup>/HM<sup>***</sup></td><td align="left" colspan="1" rowspan="1">GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup></td></tr><tr><td align="left" colspan="4" rowspan="1"><bold>Kinship constituents</bold>:</td></tr><tr><td align="left" colspan="1" rowspan="1">Pos: 1/2/3/6 kin</td><td align="left" colspan="1" rowspan="1">ND/ND/ND/ND</td><td align="left" colspan="1" rowspan="1">HM<sup>*</sup>/HM<sup>*</sup>/HM<sup>*</sup>/ND</td><td align="left" colspan="1" rowspan="1"><p>GPT3.5<sup>**</sup>/GPT3.5<sup>**</sup>/GPT3.5<sup>**</sup>/</p><p>GPT3.5<sup>*</sup></p></td></tr><tr><td align="left" colspan="1" rowspan="1">Neg: 1/2/3/6 kin</td><td align="left" colspan="1" rowspan="1">ND/ND/ND/ND</td><td align="left" colspan="1" rowspan="1">HM<sup>***</sup>/HM<sup>***</sup>/HM<sup>***</sup>/HM<sup>***</sup></td><td align="left" colspan="1" rowspan="1">GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup></td></tr><tr><td align="left" colspan="4" rowspan="1"><bold>Human population vs. ET population</bold>:</td></tr><tr><td align="left" colspan="1" rowspan="1">Pos: Humans/ETs</td><td align="left" colspan="1" rowspan="1">ND/ND</td><td align="left" colspan="1" rowspan="1">HM<sup>***</sup>/HM<sup>***</sup></td><td align="left" colspan="1" rowspan="1">GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup></td></tr><tr><td align="left" colspan="1" rowspan="1">Neg: Humans/ETs</td><td align="left" colspan="1" rowspan="1">ND/ND</td><td align="left" colspan="1" rowspan="1">HM<sup>***</sup>/HM<sup>***</sup></td><td align="left" colspan="1" rowspan="1">GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup></td></tr><tr><td align="left" colspan="4" rowspan="1"><bold>Public properties</bold>:</td></tr><tr><td align="left" colspan="1" rowspan="1">Pos: 6/60/600/6000 paintings</td><td align="left" colspan="1" rowspan="1">GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>*</sup>/GPT3.5<sup>*</sup></td><td align="left" colspan="1" rowspan="1"><p>HM<sup>*</sup>/HM<sup>**</sup>/HM<sup>***</sup>/</p><p>HM<sup>**</sup></p></td><td align="left" colspan="1" rowspan="1">GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup></td></tr><tr><td align="left" colspan="1" rowspan="1">Neg: 6/60/600/6000 paintings</td><td align="left" colspan="1" rowspan="1">ND/ND/GPT3.5<sup>***</sup>/GPT3.5<sup>*</sup></td><td align="left" colspan="1" rowspan="1">HM<sup>*</sup>/HM<sup>*</sup>/ND/ND</td><td align="left" colspan="1" rowspan="1">GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup></td></tr><tr><td align="left" colspan="4" rowspan="1"><bold>Personal stock shares</bold>:</td></tr><tr><td align="left" colspan="1" rowspan="1">Pos: 6/60/600/6000 dollars</td><td align="left" colspan="1" rowspan="1">ND/ND/ND/ND</td><td align="left" colspan="1" rowspan="1">HM<sup>***</sup>/HM<sup>***</sup>/HM<sup>**</sup>/HM<sup>*</sup></td><td align="left" colspan="1" rowspan="1">GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup></td></tr><tr><td align="left" colspan="1" rowspan="1">Neg: 6/60/600/6000 dollars</td><td align="left" colspan="1" rowspan="1">ND/ND/ND/ND</td><td align="left" colspan="1" rowspan="1">ND/ND/ND/HM<sup>**</sup></td><td align="left" colspan="1" rowspan="1">ND/ND/ND/GPT3.5<sup>***</sup></td></tr><tr><td align="left" colspan="1" rowspan="1">
<bold>Study 2: Decision premises</bold>
</td><td align="left" colspan="1" rowspan="1">
<bold>GPT-3.5 vs. Humans</bold>
</td><td align="left" colspan="1" rowspan="1">
<bold>GPT-4 vs. Humans</bold>
</td><td align="left" colspan="1" rowspan="1">
<bold>GPT-4 vs. GPT-3.5</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>Sense of fairness</bold>:</td><td align="left" colspan="3" rowspan="1">
<bold>Which decision agent preferred the unfair (pre-selected) option</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">6/600 lives</td><td align="left" colspan="1" rowspan="1">ND/ND</td><td align="left" colspan="1" rowspan="1">HM<sup>**</sup>/HM<sup>***</sup></td><td align="left" colspan="1" rowspan="1">GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup></td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>Self-age awareness</bold>:</td><td align="left" colspan="3" rowspan="1">
<bold>Which decision agent was more risk-seeking in corresponding scenarios</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">YSO-6 kin/YSY-6 kin</td><td align="left" colspan="1" rowspan="1">ND/ND</td><td align="left" colspan="1" rowspan="1">GPT4<sup>**</sup>/ND</td><td align="left" colspan="1" rowspan="1">GPT4<sup>***</sup>/ND</td></tr><tr><td align="left" colspan="1" rowspan="1">MSO-6 kin/MSY-6 kin</td><td align="left" colspan="1" rowspan="1">HM<sup>*</sup>/ND</td><td align="left" colspan="1" rowspan="1">HM<sup>**</sup>/HM<sup>***</sup></td><td align="left" colspan="1" rowspan="1">ND/GPT3.5<sup>***</sup></td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>Group-dependent goal settings (unequal EVs)</bold>:</td><td align="left" colspan="3" rowspan="1">
<bold>Which decision agent was more risk-seeking in corresponding scenarios</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">6 kin/6 lives/600 lives</td><td align="left" colspan="1" rowspan="1">ND/ND/ND</td><td align="left" colspan="1" rowspan="1">HM<sup>***</sup>/HM<sup>*</sup>/HM<sup>*</sup></td><td align="left" colspan="1" rowspan="1">GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup></td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>Group-dependent goal settings (equal EVs and two-thirds survival rate)</bold>:</td><td align="left" colspan="3" rowspan="1">
<bold>Which decision agent was more risk-seeking in corresponding scenarios</bold>
</td></tr><tr><td align="left" colspan="1" rowspan="1">6 kin/6 lives/600 lives</td><td align="left" colspan="1" rowspan="1">ND/GPT3.5<sup>*</sup>/ND</td><td align="left" colspan="1" rowspan="1">HM<sup>***</sup>/ND/HM<sup>***</sup></td><td align="left" colspan="1" rowspan="1">GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup>/GPT3.5<sup>***</sup></td></tr></tbody></table><table-wrap-foot><p>Note. Pos, Positive framing; Neg, Negative framing; ND, No differences; HM, human participants. GPT3.5, GPT-3.5; GPT4, GPT-4. <sup>*</sup>, <italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.050; <sup>**</sup>, <italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.010; <sup>***</sup>, <italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.001. We summarize the choice preference differences between AI chatbots (GPT-3.5 and GPT-4) and human benchmarks across 51 decision scenarios varied in different social contexts (Study 1) and decision premises (Study 2).</p></table-wrap-foot></table-wrap>
</p><sec id="Sec3"><title>Study 1: risk preference for social and verbal cues</title><p id="Par33">Figure <xref rid="Fig1" ref-type="fig">1</xref> shows the percentages of risk-seeking choices under positive and negative framing across different social contexts, including human lives in groups of different sizes<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>, kinship structure<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, human versus ET population<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>, public properties<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, and personal stock shares<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. The comparison was focused on the choice percentages of risk-seeking versus risk-aversion, particularly, the framing effects and their direction as an empirical probe into human choice and AI forecasting.</p><p id="Par34">
<fig id="Fig1" position="float" orientation="portrait"><label>Fig. 1</label><caption><p>Choice patterns for humans and AI chatbots in Study 1.</p></caption><graphic id="d33e1430" position="float" orientation="portrait" xlink:href="41598_2025_17188_Fig1_HTML.jpg"/></fig>
</p><p id="Par35">We visualize the percentage of risk-seeking choices for each of the 18 framing pairs (positive vs. negative) for the human (top row), GPT-3.5 (middle row), and GPT-4 (bottom row) in five social contexts in each of the panels. The social contexts involve: <bold>a</bold>. human lives; <bold>b</bold>. kinship constituents; <bold>c</bold>. entire human vs. ET population; <bold>d</bold>. public properties, and <bold>e</bold>. personal stock shares. We illustrate statistically significant framing effects on risk preference (<italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.050), presenting the percentages of risk-seeking (gamble) choices in numerical values for both framing conditions. Exact <italic toggle="yes">p</italic>-values below 0.001 are shown in scientific notation. The vertical axis of each panel denotes the percentage of the risk-seeking (gamble) choice under positive and negative framing across social contexts.</p></sec><sec id="Sec4"><title>The life-death decisions in small and large groups</title><p id="Par36">&#160;Humans exhibited framing effects in the large-group context, with a greater risk-seeking preference in negative framing than positive framing involving 600 lives (<italic toggle="yes">Z</italic> = &#8722;2.75, 95% Confidence Interval (CI), &#8722;2.95 to &#8722;2.56, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.027).</p><p id="Par37">Consistent with Hypothesis 1, GPT-4 was indiscriminately insensitive to group size and showed no framing effects across the four groups (6, 60, 600, and 6000). GPT-4 was significantly more risk-averse than humans (adjusted log Odds Ratio (OR), &#8722;5.43 to &#8722;4.23, adjusted <italic toggle="yes">ps</italic>&#8201;&lt;&#8201;1.61&#8201;&#215;&#8201;10<sup>&#8722;4</sup>) and GPT-3.5 (adjusted log OR, &#8722;5.35 to &#8722;4.26, adjusted <italic toggle="yes">ps</italic>&#8201;&lt;&#8201;3.28&#8201;&#215;&#8201;10<sup>&#8722;5</sup>).</p><p id="Par38">Unlike human participants, who were more sensitive to verbal framing in large-group contexts, GPT-3.5 was more sensitive to verbal cues in small-group contexts (6-lives: <italic toggle="yes">Z</italic>&#8201;=&#8201;2.56, 95% CI, 2.43 to 2.70, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.021) (Fig.&#160;<xref rid="Fig1" ref-type="fig">1</xref>a). Noteworthy is the observation that the direction of the framing effects was the opposite of human pattern. While humans were more risk-seeking under the negative framing, GPT-3.5 was more risk-seeking under the positive framing, and vice versa. The reversed framing effect was also inconsistent with the S-shaped value function of Prospect theory<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>.</p><p id="Par39">Across framing conditions, humans showed overall risk-seeking preferences for small groups (6-lives: <italic toggle="yes">Z</italic>&#8201;=&#8201;2.44, 95% CI, 2.30 to 2.58, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.015; 60-lives: <italic toggle="yes">Z</italic>&#8201;=&#8201;2.08, 95% CI, 1.93 to 2.24, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.037). In contrast, GPT-3.5 showed a consistent risk-neutral preference across different group sizes but was more risk-seeking in smaller groups compared to larger ones (6-lives vs. 600-lives: <italic toggle="yes">Z</italic>&#8201;=&#8201;3.10, 95% CI, 3.00 to 3.20, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.012). Unlike humans and GPT-3.5, GPT-4 exhibited a consistent overall risk-averse preference across different group sizes (for each scenario: <italic toggle="yes">Z</italic> = &#8722;11.55, 95% CI, &#8722;11.63 to &#8722;11.46, adjusted <italic toggle="yes">p</italic>&#8201;&lt;&#8201;10<sup>&#8722;15</sup>).</p></sec><sec id="Sec5"><title>Kinship and group membership constituency</title><p id="Par40">As illustrated in Fig.&#160;<xref rid="Fig1" ref-type="fig">1</xref>b, humans exhibited framing effects in the mixed groups involving either one kin and five strangers (<italic toggle="yes">Z</italic> = &#8722;3.20, 95% CI, &#8722;3.39 to &#8722;3.01, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.025) or two kin and four strangers (<italic toggle="yes">Z</italic> = &#8722;2.61, 95% CI, &#8722;2.80 to &#8722;2.42, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.033). The framing effect disappeared when the kinship constituent reached 50% or in an all-kin group of six.</p><p id="Par41">In contrast, GPT-4 was mainly insensitive to kinship membership in the mixed group contexts and remained significantly risk-averse than humans (adjusted log OR, &#8722;5.71 to &#8722;2.54, adjusted <italic toggle="yes">ps</italic>&#8201;&lt;&#8201;0.047) and GPT-3.5 (adjusted log OR, &#8722;4.38 to &#8722;2.26, adjusted <italic toggle="yes">ps</italic>&#8201;&lt;&#8201;0.028). Unlike the consistent risk-seeking preference in human choice with no framing effect in the pure kinship group, GPT-4 exhibited an inconsistent risk preference in the pure kinship group with a reversed framing effect (<italic toggle="yes">Z</italic>&#8201;=&#8201;3.32, 95% CI, 3.23 to 3.41, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.005).</p><p id="Par42">GPT-3.5, on the other hand, exhibited significant framing effects in the opposite direction of the human framing effect in the mixed group contexts with one or three kin involved (1-kin: <italic toggle="yes">Z</italic>&#8201;=&#8201;2.81, 95% CI, 2.67 to 2.95, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.011; 3-kin: <italic toggle="yes">Z</italic>&#8201;=&#8201;3.22, 95% CI, 3.09 to 3.36, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.004).</p><p id="Par43">These results were broadly consistent with Hypothesis 1 and Hypothesis 2.</p></sec><sec id="Sec6"><title>Human population vs. ET population</title><p id="Par44">Consistent with Hypothesis 3, GPT-4 showed no sensitivity to the human versus ET contrast (Fig.&#160;<xref rid="Fig1" ref-type="fig">1</xref>c). However, inconsistent with Hypothesis 3, GPT-3.5 exhibited a risk-preference pattern similar to humans, where a framing effect occurred with human lives and disappeared with ET lives (for humans: <italic toggle="yes">Z</italic> = &#8722;2.94, 95% CI, &#8722;3.13 to &#8722;2.75, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.027; for GPT-3.5: <italic toggle="yes">Z</italic> = &#8722;2.84, 95% CI, &#8722;2.97 to &#8722;2.70, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.011). GPT-4 was consistently more risk-averse than humans (adjusted log OR, &#8722;4.69 to &#8722;4.06, adjusted <italic toggle="yes">ps</italic>&#8201;&lt;&#8201;3.00&#8201;&#215;&#8201;10<sup>&#8722;4</sup>) and GPT-3.5 (adjusted log OR, &#8722;4.58 to &#8722;4.23, adjusted <italic toggle="yes">ps</italic>&#8201;&lt;&#8201;3.64&#8201;&#215;&#8201;10<sup>&#8722;5</sup>).</p></sec><sec id="Sec7"><title>Public properties</title><p id="Par45">As illustrated in Fig.&#160;<xref rid="Fig1" ref-type="fig">1</xref>d, humans became more risk-averse as the values of public property (precious museum paintings) increased, whereas GPT-3.5 and GPT-4 did not adjust their predicted risk preference as a function of property value.</p><p id="Par46">Without considering decision framings, human participants showed risk-neutral preferences for small financial values and risk-averse preferences for larger values (600-paintings: <italic toggle="yes">Z</italic> = &#8722;2.25, 95% CI, &#8722;2.40 to &#8722;2.10, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.025; 6000-paintings: <italic toggle="yes">Z</italic> = &#8722;2.66, 95% CI, &#8722;2.81 to &#8722;2.50, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.008). Risk aversion increased with financial value (6-paintings vs. 6000 paintings: <italic toggle="yes">Z</italic> = &#8722;2.58, 95% CI, &#8722;2.74 to &#8722;2.43, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.059), indicating a marginal trend that did not survive multiplecomparison correction.</p><p id="Par47">In contrast, GPT-3.5 showed a consistent risk-seeking preference across financial values (<italic toggle="yes">Z</italic>, 7.35 to 9.12, adjusted <italic toggle="yes">ps</italic>&#8201;&lt;&#8201;3.09&#8201;&#215;&#8201;10<sup>&#8722;14</sup>). Unlike humans and GPT-3.5, GPT-4 exhibited a consistent overall risk-averse preference across financial values (<italic toggle="yes">Z</italic>, &#8722;10.80 to &#8722;9.53, adjusted <italic toggle="yes">ps</italic>&#8201;&lt;&#8201;10<sup>&#8722;15</sup>).</p></sec><sec id="Sec8"><title>Personal stock investment</title><p id="Par48">As illustrated in Fig.&#160;<xref rid="Fig1" ref-type="fig">1</xref>e, humans showed overall risk-neutral preferences for small financial values and risk-averse preferences for larger values (600-dollars: <italic toggle="yes">Z</italic> = &#8722;2.46, 95% CI, &#8722;2.63 to &#8722;2.29, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.014; 6000-dollars: <italic toggle="yes">Z</italic> = &#8722;3.65, 95% CI, &#8722;3.82 to &#8722;3.49, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;2.59&#8201;&#215;&#8201;10<sup>&#8722;4</sup>). Risk aversion increased with financial value compared to small values (6-dollars vs. 6000-dollars: <italic toggle="yes">Z</italic> = &#8722;3.09, 95% CI, &#8722;3.25 to &#8722;2.92, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.006; 60-dollars vs. 6000-dollars: <italic toggle="yes">Z</italic> = &#8722;3.35, 95% CI, &#8722;3.51 to &#8722;3.19, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.005).</p><p id="Par49">In contrast, GPT-3.5 showed risk-seeking tendencies for small financial values (6-dollars: <italic toggle="yes">Z</italic>&#8201;=&#8201;3.35, 95% CI, 3.25 to 3.44, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.001) and risk-averse tendencies as values increased (60-dollars: <italic toggle="yes">Z</italic> = &#8722;2.42, 95% CI, &#8722;2.51 to &#8722;2.32, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.016; 600-dollars: <italic toggle="yes">Z</italic> = &#8722;2.72, 95% CI, &#8722;2.82 to &#8722;2.63, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.006).</p><p id="Par50">Unlike both humans and GPT-3.5, GPT-4 consistently exhibited overall risk-averse preferences across financial values (<italic toggle="yes">Z</italic>, &#8722;11.55 to &#8722;9.39, adjusted <italic toggle="yes">ps</italic>&#8201;&lt;&#8201;10<sup>&#8722;15</sup>), with risk aversion increasing as financial values grew (6-dollars vs. 6000-dollars: <italic toggle="yes">Z</italic> = &#8722;3.95, 95% CI, &#8722;3.98 to &#8722;3.91, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;4.73&#8201;&#215;&#8201;10<sup>&#8722;4</sup>; 60-dollars vs. 6000-dollars: <italic toggle="yes">Z</italic> = &#8722;3.20, 95% CI, &#8722;3.23 to &#8722;3.17, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.004; 600-dollars vs. 6000-dollars: <italic toggle="yes">Z</italic> = &#8722;3.03, 95% CI, &#8722;3.06 to &#8722;3.01, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.005).</p><p id="Par51">GPT-4 remained risk-averse in the $6000 scenario compared to humans, irrespective of the framing condition (adjusted log OR, &#8722;4.55 to &#8722;2.48, adjusted <italic toggle="yes">ps</italic>&#8201;&lt;&#8201;0.042). Consistent with Hypothesis 4, in the financial domain, GPT-4 and partially GPT-3.5 showed framing effects in the same direction as humans. However, inconsistent with Hypothesis 4, only humans adjusted their risk preference as a function of the monetary value of property or investment.</p><p id="Par52"><bold>A summary of Study 1.</bold> GPT-3.5 demonstrated framing effects in nine of the 18 framing pairs. Six of those nine framing effects were in the opposite direction compared to typical human framing effects. Two of those human-like framing effects were in the financial domain only. GPT-4 exhibited six framing effects; five were directionally consistent with typical human framing effects. These human-like framing effects were all observed in the financial domain. Neither LLMs adjusted their financial risk preference to the values of properties or stocks, as did humans.</p></sec><sec id="Sec9"><title>Study 2: effects of decision premises on risk preference</title><p id="Par53">Figure <xref rid="Fig2" ref-type="fig">2</xref> displays the results of Study 2. We examined risk preference across 15 experimental conditions, focusing on three decision premises: fairness<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, self-age awareness<sup><xref ref-type="bibr" rid="CR28">28</xref></sup> and group-dependent goal settings<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>.</p><p id="Par54">
<fig id="Fig2" position="float" orientation="portrait"><label>Fig. 2</label><caption><p>Choice patterns for humans and AI chatbots in Study 2.</p></caption><graphic id="d33e1773" position="float" orientation="portrait" xlink:href="41598_2025_17188_Fig2_HTML.jpg"/></fig>
</p><p id="Par55">We visualize the percentage of choices in 15 life-death scenarios for the human (top row), GPT-3.5 (middle row), and GPT-4 (bottom row) data in three decision premises in each of the panels. The decision premises examined include: <bold>a</bold>. sense of fairness; <bold>b &amp; c</bold>. self-age awareness relative to &#8220;saving old&#8221; and &#8220;saving young&#8221;; and <bold>d &amp; e</bold>. group-dependent goal settings. We illustrate statistically significant choice preference (<italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.050), presenting the percentages of choices in numerical values for significant results. Exact <italic toggle="yes">p</italic>-values below 0.001 are shown in scientific notation. All <italic toggle="yes">p</italic>-values smaller than 10<sup>&#8722;15</sup> (machine-precision limit) are reported as &#8220;&lt; 10<sup>&#8722;15</sup>&#8221;. Note that the vertical axis of <bold>Panel a</bold> denotes the percentages of the two alternative sure options; the vertical axes of <bold>Panels b&#8211;d</bold> denote the choice percentages of the sure option versus gamble option; the vertical axis of <bold>Panel e</bold> denotes the choice percentages of the gamble option in the one-third survival condition and two-thirds survival condition.</p></sec><sec id="Sec10"><title>Sense of fairness</title><p id="Par56">More human participants favored the fair option in the small group of six individuals than in the large group of 600 individuals (6-lives: <italic toggle="yes">Z</italic> = &#8722;5.24, 95% CI, &#8722;5.45 to &#8722;5.02, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;6.52&#8201;&#215;&#8201;10<sup>&#8722;7</sup>). In contrast, GPT-3.5 and GPT-4 exhibited similar choice preferences across the two group contexts. GPT-4 were extremely and consistently risk averse in both group contexts in favor of the fair option (<italic toggle="yes">Z</italic> = &#8722;14.00, 95% CI, &#8722;14.14 to &#8722;13.87, adjusted <italic toggle="yes">ps</italic>&#8201;&lt;&#8201;10<sup>&#8722;15</sup>) (Fig.&#160;<xref rid="Fig2" ref-type="fig">2</xref>a). These results are broadly consistent with Hypothesis 6. However, inconsistent with Hypothesis 6, GPT-3.5 exhibited strange favoritism of the unfair pre-selected sure option in both group contexts (6-lives: <italic toggle="yes">Z</italic>&#8201;=&#8201;3.92, 95% CI, 3.78 to 4.06, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;2.65&#8201;&#215;&#8201;10<sup>&#8722;4</sup>; 600-lives: <italic toggle="yes">Z</italic>&#8201;=&#8201;2.24, 95% CI, 2.10 to 2.38, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.038).</p></sec><sec id="Sec11"><title>Self-age awareness and age-related risky choice</title><p id="Par57">Young human participants in their 20s showed a clear risk-seeking preference to save all independent of the sure option being &#8220;saving older kin&#8221; (YSO-6 kin) or &#8220;saving younger kin&#8221; (YSY-6 kin) (Fig.&#160;<xref rid="Fig2" ref-type="fig">2</xref>b) (YSO-6 kin: <italic toggle="yes">Z</italic>&#8201;=&#8201;4.80, 95% CI, 4.59 to 5.01, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;4.74&#8201;&#215;&#8201;10<sup>&#8722;6</sup>; YSY-6 kin: <italic toggle="yes">Z</italic>&#8201;=&#8201;4.69, 95% CI, 4.48 to 4.90, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;6.54&#8201;&#215;&#8201;10<sup>&#8722;6</sup>). In contrast, as predicted in Hypothesis 7, both GPT-3.5 and GPT-4 were more risk-seeking in the YSO-6 kin condition (GPT-3.5: <italic toggle="yes">Z</italic>&#8201;=&#8201;5.60, 95% CI, 5.46 to 5.74, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;2.56&#8201;&#215;&#8201;10<sup>&#8722;7</sup>; GPT-4: <italic toggle="yes">Z</italic>&#8201;=&#8201;14.00, 95% CI, 13.87 to 14.14, adjusted <italic toggle="yes">p</italic>&#8201;&lt;&#8201;10<sup>&#8722;15</sup>).</p><p id="Par58">For middle-aged human participants in their 40s, saving older (MSO-6 kin) or younger kin (MSY-6 kin) (Fig.&#160;<xref rid="Fig2" ref-type="fig">2</xref>c) had different fitness values. The sure option of saving younger kin became more favorable than saving older kin (MSO-6 kin: <italic toggle="yes">Z</italic>&#8201;=&#8201;7.00, 95% CI, 6.76 to 7.24, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;1.73&#8201;&#215;&#8201;10<sup>&#8722;11</sup>).</p><p id="Par59">Consistent with Hypothesis 7, the LLMs showed no difference in risk preference between these two sure options. Meanwhile, GPT-4 revealed a reverse preference pattern exhibiting a higher preference for the sure option in both the &#8220;saving older&#8221; and &#8220;saving younger&#8221; kin situations. Middle-aged humans were more extreme in their risk-seeking preference, indicating that the sure option of saving their older kin was less favorable compared to the gamble option. In contrast, GPT-4 was more risk-averse in both the MSO-6 kin condition (<italic toggle="yes">Z</italic> = &#8722;5.04, 95% CI, &#8722;5.18 to &#8722;4.90, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;5.95&#8201;&#215;&#8201;10<sup>&#8722;7</sup>) and the MSY-6 kin condition (<italic toggle="yes">Z</italic> = &#8722;14.00, 95% CI, &#8722;14.14 to &#8722;13.87, adjusted <italic toggle="yes">p</italic>&#8201;&lt;&#8201;10<sup>&#8722;15</sup>).</p></sec><sec id="Sec12"><title>Group-dependent goal settings with an inferior gamble option of lower EV</title><p id="Par60">Human participants were increasingly risk-seeking from a large-group context involving 600 people to a small-group context with six people and to a kinship group with six kin (Fig.&#160;<xref rid="Fig2" ref-type="fig">2</xref>d) (6-kin vs. 600-lives: <italic toggle="yes">Z</italic>&#8201;=&#8201;4.17, 95% CI, 3.96 to 4.37, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;9.29&#8201;&#215;&#8201;10<sup>&#8722;5</sup>; 6-kin vs. 6-lives: <italic toggle="yes">Z</italic>&#8201;=&#8201;2.67, 95% CI, 2.46 to 2.88, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.011). This result indicates that goal-setting was increasingly higher from the large-group, small-group to the kin group, making the sure option of saving one-third of the group less and less satisfactory, resulting in a linear increase in the choice of the inferior (dominated) gamble option. In contrast, LLMs revealed a flat curve with similar risk preference across the three group contexts, with no difference in risk preference across the group dimension. Compared to humans, GPT-4 was consistently more risk-averse across group contexts (adjusted log OR, &#8722;4.81 to &#8722;2.61, adjusted <italic toggle="yes">ps</italic>&#8201;&lt;&#8201;0.029). Overall, these results support predictions from Hypothesis 8.</p></sec><sec id="Sec13"><title>Group-dependent goal settings with two levels of EV</title><p id="Par61">In this experiment, the sure and gamble options had equal EVs of different levels (one-third vs. two-thirds survival rate) across large-group, small-group, and kinship contexts (Fig.&#160;<xref rid="Fig2" ref-type="fig">2</xref>e).</p><p id="Par62">As predicted by Hypothesis 9, increasing the survival rate from one-third to two-thirds for human participants had no effect on risk preference in the kin and large group situations. Only in the small-group situation, raising the sure-option survival rate produced a trend to choose that option (<italic toggle="yes">Z</italic> = &#8722;1.99, 95% CI, &#8722;2.23 to &#8722;1.75, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.080), indicating that the sure option of saving two-thirds of the group on average passed the goal setting of the human decision-makers.</p><p id="Par63">In contrast, the GPT-3.5 consistently favored the two-thirds survival rate across the three group contexts (6-kin: <italic toggle="yes">Z</italic>&#8201;=&#8201;2.83, 95% CI, 2.71 to 2.94, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.008; 6-lives: <italic toggle="yes">Z</italic>&#8201;=&#8201;3.74, 95% CI, 3.63 to 3.86, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;4.33&#8201;&#215;&#8201;10<sup>&#8722;4</sup>; 600-lives: <italic toggle="yes">Z</italic>&#8201;=&#8201;4.30, 95% CI, 4.17 to 4.44, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;1.02&#8201;&#215;&#8201;10<sup>&#8722;4</sup>), presumably due to their lack of implicit and social group-dependent goal settings. GPT-4 remained extremely risk-averse across the three group contexts (<italic toggle="yes">Z</italic>, &#8722;14.00 to &#8722;10.64, adjusted <italic toggle="yes">ps</italic>&#8201;&lt;&#8201;10<sup>&#8722;15</sup>) and even favored the lower survival rate in the small group situation (6-lives: <italic toggle="yes">Z</italic>&#8201;=&#8201;3.32, 95% CI, 3.25 to 3.39, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.001).</p></sec><sec id="Sec14"><title>A summary of Study 2</title><p id="Par64">Across 15 social decision scenarios involving three decision premises, LLMs significantly differed from their human counterparts due to a more fixed sense of fairness insensitive to the social context of small, in-group versus large, anonymous groups. Due to their lack of self-age awareness, the two LLMs did not differentiate the decision recipients based on their age-related reproductive values relative to the age of the decision maker (young or middle-aged).</p><p id="Par65">Human risk preference was primarily regulated by their implicit group-dependent goal settings in kin, small, and large groups, irrespective of the EV of decision outcomes in these conditions. In contrast, the risky choices made by the two LLMs were much more normative and EV-dependent.</p></sec><sec id="Sec15"><title>A Validation study: GPT-4o vs. Humans</title><p id="Par66">To assess robustness of the results, we also gathered GPT-4o predictions for 16 selected scenarios (8 framing pairs), focusing on human group size effects and kinship effects. We chose OpenAI&#8217;s GPT series because of its consistent architecture and widespread use; comparisons with other generative chatbots are needed for future research. In comparison, when making social decisions, Google&#8217;s Bard and Microsoft&#8217;s Bing chatbots encountered substantial difficulties in understanding the tasks<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. Figure&#160;<xref rid="Fig3" ref-type="fig">3</xref> illustrates the results for GPT-4o and the human benchmark. Detailed statistical results refer to Tables <xref rid="MOESM1" ref-type="media">S1</xref> and <xref rid="MOESM1" ref-type="media">S2</xref> (Supplementary Materials).</p><p id="Par67">
<fig id="Fig3" position="float" orientation="portrait"><label>Fig. 3</label><caption><p>Choice patterns of humans vs. GPT-4o predictions.</p></caption><graphic id="d33e2033" position="float" orientation="portrait" xlink:href="41598_2025_17188_Fig3_HTML.jpg"/></fig>
</p><p id="Par68">The figure illustrates the percentage of risk-seeking choices for each of the eight social contexts under two framing (positive vs. negative) conditions for the human data (top row) and GPT-4o predictions (bottom row). The social contexts involve: <bold>a</bold>. human lives in groups of four different sizes; and <bold>b</bold>. kinship constituents in four small group contexts. We illustrate statistically significant framing effects on risk preference (<italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.050) by presenting the percentages of risk-seeking (gamble) choices in numerical values and corresponding p-values. The vertical axis of each panel denotes the percentage of the risk-seeking (gamble) choice.</p></sec><sec id="Sec16"><title>The life-death decisions in small and large groups</title><p id="Par69">As illustrated in Fig.&#160;<xref rid="Fig3" ref-type="fig">3</xref>a and inconsistent with humans, the predicted choices by GPT-4o were significantly risk-seeking as group size increased (e.g., 6-lives vs. 600-lives: <italic toggle="yes">Z</italic> = &#8722;3.81, 95% CI, &#8722;3.85 to &#8722;3.77, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;1.66&#8201;&#215;&#8201;10<sup>&#8722;4</sup>; 600-lives vs. 6000 lives: <italic toggle="yes">Z</italic> = &#8722;9.38, 95% CI, &#8722;9.48 to &#8722;9.29, adjusted <italic toggle="yes">p</italic>&#8201;&lt;&#8201;10<sup>&#8722;15</sup>). Compared to humans, GPT-4o was significantly more risk-averse in smaller groups (adjusted log OR, &#8722;5.31 to &#8722;2.13, adjusted <italic toggle="yes">ps</italic>&#8201;&lt;&#8201;2.07&#8201;&#215;&#8201;10<sup>&#8722;5</sup>) but showed no overall difference in risk preference from humans in the largestgroup with 6000 lives.</p><p id="Par70">GPT-4o showed framing effects in large groups in the same direction as humans, choosing the risky option more often under negative framing than positive framing for both 600 lives (<italic toggle="yes">Z</italic> = &#8722;4.38, 95% CI, &#8722;4.46 to &#8722;4.29, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;2.42&#8201;&#215;&#8201;10<sup>&#8722;5</sup>) and 6000 lives (<italic toggle="yes">Z</italic> = &#8722;4.49, 95% CI, &#8722;4.63 to &#8722;4.35, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;1.91&#8201;&#215;&#8201;10<sup>&#8722;5</sup>).</p></sec><sec id="Sec17"><title>Kinship and group membership constituency</title><p id="Par71">As illustrated in Fig.&#160;<xref rid="Fig3" ref-type="fig">3</xref>b, GPT-4o showed framing effects opposite to those of human participants in mixed groups when one or two kin were present (1-kin: <italic toggle="yes">Z</italic>&#8201;=&#8201;7.48, 95% CI, 7.37 to 7.60, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;2.92&#8201;&#215;&#8201;10<sup>&#8722;13</sup>; 2-kin: <italic toggle="yes">Z</italic>&#8201;=&#8201;10.83, 95% CI, 10.70 to 10.97, adjusted <italic toggle="yes">p</italic>&#8201;&lt;&#8201;10<sup>&#8722;15</sup>) and reversed the effect entirely in the three-kin context (<italic toggle="yes">Z</italic> = &#8722;2.52, 95% CI, &#8722;2.66 to &#8722;2.39, adjusted <italic toggle="yes">p</italic>&#8201;=&#8201;0.019).</p></sec><sec id="Sec18"><title>A summary of the validation study</title><p id="Par72">Overall, while humans were more risk-seeking in small or kin groups than large groups, GPT 4o made the opposite predictions, and reversed framing effects in mixed group contexts.</p></sec></sec><sec id="Sec19"><title>Discussion</title><p id="Par73">The present research establishes a comprehensive battery of evolutionarily valid and socially meaningful dimensions for evaluating the social decision rationality of three generations of OpenAI&#8217;s GPT-series (GPT-3.5, GPT-4, and GPT-4o) regarding their ability to forecast human decisions. Predictive accuracy of LLMs was clearly limited, suggesting a need for socially specific alignment. Of the 51 scenarios (Figs.&#160;<xref rid="Fig1" ref-type="fig">1</xref> and <xref rid="Fig2" ref-type="fig">2</xref>), GPT-3.5 aligned with human choices only in the four scenarios of saving human versus ET populations. GPT-4 diverged sharply by forecasting highly conservative preferences across social domains. GPT-4o decreased their risk-seeking preference from small to large groups, which is opposite to the human choice pattern. Second, GPT-4o matched human framing effects in large group situations but predicted reversed framing effects in the mixed groups, compared to human framing effects.</p><p id="Par74">These results suggest that human decision-making relies on evolved decision heuristics, such as mating strategies<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>, herding behaviors<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup>, and foraging energy budget rules<sup><xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR38">38</xref></sup>. In contrast, AI forecasting appears insensitive to social contexts and possibly guided more by pattern estimation<sup><xref ref-type="bibr" rid="CR39">39</xref>,<xref ref-type="bibr" rid="CR40">40</xref></sup>, EV-based computations, or, social norms rather than social domain-specific evolutionary adaptations<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Our hypotheses and interpretations remain tentative as we did not directly test the mechanisms underlying these differences.</p><p id="Par75">Study 1 presented a comprehensive test of inclusive-fitness theory predictions with scenarios involving kith-and-kin, strangers in small in-groups, and large anonymous groups<sup><xref ref-type="bibr" rid="CR26">26</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup>. Study 2 tested a small-group-dependent sense of fairness as a premise for reciprocal altruism<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>, self-age awareness in life-history trade-offs<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>, the effects of implicit goal settings on risk preference based on predictions developed from foraging theory<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>, and tri-reference point theory of risky choice<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. Although Prospect Theory&#8217;s characteristic S-shaped value function<sup><xref ref-type="bibr" rid="CR34">34</xref></sup> has long served as a descriptive benchmark for human risk behavior, LLM forecasts did not consistently follow the predictions from the theory. Rather than mirroring human biases, the LLM predictions often aligned more with normative guides. However, it is possible and worth future examinations that LLMs may fail to capture human decisions descriptively; they may hold promise as prescriptive decision-support tools or as a normative reference for human decision-making.</p><p id="Par76">GPT-3.5 matched human average risk preferences across life-death and personal-investment contexts. However, it inverted the classic human framing effects across kinship and financial contexts. This dissociation suggests that GPT-3.5 accurately internalizes baseline risk levels from its training data but misinterprets framing valence. GPT-3.5 treats gain framing as a cue for increased risk and loss framing as a cue for caution<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. A previous empirical research also revealed these reversed framing effects in GPT-3.5 for predicting human clinical decisions<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. Such a valence-inversion heuristic may stem from statistical biases in GPT-3.5&#8217;s text corpora rather than sensitivity to verbal framing.</p><p id="Par77">In contrast, GPT-4 consistently exhibited social conservatism with a strong risk-averse preference, particularly when human lives were at stake. It paid little attention to the social relations between the decision-maker and decision recipients. GPT-4 is tuned to prioritize normative rationality, safety, and cybersecurity<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup> at the cost of social validity and psychological reliability. Recent research shows that while GPT-4 excels in purely rational tasks<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>, it struggles in subtle social contexts, causing faux pas or social blunders<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. This shift in GPT-4, emphasizing normative rationality and error minimization, may limit AI&#8217;s potential to achieve the goal of artificial general intelligence (AGI) to have the flexibility needed for dynamic, context-sensitive social interactions.</p><p id="Par78">GPT-4o retained an overall conservative bias in risk preference and showed a reversed pattern in risk preference compared to humans: While humans were more risk-seeking in small groups than large groups, GPT-4o was more risk-seeking in large groups than small groups, reflecting their training to optimize EV-based estimates and enforce safety-oriented constraints. Although GPT-4o accurately forecasted the framing effects in large group contexts, it predicted reversed framing effects in mixed group situations with one ot two kin in a six-person group. Recent work shows that GPT-4o can infer latent belief structures<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> and exhibits emergent cognitive consistency akin to human cognitive dissonance moderated by free choice<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>, suggesting the values for integrating deeper social cognition into LLMs.</p><p id="Par79">Study 2 examined three key decision premises shaping human social decision-making. In fairness-related scenarios, GPT-3.5 favored the unfair option of saving pre-selected group members, a reversed preference compared to humans. At the same time, GPT-4 unwaveringly and indiscriminately favored the fair option of equal probability of survival in both small and large group contexts. Only for humans, the sense of fairness was stronger in a small in-group context than in a large-group context, supporting the notion that small-group living was a driving force for the evolution of reciprocal altruism<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> and morality is a form of cooperation<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>.</p><p id="Par80">Humans&#8217; self-consciousness allows us to use our age as a reference when making risky decisions. This self-age consciousness allowed humans to make life-stage dependent fitness evaluations for both the decision-maker and decision recipients. Our findings revealed deviations in risk patterns between humans and LLMs in age-related decisions. When the deciders were young (18&#8211;30 years), the humans indiscriminately favored the gamble option in both saving-young and saving-old conditions, indicating they equally valued their older and younger kin.</p><p id="Par81">In contrast, GPT-3.5 and GPT-4 discriminately predicted the gamble option when the unfair sure option would only save older kin, indicating a stronger preference for saving younger kin. When the human deciders were middle-aged (30&#8211;55 years), they discriminately predicted the sure option to save their younger kin than their older kin selectively. In contrast, GPT-3.5, when prompted to be middle-aged, was indiscriminately risk-neutral. At the same time, GPT-4 became more in favor of saving younger kin and uniformly chose the unfair sure option of saving only the younger kin. These differences suggest that LLMs, lacking self-consciousness and self-age awareness, could not prioritize evolutionary fitness but instead relied on the heuristic that young people have a higher survival potential than old people, presumably learned from pre-trained data.</p><p id="Par82">Regarding group-dependent goal settings, only humans demonstrated linear changes as a function of implicit goal settings or minimum requirements for kinship, small, and large groups<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. In contrast, GPT-3.5 violated the stochastic dominance axiom of normative rationality and favored the dominated gamble option with a lower EV than the sure option in kinship and small group contexts. GPT-4 adhered to the dominance axiom and uniformly favored the dominant gamble option, indicating an improved logical consistency but a low sensitivity to social relations. Humans set higher minimum-requirement thresholds when decisions affect close kin or small ingroups<sup><xref ref-type="bibr" rid="CR5">5</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup>&#8212;reflecting an inclusivefitness-driven, and group-dependent aspiration level. In contrast, GPT-3.5 and GPT-4 adopted the same threshold across all group compositions.</p><p id="Par83">The development from GPT-3.5, GPT-4, to GPT-4o might suggest a fine-tuning trajectory, prioritizing efficiency and safety over psychological accuracy. The forecasting results from GPT-4o indicate that a core limitation, insensitivity to implicit social and evolutionary cues, persists with this more recent model of LLMs. However, human social rationality remains adaptable, incorporating contextual and emotional cues that neither AI model fully replicated. Recent developments in cognitive neuroscience and related disciplines have shown dissociations between language and thought<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. Language is not cognition; it reflects and regulates cognition. Notably, such reflection is more implicit, embodied, and possibly misleading for social cognition and intuition, thus elusive for LLMs to learn and grasp.</p><p id="Par84">AI models&#8217; reliance on conservatism and logical reasoning contrasts with human decision-making, which integrates evolutionary and life-history premises, psychological variables, and social constraints. While excelling in technical tasks like programming<sup><xref ref-type="bibr" rid="CR49">49</xref>,<xref ref-type="bibr" rid="CR50">50</xref></sup> and data analysis<sup><xref ref-type="bibr" rid="CR51">51</xref>,<xref ref-type="bibr" rid="CR52">52</xref></sup>, current AI models still struggle to predict human decisions in social and ecological realms. As their creators, humans should guide AI development toward a deeper understanding of human social rationality.</p><p id="Par85">Our research suggests that current general-purpose LLMs may not fully capture the evolutionary and social constraints that shape human decisions. This limitation is crucial for LLMs&#8217; applications in sensitive healthcare or law enforcement. By quantifying where these models succeed and where they fall short, we also assess their potential to serve as cost-effective synthetic participants in behavioral assessment, reducing the logistical and ethical burdens of human recruitment. Moreover, mapping LLM predictive limits informs the design of AI-based decision aids for practitioners in clinical, financial, and policy-making areas. It also highlights the need for transparency in model mechanisms and open access to AI parameters to advance safe AI development and understanding human behavior<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>.</p><p id="Par86">Finally, our findings suggest a set of criteria for a psychological version of the Turing Test for evaluating AI as predictive tools of human social decision-making. These criteria&#8212;framing effects and social context-dependent risk preferences across kinship, group size, social relations, fairness norms, selfage awareness, public versus personal properties, and group-dependent aspiration levels&#8212;provide actionable guidelines for evaluating when and how LLMs might reliably emulate human choices, and where they require further fine-tuning before deployment as decision aids.</p><p id="Par87">We acknowledge several key limitations of our research. First, our human benchmark comprised predominantly young adults (mostly university students) with only two older cohorts (<italic toggle="yes">M</italic>
<sub>age</sub> = 41.4 years, <italic toggle="yes">N</italic>&#8201;=&#8201;66); decision strategies change across the lifespan, so our results may not extend to older or more diverse populations. Second, we did not preregister our hypotheses or analysis plan, leaving open the possibility of unintentional analytical bias. Third, we used only one prompt formulation per scenario&#8212;adapted from prior work&#8212;without testing alternative phrasings so that some observed LLM failures may reflect prompt sensitivity rather than true gaps in social reasoning. Finally, our evaluation was limited to OpenAI&#8217;s GPT series, which constrains the generalizability of our findings. Future work should compare a broader range of LLM architectures and platforms and include robustness checks on prompt designs to validate and extend these findings.</p></sec><sec id="Sec20"><title>Methods</title><p id="Par88">We compiled a series of decision scenarios to comprehensively assess the predictions on human social decision-making for two LLMs, GPT-3.5 and GPT-4, across various social contexts (Study 1) and decision premises (Study 2). We repeated the same analyses in a validation study using GPT-4o on 16 scenarios (8 framing pairs) related to group size and kinship effects to test the robustness of our results. AI forecasts were benchmarked against choices from 2,104 human participants to map where and how AI aligns with (or diverges from) human social decision rationality.</p><sec id="Sec21"><title>Ethical compliance</title><p id="Par89">This research was approved by the Institutional Review Committee of the Chinese University of Hong Kong (Shenzhen) under protocol (no. EF2024130001) and was conducted per the Declaration of Helsinki; all experiments were performed per the relevant guidelines and regulations. For all the previous human studies included in the analyses, informed consent was obtained from all participants.</p></sec><sec id="Sec22"><title>Social decision-making scenarios</title><p id="Par90">We synthesized 51 decision scenarios from six published research papers<sup><xref ref-type="bibr" rid="CR25">25</xref>&#8211;<xref ref-type="bibr" rid="CR30">30</xref></sup>. We constructed all the decision scenarios in English, considering that the training data for GPT-3.5, GPT-4, and GPT-4o are most comparative in English<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Details of the decision scenarios prompted by AI chatbots for all studies are reported in Table <xref rid="MOESM1" ref-type="media">S3</xref> (Supplementary Materials).</p></sec><sec id="Sec23"><title>Decision scenarios in study 1</title><p id="Par91">We used 36 decision scenarios (18 framing pairs) to investigate how verbal framing manipulations (positive and negative) affected AI chatbots&#8217; risk preferences across five social contexts. Each decision scenario had one positive and one negative version. In the positive framing, we described the expected choice outcomes in terms of gain or survival rate, while in the negative framing, we described the choice outcomes in terms of loss or mortality rate. For each decision scenario, the decision-maker had to make a binary choice between a sure outcome and its gamble alternative with an equal EV.</p><p id="Par92">1) Life-death problems with different group sizes: We used eight decision scenarios (four framing pairs) with different numbers of human lives at stake (6, 60, 600, and 6000 lives, respectively)<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. Each decision scenario presented a decision task of saving lives infected by a fatal disease. The following is an example of the positive framing involving 600 lives:</p><p id="Par93">&#8220;<italic toggle="yes">Imagine that 600 people are infected by a fatal disease. Two alternative medical plans to treat the disease have been proposed. Assume that the exact scientific estimates of the consequences of the plans are as follows: if Plan A is adopted</italic>,<italic toggle="yes"> 200 people will be saved; if Plan B is adopted</italic>,<italic toggle="yes"> there is a one-third probability that all 600 people will be saved</italic>,<italic toggle="yes"> and a two-thirds probability that none of them will be saved.</italic>&#8221;</p><p id="Par94">2) Life-death problems in a kinship group: We used eight decision scenarios (four framing pairs) with different numbers of kin in the group (1, 2, 3, and 6 kin out of a six-member group, respectively). The following is an example of positive framing involving six kin:</p><p id="Par95">&#8220;<italic toggle="yes">Imagine that six people in your family</italic>,<italic toggle="yes"> including both of your parents</italic>,<italic toggle="yes"> your brothers</italic>,<italic toggle="yes"> and your sisters</italic>,<italic toggle="yes"> are infected by a fatal disease. Two alternative medical plans to treat the disease have been proposed. Assume that the exact scientific estimates of the consequences of the plans are as follows: if Plan A is adopted</italic>,<italic toggle="yes"> two of them will be saved; if Plan B is adopted</italic>,<italic toggle="yes"> there is a one-third probability that all six of them will be saved</italic>,<italic toggle="yes"> and a two-thirds probability that none of them will be saved.</italic>&#8221;</p><p id="Par96">3) Life-death problems involving human and ET populations: We used four decision scenarios (two framing pairs) involving 6&#160;billion human lives and 6&#160;billion ET lives<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. The following is an example of positive framing involving the human population:</p><p id="Par97">&#8220;<italic toggle="yes">Imagine that 6&#160;billion humans are infected by a fatal disease. Two alternative medical plans to treat the disease have been proposed. Assume that the exact scientific estimates of the consequences of the plans are as follows: if Plan A is adopted</italic>,<italic toggle="yes"> 2&#160;billion humans will be saved; if Plan B is adopted</italic>,<italic toggle="yes"> there is a one-third probability that all 6&#160;billion humans will be saved</italic>,<italic toggle="yes"> and a two-thirds probability that none of them will be saved.</italic>&#8221;</p><p id="Par98">4) Public property problems: We used eight decision scenarios (four framing pairs) with different numbers of precious museum paintings at stake (6, 60, 600, and 6000, respectively)<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. The following is an example of positive framing involving 600 paintings:</p><p id="Par99">&#8220;<italic toggle="yes">Imagine that 600 pieces of precious paintings in a world-famous museum are accidentally exposed to disastrous chemical pollution. Two alternative plans to rescue these art treasures have been proposed. Assume that the exact estimates of the consequences of the plans made by scientists are as follows: if you choose Plan A</italic>,<italic toggle="yes"> 200 pieces will be saved from chemical pollution; if you choose Plan B</italic>,<italic toggle="yes"> there is a one-third probability that all the paintings will be saved</italic>,<italic toggle="yes"> and a two-thirds probability that none of these paintings will be saved.</italic>&#8221;</p><p id="Par100">5) Personal stock investment problems: We used eight decision scenarios (four framing pairs) with different amounts of stock shares at stake (6, 60, 600, and 6000 dollars, respectively)<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. The following is an example of positive framing involving 600 dollars:</p><p id="Par101">&#8220;<italic toggle="yes">Imagine that you bought $600 worth of stock from a company that recently filed a bankruptcy claim. The company now provides two plans to recover some of your money. If you choose Plan A</italic>,<italic toggle="yes"> you will save $200. If you choose Plan B</italic>,<italic toggle="yes"> you will take part in a random drawing procedure with exactly a one-third probability of saving all your money and two-thirds probability of saving none of your money.</italic>&#8221;</p></sec><sec id="Sec24"><title>Decision scenarios in study 2</title><p id="Par102">We presented 15 scenarios with manipulations of three decision premises, each with an example problem below.</p><p id="Par103">1) Sense of fairness in either a small group of six people or a large group of 600 people<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>: The following exemplifies the two scenarios in which one of the two sure options involved a pre-selection of the survivors:</p><p id="Par104">&#8220;<italic toggle="yes">Imagine that 600/6 people are infected by a fatal disease. Two alternative medical plans to treat the disease have been proposed. Assume that the exact scientific estimate of the consequences of the plans is as follows: if Plan A is adopted</italic>,<italic toggle="yes"> 200/2 people will be saved; if Plan B is adopted</italic>,<italic toggle="yes"> 200/2 people will be selected to be saved.</italic>&#8221;</p><p id="Par105">2) Self-age awareness and age-related risky choice<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>: Two decision scenarios involved a young decision-maker (between 18 and 30 years old) for either saving his/her older kin (denoted as &#8220;<italic toggle="yes">YSO-6 kin</italic>&#8221;) or younger kin (denoted as &#8220;<italic toggle="yes">YSY-6 kin</italic>&#8221;). Another two decision scenarios involved a middle-aged decision-maker (between 30 and 55 years old) for either saving his/her older kin (denoted as &#8220;<italic toggle="yes">MSO-6 kin</italic>&#8221;) or younger kin (denoted as &#8220;<italic toggle="yes">MSY-6 kin</italic>&#8221;).</p><p id="Par106">Each decision scenario included one sure option and one gamble option of equal EV. The following is an example of &#8220;<italic toggle="yes">YSO-6 kin</italic>&#8221; prompted to AI chatbots:</p><p id="Par107">&#8220;<italic toggle="yes">Imagine that six people in your family</italic>,<italic toggle="yes"> including both of your parents</italic>,<italic toggle="yes"> your brother</italic>,<italic toggle="yes"> your sister</italic>,<italic toggle="yes"> your son</italic>,<italic toggle="yes"> and your daughter</italic>,<italic toggle="yes"> are infected by a fatal disease. Two alternative medical plans to treat the disease have been proposed. Assume that the exact estimates of the consequences of the plans are as follows: if Plan A is adopted</italic>,<italic toggle="yes"> two older people will be saved; if Plan B is adopted</italic>,<italic toggle="yes"> there is a one-third probability that all six of them will be saved</italic>,<italic toggle="yes"> and a two-thirds probability that none of them will be saved. I am a young human (aged between 18&#8211;30 years old). In this case</italic>,<italic toggle="yes"> which plan do you think I would choose?</italic>&#8221;</p><p id="Par108">3) Group-dependent goal settings: Humans make decisions considering their status quo and goal settings regarding different environmental givens<sup><xref ref-type="bibr" rid="CR33">33</xref>,<xref ref-type="bibr" rid="CR41">41</xref></sup>. For instance, human subjects tend to violate the dominance principle (favor the choice with a higher expected value) to take risks in saving all in a small size group, particularly when genetic relatives are involved, while they tend to follow the dominance principle in a relatively large size group<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>. This social rationality in humans reflects different goal settings in terms of different contextual premises (e.g., small group vs. large group). To explore whether AI&#8217;s predictions would be reference-based akin to humans, or just following the single-line rule across various contextual givens, we applied three positive decision framings involving six kin, six individuals, and 600 people, respectively<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup>. The following is an example involving 600 lives with a two-thirds survival probability:</p><p id="Par109">&#8220;<italic toggle="yes">Imagine that 600 people are infected by a fatal disease. Two alternative medical plans to treat the disease have been proposed. Assume that the exact scientific estimates of the consequences of the plans are as follows: if Plan A is adopted</italic>,<italic toggle="yes"> 400 people will be saved; if Plan B is adopted</italic>,<italic toggle="yes"> there is a one-third probability that all 600 people will be saved</italic>,<italic toggle="yes"> and two-thirds probability than none of them will be saved.</italic>&#8221;</p></sec><sec id="Sec25"><title>Experimental model testing</title><p id="Par110">We tested three versions of generative AI chatbots, GPT-3.5, GPT-4, and GPT-4o, from OpenAI (<ext-link ext-link-type="uri" xlink:href="https://chat.openai.com/">https://chat.openai.com/</ext-link>). All three AI chatbots generate responses based on extensive pre-training on large datasets and reinforcement learning from human feedback (RLHF) training strategy<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. Additionally, they are designed to avoid producing monotonous answers when asked similar questions, thanks to a degree of response variability (also known as &#8220;temperature&#8221;)<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. All prompts were submitted using the default sampling parameters (temperature&#8201;=&#8201;1; top_p&#8201;=&#8201;1).</p><p id="Par111">We adopted a standard method of scenario-based behavioral test of LLMs, similarly used in prompt engineering<sup><xref ref-type="bibr" rid="CR9">9</xref>&#8211;<xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR54">54</xref></sup>, to collect responses from AI chatbots by presenting each decision scenario in a single trial. Each trial was initiated in a new window to eliminate any prior context effects. We used two OpenAI Plus accounts to prompt GPT-3.5, GPT-4, and GPT-4o in a randomized order of decision scenarios via the user interface. During pilot testing, we observed that the AI chatbots occasionally either refused to decide or provided ambiguous responses (e.g., choosing both options A and B) when asked to decide for themselves (e.g., &#8220;<italic toggle="yes">In this case</italic>,<italic toggle="yes"> which plan would you favor?</italic>&#8220;). We also recognized that since LLMs are designed to simulate human reasoning, they work more smoothly when instructed to make decisions from a human perspective rather than on their own behalf.</p><p id="Par112">For each decision scenario, the AI chatbots responded directly to the choice prompt without providing explanations. We collected 100 responses for each decision scenario to match the human experimental design. Each response was collected in a separate chat session to maintain each answer&#8217;s independence so that GPT-3.5 and GPT-4 could not retain memory across sessions during our data collection period. For GPT-4o, each prompt was entered into a fresh, temporary chat window, closed immediately after the response was made. Details of the data collection for all three AI chatbots are reported in Table <xref rid="MOESM1" ref-type="media">S4</xref> (Supplementary Materials).</p><p id="Par113">We collected 4,800 responses from GPT-3.5 and GPT-4, respectively. In the Validation Study, we collected 1,600 responses from GPT-4o. In all scenarios, we compelled AI chatbots to pick only one option directly as the response in each decision scenario: &#8220;<italic toggle="yes">In this case</italic>,<italic toggle="yes"> which plan do you think I would choose? There is no right or wrong answer. Please output your answer (Plan A or B) directly without any explanation.</italic>&#8221;</p></sec><sec id="Sec26"><title>Human data benchmark</title><p id="Par114">We extracted the responses of human participants from six published experimental research papers<sup><xref ref-type="bibr" rid="CR25">25</xref>&#8211;<xref ref-type="bibr" rid="CR30">30</xref></sup> as the human data benchmark. We chose to synthesize human data from previous studies that had been replicated many times. A recent study<sup><xref ref-type="bibr" rid="CR55">55</xref></sup> replicating Tversky and Kahneman&#8217;s Prospect Theory<sup><xref ref-type="bibr" rid="CR33">33</xref></sup> across 19 countries and 13 languages further reveals a high reliability of replicating some typical human decision-making biases, such as loss aversion. In total, we incorporated data from 2,104 human participants. Details of the human data benchmark for all studies are displayed in Table <xref rid="MOESM1" ref-type="media">S5</xref> (Supplementary Materials).</p><p id="Par115">In Study 1, we included 1,554 human participants (708 males and 846 females) with a mean age of 20.5 years. In Study 2, we included 550 human participants (267 males and 283 females) with a mean age of 26.8 years. In the validation study, we included 768 human participants (384 males and 384 females) with a mean age of 20.8 years. These data were collected via a between-group experimental design, in which participants were randomly assigned to different decision scenarios, and each of them completed only one decision scenario.</p></sec><sec id="Sec27"><title>Statistical analysis</title><p id="Par116">We employed a consistent statistical analysis scheme for both studies and the validation study to compare AI predictions and human preferences in social decision-making. All statistical tests were two-tailed, and all the analyses and visualizations were conducted via R/RStudio (version 4.1.3).</p><sec id="Sec28"><title>Within-group analysis</title><p id="Par117">We conducted proportion <italic toggle="yes">Z</italic>-tests to compare choice preferences between each framing pair and within each decision scenario for humans and AI chatbots. All tests applied a continuity correction (&#949; = 1) for zero counts and yielded 95% CI from the <italic toggle="yes">Z</italic>-values. The <italic toggle="yes">p</italic>-values from each social context or decision premise were adjusted for multiple comparisons using the Benjamini-Hochberg procedure, with a false discovery rate (FDR) threshold of 0.05.</p><p id="Par118">In Study 1, we first compared positive and negative versions of each scenario to assess framing effects. We then merged both framings in the &#8220;human lives&#8221;, &#8220;public properties&#8221;, and &#8220;personal stock shares&#8221; social contexts to examine (a) how the share of risk-seeking versus risk-averse choices varied with group size (6, 60, 600, 6000) and (b) whether risk-seeking rates departed from the50% benchmark. In the Validation Study, we applied the same within-group procedure as in Study 1 to GPT-4o.</p><p id="Par119">In Study 2, we compared risk-seeking and risk-averse choices within each scenario. For &#8220;group-dependent goal&#8221; scenarios where the gamble option had lower EV, we ran pairwise proportion <italic toggle="yes">Z-</italic>tests on risk-seeking rates for three conditions (6 kin, 6 lives, 600 lives) to see if AI chatbots mirrored human sensitivity to implicit survival thresholds. We also compared risk-seeking rates between one-third and two-thirds survival-rate conditions.</p></sec><sec id="Sec29"><title>Between-group analysis</title><p id="Par120">We employed meta-analytic methods to compare the choices between the human benchmark and the two LLMs (GPT-3.5 and GPT-4) and between two LLMs in Studies 1 and 2. In the Validation Study, our between-group analysis focused on the comparisons between the human benchmark and GPT-4o. Considering the unbalanced sample sizes in some decision scenarios between AI chatbots and human participants, we chose this method. Before the between-group analysis, we applied continuity corrections (&#949; = 1) to manage zero counts in the raw data. We conducted the heterogeneity analysis via a random-effect model (REM) with the DerSimonian-Laird method to calculate the adjusted log OR for each decision scenario. Between-group <italic toggle="yes">p</italic>-values were adjusted for multiple comparisons using the Benjamini-Hochberg procedure (FDR&#8201;=&#8201;0.05).</p></sec></sec></sec><sec id="Sec31" sec-type="supplementary-material"><title>Supplementary Information</title><p>Below is the link to the electronic supplementary material.</p><p>
<supplementary-material content-type="local-data" id="MOESM1" position="float" orientation="portrait"><media xlink:href="41598_2025_17188_MOESM1_ESM.pdf" position="float" orientation="portrait"><caption><p>Supplementary Material 1</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>We thank Yining Mao for helping with the data collection.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>X.T.W. designed the research, supervised the research and data analysis, and wrote the final draft. F.X. designed the research, analyzed the data and wrote the original draft.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>All data, code, and figures are available in an online repository (https://github.com/Cellphonexf/AI-Social-Decision-Making-Turing-Tests.git).</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par121">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><mixed-citation publication-type="other">Buss, D. <italic toggle="yes">Evolutionary psychology: the new science of the mind</italic>. 6th Rev. EdRoutledge, (2019).</mixed-citation></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Wang, X. T. &amp; Lu, J. Y. <italic toggle="yes">Wisdom of Evolution and Rationalities of Decision Making</italic> (Final Ed (East China Normal Univ., 2016).</mixed-citation></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Capraro</surname><given-names>V</given-names></name><name name-style="western"><surname>Di Paolo</surname><given-names>R</given-names></name><name name-style="western"><surname>Pizziol</surname><given-names>V</given-names></name></person-group><article-title>A publicly available benchmark for assessing large Language models&#8217; ability to predict how humans balance self-interest and the interest of others</article-title><source>Sci. Rep.</source><year>2025</year><volume>15</volume><fpage>21428</fpage><pub-id pub-id-type="pmid">40595689</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-025-01715-7</pub-id><pub-id pub-id-type="pmcid">PMC12216366</pub-id></element-citation><mixed-citation id="mc-CR3" publication-type="journal">Capraro, V., Di Paolo, R. &amp; Pizziol, V. A publicly available benchmark for assessing large Language models&#8217; ability to predict how humans balance self-interest and the interest of others. <italic toggle="yes">Sci. Rep.</italic><bold>15</bold>, 21428 (2025).<pub-id pub-id-type="pmid">40595689</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-025-01715-7</pub-id><pub-id pub-id-type="pmcid">PMC12216366</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Simon, H. A. <italic toggle="yes">Administrative behavior</italic>. 4th Rev. Ed (Simon &amp; Schuster, (2013).</mixed-citation></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hamilton</surname><given-names>WD</given-names></name></person-group><article-title>The genetical evolution of social behaviour. II</article-title><source>J. Theor. Biol.</source><year>1964</year><volume>7</volume><fpage>17</fpage><lpage>52</lpage><pub-id pub-id-type="pmid">5875340</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/0022-5193(64)90039-6</pub-id></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Hamilton, W. D. The genetical evolution of social behaviour. II. <italic toggle="yes">J. Theor. Biol.</italic><bold>7</bold>, 17&#8211;52 (1964).<pub-id pub-id-type="pmid">5875340</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/0022-5193(64)90039-6</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Capraro</surname><given-names>V</given-names></name><name name-style="western"><surname>Di Paolo</surname><given-names>R</given-names></name><name name-style="western"><surname>Perc</surname><given-names>M</given-names></name><name name-style="western"><surname>Pizziol</surname><given-names>V</given-names></name></person-group><article-title>Language-based game theory in the age of artificial intelligence</article-title><source>J. R Soc. Interface</source><year>2024</year><volume>21</volume><fpage>20230720</fpage><pub-id pub-id-type="pmid">38471531</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1098/rsif.2023.0720</pub-id><pub-id pub-id-type="pmcid">PMC10932721</pub-id></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Capraro, V., Di Paolo, R., Perc, M. &amp; Pizziol, V. Language-based game theory in the age of artificial intelligence. <italic toggle="yes">J. R Soc. Interface</italic>. <bold>21</bold>, 20230720 (2024).<pub-id pub-id-type="pmid">38471531</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1098/rsif.2023.0720</pub-id><pub-id pub-id-type="pmcid">PMC10932721</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>XT</given-names></name></person-group><article-title>Risk communication and risky choice in context: ambiguity and ambivalence hypothesis</article-title><source>Ann. N Y Acad. Sci.</source><year>2008</year><volume>1128</volume><issue>1</issue><fpage>78</fpage><lpage>89</lpage><pub-id pub-id-type="pmid">18469216</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1196/annals.1399.009</pub-id></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Wang, X. T. Risk communication and risky choice in context: ambiguity and ambivalence hypothesis. <italic toggle="yes">Ann. N Y Acad. Sci.</italic><bold>1128</bold> (1), 78&#8211;89 (2008).<pub-id pub-id-type="pmid">18469216</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1196/annals.1399.009</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hagendorff</surname><given-names>T</given-names></name><name name-style="western"><surname>Fabi</surname><given-names>S</given-names></name><name name-style="western"><surname>Kosinski</surname><given-names>M</given-names></name></person-group><article-title>Human-like intuitive behavior and reasoning biases emerged in large Language models but disappeared in ChatGPT</article-title><source>Nat. Comput. Sci.</source><year>2023</year><volume>3</volume><fpage>833</fpage><lpage>838</lpage><pub-id pub-id-type="pmid">38177754</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s43588-023-00527-x</pub-id><pub-id pub-id-type="pmcid">PMC10766525</pub-id></element-citation><mixed-citation id="mc-CR8" publication-type="journal">Hagendorff, T., Fabi, S. &amp; Kosinski, M. Human-like intuitive behavior and reasoning biases emerged in large Language models but disappeared in ChatGPT. <italic toggle="yes">Nat. Comput. Sci.</italic><bold>3</bold>, 833&#8211;838 (2023).<pub-id pub-id-type="pmid">38177754</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s43588-023-00527-x</pub-id><pub-id pub-id-type="pmcid">PMC10766525</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Suri</surname><given-names>G</given-names></name><name name-style="western"><surname>Slater</surname><given-names>LR</given-names></name><name name-style="western"><surname>Ziaee</surname><given-names>A</given-names></name><name name-style="western"><surname>Nguyen</surname><given-names>M</given-names></name></person-group><article-title>Do large Language models show decision heuristics similar to humans? A case study using GPT-3.5</article-title><source>J. Exp. Psychol. Gen.</source><year>2024</year><volume>153</volume><fpage>1066</fpage><lpage>1075</lpage><pub-id pub-id-type="pmid">38330366</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1037/xge0001547</pub-id></element-citation><mixed-citation id="mc-CR9" publication-type="journal">Suri, G., Slater, L. R., Ziaee, A. &amp; Nguyen, M. Do large Language models show decision heuristics similar to humans? A case study using GPT-3.5. <italic toggle="yes">J. Exp. Psychol. Gen.</italic><bold>153</bold>, 1066&#8211;1075 (2024).<pub-id pub-id-type="pmid">38330366</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1037/xge0001547</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>YK</given-names></name><name name-style="western"><surname>Huang</surname><given-names>Z</given-names></name><name name-style="western"><surname>Seligman</surname><given-names>M</given-names></name><name name-style="western"><surname>Peng</surname><given-names>KP</given-names></name></person-group><article-title>Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><fpage>7095</fpage><pub-id pub-id-type="pmid">38528008</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-024-55949-y</pub-id><pub-id pub-id-type="pmcid">PMC10963757</pub-id></element-citation><mixed-citation id="mc-CR10" publication-type="journal">Zhao, Y. K., Huang, Z., Seligman, M. &amp; Peng, K. P. Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots. <italic toggle="yes">Sci. Rep.</italic><bold>14</bold>, 7095 (2024).<pub-id pub-id-type="pmid">38528008</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-024-55949-y</pub-id><pub-id pub-id-type="pmcid">PMC10963757</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Strachan</surname><given-names>JW</given-names></name><etal/></person-group><article-title>Testing theory of Mind in large Language models and humans</article-title><source>Nat. Hum. Behav.</source><year>2024</year><volume>8</volume><fpage>1285</fpage><lpage>1295</lpage><pub-id pub-id-type="pmid">38769463</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41562-024-01882-z</pub-id><pub-id pub-id-type="pmcid">PMC11272575</pub-id></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Strachan, J. W. et al. Testing theory of Mind in large Language models and humans. <italic toggle="yes">Nat. Hum. Behav.</italic><bold>8</bold>, 1285&#8211;1295 (2024).<pub-id pub-id-type="pmid">38769463</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41562-024-01882-z</pub-id><pub-id pub-id-type="pmcid">PMC11272575</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Wei, J. et al. Emergent abilities of large language models. Preprint at (2022). 10.48550/arXiv.2206.07682</mixed-citation></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fedorenko</surname><given-names>E</given-names></name><name name-style="western"><surname>Piantadosi</surname><given-names>ST</given-names></name><name name-style="western"><surname>Gibson</surname><given-names>EA</given-names></name></person-group><article-title>Language is primarily a tool for communication rather than thought</article-title><source>Nature</source><year>2024</year><volume>630</volume><fpage>575</fpage><lpage>586</lpage><pub-id pub-id-type="pmid">38898296</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41586-024-07522-w</pub-id></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Fedorenko, E., Piantadosi, S. T. &amp; Gibson, E. A. Language is primarily a tool for communication rather than thought. <italic toggle="yes">Nature</italic><bold>630</bold>, 575&#8211;586 (2024).<pub-id pub-id-type="pmid">38898296</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41586-024-07522-w</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Bauer, K., Liebich, L., Hinz, O. &amp; Kosfeld, M. Decoding GPT&#8217;s hidden rationality of cooperation. SAFE Working Paper 401Leibniz Institute for Financial Research SAFE, SSRN (2023). <ext-link ext-link-type="uri" xlink:href="https://ssrn.com/abstract=4576036">https://ssrn.com/abstract=4576036</ext-link></mixed-citation></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brookins</surname><given-names>P</given-names></name><name name-style="western"><surname>DeBacker</surname><given-names>JM</given-names></name></person-group><article-title>Playing games with GPT: what can we learn about a large Language model from canonical strategic games</article-title><source>Econ. Bull.</source><year>2024</year><volume>44</volume><fpage>25</fpage><lpage>37</lpage></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Brookins, P. &amp; DeBacker, J. M. Playing games with GPT: what can we learn about a large Language model from canonical strategic games. <italic toggle="yes">Econ. Bull.</italic><bold>44</bold>, 25&#8211;37 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>F</given-names></name></person-group><article-title>GPT in game theory experiments</article-title><source>Preprint At.</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arXiv.2305.05516</pub-id></element-citation><mixed-citation id="mc-CR16" publication-type="journal">Guo, F. GPT in game theory experiments. <italic toggle="yes">Preprint At.</italic>10.48550/arXiv.2305.05516 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Akata, E. et al. Playing repeated games with large Language models. <italic toggle="yes">Nat Hum. Behav</italic>, 1&#8211;11 (2025).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41562-025-02172-y</pub-id><pub-id pub-id-type="pmcid">PMC12283376</pub-id><pub-id pub-id-type="pmid">40341716</pub-id></mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Aher, G., Arriaga, R. I. &amp; Kalai, A. T. Using large language models to simulate multiple humans and replicate human subject studies. <italic toggle="yes">Proc. 40th Int. Conf. Mach. Learn.</italic>, PMLR 202 (2023).</mixed-citation></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dillion</surname><given-names>D</given-names></name><name name-style="western"><surname>Tandon</surname><given-names>N</given-names></name><name name-style="western"><surname>Gu</surname><given-names>Y</given-names></name><name name-style="western"><surname>Gray</surname><given-names>K</given-names></name></person-group><article-title>Can AI Language models replace human participants?</article-title><source>Trends Cogn. Sci.</source><year>2023</year><volume>27</volume><fpage>597</fpage><lpage>600</lpage><pub-id pub-id-type="pmid">37173156</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.tics.2023.04.008</pub-id></element-citation><mixed-citation id="mc-CR19" publication-type="journal">Dillion, D., Tandon, N., Gu, Y. &amp; Gray, K. Can AI Language models replace human participants? <italic toggle="yes">Trends Cogn. Sci.</italic><bold>27</bold>, 597&#8211;600 (2023).<pub-id pub-id-type="pmid">37173156</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.tics.2023.04.008</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schmidt</surname><given-names>EM</given-names></name><name name-style="western"><surname>Bonati</surname><given-names>S</given-names></name><name name-style="western"><surname>K&#246;bis</surname><given-names>N</given-names></name><name name-style="western"><surname>Soraperra</surname><given-names>I</given-names></name></person-group><article-title>GPT-3.5 altruistic advice is sensitive to reciprocal concerns but not to strategic risk</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><fpage>22274</fpage><pub-id pub-id-type="pmid">39333331</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-024-73306-x</pub-id><pub-id pub-id-type="pmcid">PMC11436787</pub-id></element-citation><mixed-citation id="mc-CR20" publication-type="journal">Schmidt, E. M., Bonati, S., K&#246;bis, N. &amp; Soraperra, I. GPT-3.5 altruistic advice is sensitive to reciprocal concerns but not to strategic risk. <italic toggle="yes">Sci. Rep.</italic><bold>14</bold>, 22274 (2024).<pub-id pub-id-type="pmid">39333331</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-024-73306-x</pub-id><pub-id pub-id-type="pmcid">PMC11436787</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">Kasberger, B., Martin, S., Normann, H. T. &amp; Werner, T. Algorithmic Cooperation. <italic toggle="yes">Available SSRN</italic><bold>4389647</bold> (2023).</mixed-citation></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xie</surname><given-names>C</given-names></name><etal/></person-group><article-title>Can large Language model agents simulate human trust behavior?</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2024</year><volume>37</volume><fpage>15674</fpage><lpage>15729</lpage></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Xie, C. et al. Can large Language model agents simulate human trust behavior? <italic toggle="yes">Adv. Neural Inf. Process. Syst.</italic><bold>37</bold>, 15674&#8211;15729 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kaiser</surname><given-names>M</given-names></name><etal/></person-group><article-title>Leveraging LLMs for predictive insights in food policy and behavioral interventions</article-title><source>Preprint At.</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2411.08563</pub-id></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Kaiser, M. et al. Leveraging LLMs for predictive insights in food policy and behavioral interventions. <italic toggle="yes">Preprint At.</italic>10.48550/arXiv.2411.08563 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>JQ</given-names></name><name name-style="western"><surname>Yan</surname><given-names>H</given-names></name><name name-style="western"><surname>Griffiths</surname><given-names>TL</given-names></name></person-group><article-title>Language models trained to do arithmetic predict human risky and intertemporal choice</article-title><source>Preprint At.</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2405.19313</pub-id></element-citation><mixed-citation id="mc-CR24" publication-type="journal">Zhu, J. Q., Yan, H. &amp; Griffiths, T. L. Language models trained to do arithmetic predict human risky and intertemporal choice. <italic toggle="yes">Preprint At.</italic>10.48550/arXiv.2405.19313 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>XT</given-names></name><name name-style="western"><surname>Simons</surname><given-names>F</given-names></name><name name-style="western"><surname>Br&#233;dart</surname><given-names>S</given-names></name></person-group><article-title>Social cues and verbal framing in risky choice</article-title><source>J. Behav. Decis. Mak.</source><year>2001</year><volume>14</volume><fpage>1</fpage><lpage>15</lpage></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Wang, X. T., Simons, F. &amp; Br&#233;dart, S. Social cues and verbal framing in risky choice. <italic toggle="yes">J. Behav. Decis. Mak.</italic><bold>14</bold>, 1&#8211;15 (2001).</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>XT</given-names></name></person-group><article-title>Framing effects: dynamics and task domains</article-title><source>Organ. Behav. Hum. Decis. Process.</source><year>1996</year><volume>68</volume><fpage>145</fpage><lpage>157</lpage><pub-id pub-id-type="pmid">8954876</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1006/obhd.1996.0095</pub-id></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Wang, X. T. Framing effects: dynamics and task domains. <italic toggle="yes">Organ. Behav. Hum. Decis. Process.</italic><bold>68</bold>, 145&#8211;157 (1996).<pub-id pub-id-type="pmid">8954876</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1006/obhd.1996.0095</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>XT</given-names></name></person-group><article-title>Domain-specific rationality in human choices: violations of utility axioms and social contexts</article-title><source>Cognition</source><year>1996</year><volume>60</volume><fpage>31</fpage><lpage>63</lpage><pub-id pub-id-type="pmid">8766389</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/0010-0277(95)00700-8</pub-id></element-citation><mixed-citation id="mc-CR27" publication-type="journal">Wang, X. T. Domain-specific rationality in human choices: violations of utility axioms and social contexts. <italic toggle="yes">Cognition</italic><bold>60</bold>, 31&#8211;63 (1996).<pub-id pub-id-type="pmid">8766389</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/0010-0277(95)00700-8</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>XT</given-names></name></person-group><article-title>Evolutionary hypotheses of risk-sensitive choice: age differences and perspective change</article-title><source>Ethol. Sociobiol.</source><year>1996</year><volume>17</volume><fpage>1</fpage><lpage>15</lpage></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Wang, X. T. Evolutionary hypotheses of risk-sensitive choice: age differences and perspective change. <italic toggle="yes">Ethol. Sociobiol.</italic><bold>17</bold>, 1&#8211;15 (1996).</mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>XT</given-names></name><name name-style="western"><surname>Johnston</surname><given-names>VS</given-names></name></person-group><article-title>Perceived social context and risk preference: a re-examination of framing effects in a life&#8208;death decision problem</article-title><source>J. Behav. Decis. Mak.</source><year>1995</year><volume>8</volume><fpage>279</fpage><lpage>293</lpage></element-citation><mixed-citation id="mc-CR29" publication-type="journal">Wang, X. T. &amp; Johnston, V. S. Perceived social context and risk preference: a re-examination of framing effects in a life&#8208;death decision problem. <italic toggle="yes">J. Behav. Decis. Mak.</italic><bold>8</bold>, 279&#8211;293 (1995).</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>XT</given-names></name></person-group><article-title>Risk as reproductive variance</article-title><source>Evol. Hum. Behav.</source><year>2002</year><volume>23</volume><fpage>35</fpage><lpage>57</lpage></element-citation><mixed-citation id="mc-CR30" publication-type="journal">Wang, X. T. Risk as reproductive variance. <italic toggle="yes">Evol. Hum. Behav.</italic><bold>23</bold>, 35&#8211;57 (2002).</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>XT</given-names></name></person-group><article-title>Emotions within reason: resolving conflicts in risk preference</article-title><source>Cogn. Emot.</source><year>2006</year><volume>20</volume><fpage>1132</fpage><lpage>1152</lpage></element-citation><mixed-citation id="mc-CR31" publication-type="journal">Wang, X. T. Emotions within reason: resolving conflicts in risk preference. <italic toggle="yes">Cogn. Emot.</italic><bold>20</bold>, 1132&#8211;1152 (2006).</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Levin</surname><given-names>IP</given-names></name><name name-style="western"><surname>Schneider</surname><given-names>SL</given-names></name><name name-style="western"><surname>Gaeth</surname><given-names>GJ</given-names></name></person-group><article-title>All frames are not created equal: a typology and critical analysis of framing effects</article-title><source>Organ. Behav. Hum. Decis. Process.</source><year>1998</year><volume>76</volume><issue>2</issue><fpage>149</fpage><lpage>188</lpage><pub-id pub-id-type="pmid">9831520</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1006/obhd.1998.2804</pub-id></element-citation><mixed-citation id="mc-CR32" publication-type="journal">Levin, I. P., Schneider, S. L. &amp; Gaeth, G. J. All frames are not created equal: a typology and critical analysis of framing effects. <italic toggle="yes">Organ. Behav. Hum. Decis. Process.</italic><bold>76</bold> (2), 149&#8211;188 (1998).<pub-id pub-id-type="pmid">9831520</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1006/obhd.1998.2804</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tversky</surname><given-names>A</given-names></name><name name-style="western"><surname>Kahneman</surname><given-names>D</given-names></name></person-group><article-title>The framing of decisions and the rationality of choice</article-title><source>Science</source><year>1981</year><volume>211</volume><fpage>453</fpage><lpage>458</lpage><pub-id pub-id-type="pmid">7455683</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1126/science.7455683</pub-id></element-citation><mixed-citation id="mc-CR33" publication-type="journal">Tversky, A. &amp; Kahneman, D. The framing of decisions and the rationality of choice. <italic toggle="yes">Science</italic><bold>211</bold>, 453&#8211;458 (1981).<pub-id pub-id-type="pmid">7455683</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1126/science.7455683</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Kahneman, D. &amp; Tversky, A. <italic toggle="yes">Handbook of the Fundamentals of Financial Decision Making: Part I</italic> (Final Ed (World Scientific, 2013).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Miller, G. <italic toggle="yes">The mating mind: how sexual choice shaped the evolution of human nature</italic>. 2nd Rev. EdAnchor, (2001).</mixed-citation></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shamay-Tsoory</surname><given-names>SG</given-names></name><name name-style="western"><surname>Saporta</surname><given-names>N</given-names></name><name name-style="western"><surname>Marton-Alper</surname><given-names>IZ</given-names></name><name name-style="western"><surname>Gvirts</surname><given-names>HZ</given-names></name></person-group><article-title>Herding brains: a core neural mechanism for social alignment</article-title><source>Trends Cogn. Sci.</source><year>2019</year><volume>23</volume><fpage>174</fpage><lpage>186</lpage><pub-id pub-id-type="pmid">30679099</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.tics.2019.01.002</pub-id></element-citation><mixed-citation id="mc-CR36" publication-type="journal">Shamay-Tsoory, S. G., Saporta, N., Marton-Alper, I. Z. &amp; Gvirts, H. Z. Herding brains: a core neural mechanism for social alignment. <italic toggle="yes">Trends Cogn. Sci.</italic><bold>23</bold>, 174&#8211;186 (2019).<pub-id pub-id-type="pmid">30679099</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.tics.2019.01.002</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Raafat</surname><given-names>RM</given-names></name><name name-style="western"><surname>Chater</surname><given-names>N</given-names></name><name name-style="western"><surname>Frith</surname><given-names>C</given-names></name></person-group><article-title>Herding in humans</article-title><source>Trends Cogn. Sci.</source><year>2009</year><volume>13</volume><fpage>420</fpage><lpage>428</lpage><pub-id pub-id-type="pmid">19748818</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.tics.2009.08.002</pub-id></element-citation><mixed-citation id="mc-CR37" publication-type="journal">Raafat, R. M., Chater, N. &amp; Frith, C. Herding in humans. <italic toggle="yes">Trends Cogn. Sci.</italic><bold>13</bold>, 420&#8211;428 (2009).<pub-id pub-id-type="pmid">19748818</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.tics.2009.08.002</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rosati</surname><given-names>AG</given-names></name></person-group><article-title>Foraging cognition: reviving the ecological intelligence hypothesis</article-title><source>Trends Cogn. Sci.</source><year>2017</year><volume>21</volume><fpage>691</fpage><lpage>702</lpage><pub-id pub-id-type="pmid">28625354</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.tics.2017.05.011</pub-id></element-citation><mixed-citation id="mc-CR38" publication-type="journal">Rosati, A. G. Foraging cognition: reviving the ecological intelligence hypothesis. <italic toggle="yes">Trends Cogn. Sci.</italic><bold>21</bold>, 691&#8211;702 (2017).<pub-id pub-id-type="pmid">28625354</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.tics.2017.05.011</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Brown</surname><given-names>T</given-names></name><etal/></person-group><article-title>Language models are few-shot learners</article-title><source>Adv. Neural Inf. Process. Syst.</source><year>2020</year><volume>33</volume><fpage>1877</fpage><lpage>1901</lpage></element-citation><mixed-citation id="mc-CR39" publication-type="journal">Brown, T. et al. Language models are few-shot learners. <italic toggle="yes">Adv. Neural Inf. Process. Syst.</italic><bold>33</bold>, 1877&#8211;1901 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Achiam, J. et al. GPT-4 technical report. Preprint at (2023). 10.48550/arXiv.2303.08774</mixed-citation></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>XT</given-names></name><name name-style="western"><surname>Johnson</surname><given-names>JG</given-names></name></person-group><article-title>A tri-reference point theory of decision making under risk</article-title><source>J. Exp. Psychol. Gen.</source><year>2012</year><volume>141</volume><fpage>743</fpage><lpage>756</lpage><pub-id pub-id-type="pmid">22390265</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1037/a0027415</pub-id></element-citation><mixed-citation id="mc-CR41" publication-type="journal">Wang, X. T. &amp; Johnson, J. G. A tri-reference point theory of decision making under risk. <italic toggle="yes">J. Exp. Psychol. Gen.</italic><bold>141</bold>, 743&#8211;756 (2012).<pub-id pub-id-type="pmid">22390265</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1037/a0027415</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vaswani</surname><given-names>A</given-names></name><etal/></person-group><article-title>Attention is all you need</article-title><source>Preprint At.</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arXiv.1706.03762</pub-id></element-citation><mixed-citation id="mc-CR42" publication-type="journal">Vaswani, A. et al. Attention is all you need. <italic toggle="yes">Preprint At.</italic>10.48550/arXiv.1706.03762 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Markov</surname><given-names>T</given-names></name><etal/></person-group><article-title>A holistic approach to undesired content detection in the real world</article-title><source>Proc. AAAI Conf. Artif. Intell.</source><year>2023</year><volume>37</volume><fpage>15009</fpage><lpage>15018</lpage></element-citation><mixed-citation id="mc-CR43" publication-type="journal">Markov, T. et al. A holistic approach to undesired content detection in the real world. <italic toggle="yes">Proc. AAAI Conf. Artif. Intell.</italic><bold>37</bold>, 15009&#8211;15018 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><citation-alternatives><element-citation id="ec-CR44" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mozikov</surname><given-names>M</given-names></name><etal/></person-group><article-title>The good, the bad, and the hulk-like GPT: analyzing emotional decisions of large Language models in Cooperation and bargaining games</article-title><source>Preprint At.</source><year>2024</year><pub-id pub-id-type="doi">10.48550/arXiv.2406.03299</pub-id></element-citation><mixed-citation id="mc-CR44" publication-type="journal">Mozikov, M. et al. The good, the bad, and the hulk-like GPT: analyzing emotional decisions of large Language models in Cooperation and bargaining games. <italic toggle="yes">Preprint At.</italic>10.48550/arXiv.2406.03299 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Ma, A. &amp; Powell, D. Can large language models predict associations among human attitudes? Preprint at (2025). 10.48550/arXiv.2503.21011</mixed-citation></ref><ref id="CR46"><label>46.</label><mixed-citation publication-type="other">Lehr, S. A., Saichandran, K. S., Harmon-Jones, E., Vitali, N. &amp; Banaji, M. R. Kernels of selfhood: GPT-4o shows human-like patterns of cognitive dissonance moderated by free choice. <italic toggle="yes">Proc. Natl Acad. Sci.</italic> 122, e2501823122 (2025).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1073/pnas.2501823122</pub-id><pub-id pub-id-type="pmcid">PMC12107184</pub-id><pub-id pub-id-type="pmid">40366689</pub-id></mixed-citation></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Trivers</surname><given-names>RL</given-names></name></person-group><article-title>The evolution of reciprocal altruism</article-title><source>Q. Rev. Biol.</source><year>1971</year><volume>46</volume><issue>1</issue><fpage>35</fpage><lpage>57</lpage></element-citation><mixed-citation id="mc-CR47" publication-type="journal">Trivers, R. L. The evolution of reciprocal altruism. <italic toggle="yes">Q. Rev. Biol.</italic><bold>46</bold> (1), 35&#8211;57 (1971).</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><mixed-citation publication-type="other">Kahneman, D. Thinking, fast and slow. <italic toggle="yes">Farrar Straus Giroux</italic> (2011).</mixed-citation></ref><ref id="CR49"><label>49.</label><citation-alternatives><element-citation id="ec-CR49" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yilmaz</surname><given-names>R</given-names></name><name name-style="western"><surname>Yilmaz</surname><given-names>FGK</given-names></name></person-group><article-title>Augmented intelligence in programming learning: examining student views on the use of ChatGPT for programming learning</article-title><source>Comput. Hum. Behav.</source><year>2023</year><volume>1</volume><fpage>100005</fpage></element-citation><mixed-citation id="mc-CR49" publication-type="journal">Yilmaz, R. &amp; Yilmaz, F. G. K. Augmented intelligence in programming learning: examining student views on the use of ChatGPT for programming learning. <italic toggle="yes">Comput. Hum. Behav.</italic><bold>1</bold>, 100005 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR50"><label>50.</label><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rahman</surname><given-names>MM</given-names></name><name name-style="western"><surname>Watanobe</surname><given-names>Y</given-names></name></person-group><article-title>ChatGPT for education and research: opportunities, threats, and strategies</article-title><source>Appl. Sci.</source><year>2023</year><volume>13</volume><fpage>5783</fpage></element-citation><mixed-citation id="mc-CR50" publication-type="journal">Rahman, M. M. &amp; Watanobe, Y. ChatGPT for education and research: opportunities, threats, and strategies. <italic toggle="yes">Appl. Sci.</italic><bold>13</bold>, 5783 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tabone</surname><given-names>W</given-names></name><name name-style="western"><surname>De Winter</surname><given-names>J</given-names></name></person-group><article-title>Using ChatGPT for human-computer interaction research: a primer</article-title><source>R Soc. Open. Sci.</source><year>2023</year><volume>10</volume><fpage>231053</fpage><pub-id pub-id-type="pmid">37711151</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1098/rsos.231053</pub-id><pub-id pub-id-type="pmcid">PMC10498031</pub-id></element-citation><mixed-citation id="mc-CR51" publication-type="journal">Tabone, W. &amp; De Winter, J. Using ChatGPT for human-computer interaction research: a primer. <italic toggle="yes">R Soc. Open. Sci.</italic><bold>10</bold>, 231053 (2023).<pub-id pub-id-type="pmid">37711151</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1098/rsos.231053</pub-id><pub-id pub-id-type="pmcid">PMC10498031</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><citation-alternatives><element-citation id="ec-CR52" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hassani</surname><given-names>H</given-names></name><name name-style="western"><surname>Silva</surname><given-names>ES</given-names></name></person-group><article-title>The role of ChatGPT in data science: how AI-assisted conversational interfaces are revolutionizing the field</article-title><source>Big Data Cogn. Comput.</source><year>2023</year><volume>7</volume><fpage>62</fpage></element-citation><mixed-citation id="mc-CR52" publication-type="journal">Hassani, H. &amp; Silva, E. S. The role of ChatGPT in data science: how AI-assisted conversational interfaces are revolutionizing the field. <italic toggle="yes">Big Data Cogn. Comput.</italic><bold>7</bold>, 62 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR53"><label>53.</label><citation-alternatives><element-citation id="ec-CR53" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Frank</surname><given-names>MC</given-names></name></person-group><article-title>Openly accessible LLMs can help Us to understand human cognition</article-title><source>Nat. Hum. Behav.</source><year>2023</year><volume>7</volume><fpage>1825</fpage><lpage>1827</lpage><pub-id pub-id-type="pmid">37985910</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41562-023-01732-4</pub-id></element-citation><mixed-citation id="mc-CR53" publication-type="journal">Frank, M. C. Openly accessible LLMs can help Us to understand human cognition. <italic toggle="yes">Nat. Hum. Behav.</italic><bold>7</bold>, 1825&#8211;1827 (2023).<pub-id pub-id-type="pmid">37985910</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41562-023-01732-4</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR54"><label>54.</label><citation-alternatives><element-citation id="ec-CR54" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>Q</given-names></name><etal/></person-group><article-title>Does conceptual representation require embodiment? Insights from large Language models</article-title><source>Preprint At.</source><year>2023</year><pub-id pub-id-type="doi">10.48550/arXiv.2305.19103</pub-id></element-citation><mixed-citation id="mc-CR54" publication-type="journal">Xu, Q. et al. Does conceptual representation require embodiment? Insights from large Language models. <italic toggle="yes">Preprint At.</italic>10.48550/arXiv.2305.19103 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR55"><label>55.</label><citation-alternatives><element-citation id="ec-CR55" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ruggeri</surname><given-names>K</given-names></name><etal/></person-group><article-title>Replicating patterns of prospect theory for decision under risk</article-title><source>Nat. Hum. Behav.</source><year>2020</year><volume>4</volume><fpage>622</fpage><lpage>633</lpage><pub-id pub-id-type="pmid">32424259</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41562-020-0886-x</pub-id></element-citation><mixed-citation id="mc-CR55" publication-type="journal">Ruggeri, K. et al. Replicating patterns of prospect theory for decision under risk. <italic toggle="yes">Nat. Hum. Behav.</italic><bold>4</bold>, 622&#8211;633 (2020).<pub-id pub-id-type="pmid">32424259</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41562-020-0886-x</pub-id></mixed-citation></citation-alternatives></ref></ref-list></back></article>
        
    </metadata>
</record>
    </GetRecord>

</OAI-PMH>