


<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
    <responseDate>2025-09-09T13:51:50Z</responseDate>
    <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:12405791" metadataPrefix="pmc">https://pmc.ncbi.nlm.nih.gov/api/oai/v1/mh/</request>
    
    <GetRecord>
        <record>
    <header>
    <identifier>oai:pubmedcentral.nih.gov:12405791</identifier>
    <datestamp>2025-09-04</datestamp>
    
        
        <setSpec>medinform</setSpec>
        
    
        
        <setSpec>pmc-open</setSpec>
        
    
</header>
    <metadata>
        
        <article xmlns="https://jats.nlm.nih.gov/ns/archiving/1.4/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.4/ https://jats.nlm.nih.gov/archiving/1.4/xsd/JATS-archivearticle1-4.xsd" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">JMIR Med Inform</journal-id><journal-id journal-id-type="iso-abbrev">JMIR Med Inform</journal-id><journal-id journal-id-type="pmc-domain-id">2655</journal-id><journal-id journal-id-type="pmc-domain">medinform</journal-id><journal-id journal-id-type="publisher-id">JMI</journal-id><journal-title-group><journal-title>JMIR Medical Informatics</journal-title></journal-title-group><issn pub-type="epub">2291-9694</issn><publisher><publisher-name>JMIR Publications Inc.</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12405791</article-id><article-id pub-id-type="pmcid-ver">PMC12405791.1</article-id><article-id pub-id-type="pmcaid">12405791</article-id><article-id pub-id-type="pmcaiid">12405791</article-id><article-id pub-id-type="pmid">40828572</article-id><article-id pub-id-type="doi">10.2196/75022</article-id><article-id pub-id-type="publisher-id">v13i1e75022</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Original Paper</subject></subj-group><subj-group subj-group-type="article-type"><subject>Original Paper</subject></subj-group></article-categories><title-group><article-title>Deep Learning and Image Generator Health Tabular Data (IGHT) for Predicting Overall Survival in Patients With Colorectal Cancer: Retrospective Study</article-title></title-group><contrib-group><contrib contrib-type="editor"><name name-style="western"><surname>Coristine</surname><given-names initials="A">Andrew</given-names></name></contrib></contrib-group><contrib-group><contrib contrib-type="reviewer"><name name-style="western"><surname>Woo</surname><given-names initials="H">Hyekyung</given-names></name></contrib><contrib contrib-type="reviewer"><name name-style="western"><surname>Chushig-Muzo</surname><given-names initials="D">David</given-names></name></contrib><contrib contrib-type="reviewer"><name name-style="western"><surname>Polamuri</surname><given-names initials="SR">Subba Rao</given-names></name></contrib><contrib contrib-type="reviewer"><name name-style="western"><surname>Bai</surname><given-names initials="E">Enze</given-names></name></contrib></contrib-group><contrib-group><contrib id="contrib1" contrib-type="author"><name name-style="western"><surname>Oh</surname><given-names initials="SH">Seo Hyun</given-names></name><degrees>BS</degrees><xref rid="aff1" ref-type="aff">1</xref><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-8047-3032</contrib-id></contrib><contrib id="contrib2" contrib-type="author" corresp="yes" equal-contrib="yes"><name name-style="western"><surname>Lee</surname><given-names initials="Y">Youngho</given-names></name><degrees>PhD</degrees><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0003-0720-0569</contrib-id><xref rid="aff2" ref-type="aff">2</xref><address><institution>Department of Computer Engineering</institution><institution>Gachon University</institution><addr-line>1342, Seongnam-daero</addr-line><addr-line>Sung-nam si, 13120</addr-line><country>Republic of Korea</country><fax>82 31 750 5743</fax><phone>82 31 750 5011</phone><email>lyh@gachon.ac.kr</email></address></contrib><contrib id="contrib3" contrib-type="author" equal-contrib="yes"><name name-style="western"><surname>Baek</surname><given-names initials="JH">Jeong-Heum</given-names></name><degrees>MD, PhD</degrees><xref rid="aff3" ref-type="aff">3</xref><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0001-9124-8041</contrib-id></contrib><contrib id="contrib4" contrib-type="author"><name name-style="western"><surname>Sunwoo</surname><given-names initials="W">Woongsang</given-names></name><degrees>MD</degrees><xref rid="aff4" ref-type="aff">4</xref><contrib-id contrib-id-type="orcid" authenticated="false">https://orcid.org/0000-0002-0259-8654</contrib-id></contrib></contrib-group><aff id="aff1">
<label>1</label>
<institution>Department of IT Convergence</institution>
<institution>Gachon University</institution>
<addr-line>1342, Seongnam-daero, Sung-nam si</addr-line>
<country>Republic of Korea</country>
</aff><aff id="aff2">
<label>2</label>
<institution>Department of Computer Engineering</institution>
<institution>Gachon University</institution>
<addr-line>Sung-nam si</addr-line>
<country>Republic of Korea</country>
</aff><aff id="aff3">
<label>3</label>
<institution>Division of Colon and Rectal Surgery</institution>
<institution>Department of Surgery</institution>
<institution>Gil Medical Center</institution>
<addr-line>21, Namdong-daero 774beon-gil, Incheon</addr-line>
<country>Republic of Korea</country>
</aff><aff id="aff4">
<label>4</label>
<institution>Department of Otorhinolaryngology-Head and Neck Surgery</institution>
<institution>Gachon University College of Medicine</institution>
<institution>Gil Medical Center</institution>
<addr-line>21, Namdong-daero 774beon-gil, Incheon</addr-line>
<country>Republic of Korea</country>
</aff><author-notes><corresp>Corresponding Author: Youngho Lee <email>lyh@gachon.ac.kr</email></corresp></author-notes><pub-date pub-type="collection"><year>2025</year></pub-date><pub-date pub-type="epub"><day>19</day><month>8</month><year>2025</year></pub-date><volume>13</volume><issue-id pub-id-type="pmc-issue-id">479644</issue-id><elocation-id>e75022</elocation-id><history><date date-type="received"><day>30</day><month>3</month><year>2025</year></date><date date-type="rev-request"><day>28</day><month>4</month><year>2025</year></date><date date-type="rev-recd"><day>23</day><month>6</month><year>2025</year></date><date date-type="accepted"><day>31</day><month>7</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>19</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-04 00:25:59.930"><day>04</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169;Seo Hyun Oh, Youngho Lee, Jeong-Heum Baek, Woongsang Sunwoo. Originally published in JMIR Medical Informatics (https://medinform.jmir.org), 19.08.2025.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (<ext-link xlink:href="https://creativecommons.org/licenses/by/4.0/" ext-link-type="uri">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Medical Informatics, is properly cited. The complete bibliographic information, a link to the original publication on <ext-link xlink:href="https://medinform.jmir.org/" ext-link-type="uri">https://medinform.jmir.org/</ext-link>, as well as this copyright and license information must be included.</license-p></license></permissions><self-uri content-type="pmc-pdf" xlink:href="medinform_v13i1e75022.pdf"/><self-uri xlink:href="https://medinform.jmir.org/2025/1/e75022"/><abstract><sec sec-type="background"><title>Background</title><p>Recent advances in artificial intelligence (AI) have contributed to improved predictive modeling in health care, particularly in oncology. Traditional methods often rely on structured tabular data, but these approaches can struggle to capture complex interactions among clinical variables. Image generator for health tabular data (IGHT) transform tabular electronic medical record (EMR) data into structured 2D image matrices, enabling the use of powerful computer vision&#8211;based deep learning models. This approach offers a novel baseline for survival prediction in colorectal cancer by leveraging spatial encoding of clinical features, potentially enhancing prognostic accuracy and interpretability.</p></sec><sec sec-type="objective"><title>Objective</title><p>This study aimed to develop and evaluate a deep learning model using EMR data to predict 5-year overall survival in patients with colorectal cancer and to examine the clinical interpretability of model predictions using explainable artificial intelligence (XAI) techniques.</p></sec><sec sec-type="methods"><title>Methods</title><p>Anonymized EMR data of 3321 patients at the Gil Medical Center were analyzed. The dataset included demographic details, tumor characteristics, laboratory values, treatment modalities, and follow-up outcomes. Clinical variables were converted into 2D image matrices using the IGHT. Patients were stratified into colon and rectal cancer groups to account for biological and prognostic differences. Three models were developed and compared: a conventional artificial neural network (ANN), a basic convolutional neural network (CNN), and a transfer learning&#8211;based Visual Geometry Group (VGG)16 model. Model performance was assessed using accuracy, sensitivity, specificity, precision, and F1-scores. To interpret model decisions, gradient-weighted class activation mapping (Grad-CAM) was applied to visualize regions of the input images that contributed most to predictions, enabling identification of key prognostic features.</p></sec><sec sec-type="results"><title>Results</title><p>Among the tested models, VGG16 exhibited superior predictive performance, achieving an accuracy of 78.44% for colon cancer and 74.83% for rectal cancer. It showed notably high specificity (89.55% for colon cancer and 87.9% for rectal cancer), indicating strong reliability in correctly identifying patients likely to survive beyond 5 years. Compared to ANN and CNN models, VGG16 achieved a better balance between sensitivity and specificity, demonstrating robustness in the presence of moderate class imbalance within the dataset. Grad-CAM visualization highlighted clinically relevant features (eg, age, gender, smoking history, American Society of Anesthesiologists physical status classification (ASA) grade, liver disease, pulmonary disease, and initial carcinoembryonic antigen [CEA] levels). Conversely, the CNN model yielded lower overall accuracy and low specificity, which limits its immediate applicability in clinical settings.</p></sec><sec sec-type="conclusions"><title>Conclusions</title><p>The proposed IGHT-based deep learning model, particularly leveraging the VGG16 architecture, demonstrates promising accuracy and interpretability in predicting 5-year overall survival in patients with colorectal cancer. Its capability to effectively stratify patients into risk categories with balanced sensitivity and specificity underscores its potential utility as a clinical decision support system (CDSS) tool. Future studies incorporating external validation with multicenter cohorts and prospective designs are necessary to establish generalizability and clinical integration feasibility.</p></sec></abstract><kwd-group><kwd>colon</kwd><kwd>rectum</kwd><kwd>cancer</kwd><kwd>predict</kwd><kwd>prediction</kwd><kwd>predictions</kwd><kwd>predictive</kwd><kwd>model</kwd><kwd>models</kwd><kwd>health care</kwd><kwd>clinical informatics</kwd><kwd>electronic health record</kwd><kwd>EHR</kwd><kwd>South Korea</kwd><kwd>convolutional neural networks</kwd><kwd>VGG16</kwd><kwd>neural network</kwd><kwd>deep learning</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="introduction"><title>Introduction</title><p>Artificial intelligence (AI) has evolved, becoming integral to various fields, including health care, bioscience, and medical diagnostics. In the medical field, AI applications range from disease detection to drug prescription optimization [<xref rid="ref1" ref-type="bibr">1</xref>]. Health care providers use AI for patient disease prediction and data anonymization, addressing the growing health care costs associated with increasing chronic diseases and life expectancy [<xref rid="ref2" ref-type="bibr">2</xref>]. Recent studies have increasingly applied advanced machine learning and explainable artificial intelligence (XAI) methods to improve disease survival prediction. For instance, Yang et al [<xref rid="ref3" ref-type="bibr">3</xref>] and Yau et al [<xref rid="ref4" ref-type="bibr">4</xref>] have demonstrated effective survival modeling using time-to-event algorithms and survival tree analyses, respectively. Huang et al [<xref rid="ref5" ref-type="bibr">5</xref>] applied explainable machine learning approaches to uncover important risk factors across major cancers, including colorectal cancer. These studies highlight the growing trend and validate the clinical relevance of AI-driven survival prediction.</p><p>Recent advances in medical imaging technology have enabled sophisticated tumor image analysis using AI models [<xref rid="ref6" ref-type="bibr">6</xref>]. In the field of medical image analysis, active research is being conducted using convolutional neural network (CNN) models to predict responses to treatment in patients with colorectal cancer. Yang et al [<xref rid="ref7" ref-type="bibr">7</xref>] built a model to evaluate the risk of recurrence and metastasis by applying deep learning technology to the data of patients with benign breast cancer. Full hematoxylin and eosin (H&amp;E)-stained images were obtained from surgical specimens of patients with breast cancer, and a CNN was applied to them. The model used in the study achieved an area under the curve (AUC) of 0.76 and showed the potential for evaluating the risk of recurrence and metastasis in patients with human epidermal growth factor receptor 2 (HER2)-positive breast cancer. Althammer et al [<xref rid="ref8" ref-type="bibr">8</xref>] conducted a study on image analysis to predict the response to durvalumab therapy targeting the programmed cell death-1/programmed cell death ligand-1 (PD1/PD-L1) pathway in patients with non&#8211;small cell lung cancer. The results showed that the median overall survival in patients receiving durvalumab was 21 months for those positive for the CD8xPD-L1 signature and 7.8 months for those negative (<italic toggle="yes">P</italic>&lt;.001).</p><p>In particular, the image-guided tabular data (IGTD) method proposed by Zhu et al [<xref rid="ref9" ref-type="bibr">9</xref>] showed that arranging numerical variables into a 2D matrix format can improve model performance by enabling CNNs to capture local feature interactions. Inspired by this approach, we developed an image generator for health tabular data (IGHT) encoding method tailored to our clinical dataset, converting 25 normalized features into a 5&#215;5 image matrix. This fixed spatial layout allows the model to process tabular information through CNN-based architectures, facilitating the use of transfer learning and potentially enhancing predictive performance. There has been growing interest in transforming structured tabular data into image representations to leverage the power of CNNs in domains traditionally dominated by machine learning. For example, Lara-Abelenda et al [<xref rid="ref10" ref-type="bibr">10</xref>] introduced low mixed image-guided tabular data IGTD (LM-IGTD), an enhanced pipeline based on the IGTD approach. Their method applies noise-based augmentation and preserves explicit feature-to-pixel mappings, allowing for the integration of post hoc explanation techniques, such as gradient-weighted class activation mapping (Grad-CAM) and saliency maps for better interpretability. These developments reflect a broader trend toward interpretable and robust AI in clinical settings.</p><p>In this study, we built upon this direction by applying a novel tabular-to-image transformation method, IGHT, which converts structured electronic medical record (EMR) features(tabular data) into images, enabling deep CNN architectures to extract spatial patterns for survival prediction in patients with colorectal cancer.</p><p>Building upon these developments, Sharma et al [<xref rid="ref11" ref-type="bibr">11</xref>] introduced DeepInsight, one of the pioneering frameworks that enables CNNs to process tabular data by converting them into image representations. This method uses dimensionality reduction techniques, such as t-distributed stochastic neighbor embedding (t-SNE) or principal component analysis (PCA), to spatially arrange high-dimensional features in a 2D grid, while preserving interfeature relationships. Such transformations allow CNNs to extract local and global patterns from structured data&#8212;patterns that may be overlooked by traditional machine learning approaches. This early work demonstrated the potential of tabular-to-image conversion to enhance classification performance across domains, and it has laid the foundation for subsequent advances, including this study.</p><p>AI has shown strong performance in learning from existing patient information using big data and in predicting or recommending outcomes desired by clinicians, who are key decision makers. In particular, the clinical decision support system (CDSS), an AI system that helps clinicians make decisions, has attracted considerable attention. The CDSS trains an AI model with existing medical knowledge and patient data to predict outcomes and make recommendations for clinicians based on new patient data [<xref rid="ref12" ref-type="bibr">12</xref>]. Watson for Oncology (WFO), a representative CDSS, was developed by IBM Corp and is a system that analyzes more than 15 million pages of medical documents, 300 medical journals, and 200 guidelines [<xref rid="ref13" ref-type="bibr">13</xref>]. The WFO was designed to recommend treatment options to oncologists and was first introduced and used in 2016 at the Gil Medical Center in Korea [<xref rid="ref14" ref-type="bibr">14</xref>]. Although the WFO at the Gil Medical Center helped inform treatment decisions, it could not reflect regional characteristics due to differences in insurance coverage and medical costs [<xref rid="ref15" ref-type="bibr">15</xref>].</p><p>Lee et al [<xref rid="ref16" ref-type="bibr">16</xref>] compared the treatment recommendations generated by the WFO with the actual treatment received by 656 patients with stage 2, 3, and 4 colon cancer between 2009 and 2016 to determine the concordance rate. The results showed that the agreement rate between the WFO and the Gil Medical Center&#8217;s treatment recommendations was low, at only 48.9%. Since the WFO was trained using data from Americans patients, the prescription recommendations did not match well with those for Korean patients. In addition, it was noted that the treatment recommendations under the Korean insurance system and those under the American insurance system differed, resulting in a low concordance rate with actual treatment practices. Therefore, it is important to establish a CDSS using Korean patient data.</p><p>To build a CDSS, many studies have been conducted to enhance the performance of medical data&#8211;based AI models. Park et al [<xref rid="ref17" ref-type="bibr">17</xref>] used an oversampling technique to address data imbalance and predicted colorectal cancer chemotherapy based on data from the Gil Medical Center in Korea using a deep learning model. Kwon et al [<xref rid="ref18" ref-type="bibr">18</xref>] used machine learning models, such as the gradient boosted model, the distributed random forest, the generalized linear model, and the deep neural network, for a stacking ensemble. They diagnosed breast cancer using the best-performing model in the stacking ensemble. Oh et al [<xref rid="ref19" ref-type="bibr">19</xref>] classified colorectal cancer chemotherapy regimens using machine learning models: k-nearest neighbor (kNN), support vector machine (SVM), decision tree, and light gradient boosting machine (LightGBM), and compared the results across multiple models.</p><p>Colorectal cancer is the second-most common malignancy in South Korea [<xref rid="ref20" ref-type="bibr">20</xref>] and the second leading cause of cancer-related mortality worldwide [<xref rid="ref21" ref-type="bibr">21</xref>]. Surgery is the primary treatment for colon and rectal cancer [<xref rid="ref22" ref-type="bibr">22</xref>], but due to its high postsurgical mortality rate, ongoing prognosis management is essential. Therefore, research aimed at extending patient survival is being conducted from various perspectives, including the prediction of overall survival, disease-free survival, and recurrence. Overall survival and disease-free survival periods are key indicators for assessing a patient&#8217;s prognosis, and to extend life expectancy, further research is needed to evaluate prognosis based on patient-specific factors.</p><p>Studies on predicting the survival period have mainly been conducted in the clinical field using statistical techniques, such as the Kaplan-Meier method [<xref rid="ref23" ref-type="bibr">23</xref>] and Cox&#8217;s proportional hazards model [<xref rid="ref24" ref-type="bibr">24</xref>]. Yeom et al [<xref rid="ref25" ref-type="bibr">25</xref>] identified prognostic factors that increase the risk of death in patients with terminal cancer and predicted the survival period according to the number of these prognostic factors. Using the Kaplan-Meier method and the log-rank test, they investigated whether there were differences in the survival period according to clinical variables. Using Cox&#8217;s proportional hazards model, they identified variables that increase the risk of death among clinical variables and used them as prognostic factors, which were then incorporated into the Weibull proportional hazards function model to predict survival periods.</p><p><xref rid="figure1" ref-type="fig">Figure 1</xref> presents an overview of the pipeline of this study. In this study, we aimed to develop and evaluate deep learning&#8211;based models for predicting survival using data from patients with colorectal cancer. This study used image data by converting tabular electronic medical record (EMR) data received from the health care system into image data to enhance their utility. These deep learning models are expected to improve the performance of predicting the prognosis of patients with colorectal cancer by effectively using EMR data.</p><fig position="float" id="figure1" orientation="portrait"><label>Figure 1</label><caption><p>Overview pipeline of this study process. ANN: artificial neural network; ASA: American Society of Anesthesiologists physical status classification; CEA: carcinoembryonic antigen; CNN: convolutional neural network; DM: diabetes mellitus; LN: lymph nodes; LVI: lymphovascular invasion; pM: pathological distant metastasis; pN: pathological regional lymph node; PNI: perineural invasion; pT: pathological primary tumor; pTNM: pathological tumor, node, metastasis.</p></caption><graphic position="float" orientation="portrait" xlink:href="medinform_v13i1e75022_fig1.jpg"/></fig></sec><sec sec-type="methods"><title>Methods</title><sec><title>Dataset</title><p>In this study, EMR data were retrospectively collected from the Gil Medical Center, a tertiary referral hospital in Incheon, South Korea. A total of 3321 patients who underwent elective surgery with curative intent for colorectal cancer between 2004 and 2018 were included. The dataset was constructed through iterative chart review conducted by colorectal cancer clinicians.</p><p>Demographics are patient characteristics that can typically be known without the need for surgery. These included basic information, such as age, sex, BMI, and the patient status, as classified by the American Society of Anesthesiologists physical status classification (ASA): &#8220;DM_history&#8221; refers to a history of diabetes mellitus; &#8220;Pulmonary_disease,&#8221; a history of lung disease; &#8220;Liver_disease,&#8221; a history of liver disease; and &#8220;Kidney_disease,&#8221; a history of kidney disease.</p><p>Perioperative clinical features included information about each patient gathered before and after surgery: &#8220;Initial CEA&#8221; and &#8220;Initial Hb&#8221; represent the first blood tests performed at the time of cancer diagnosis (serum carcinoembryonic antigen [CEA] and serum hemoglobin [Hb] levels, respectively), &#8220;Transfusion_op&#8221; indicates whether a blood transfusion was performed during surgery; &#8220;Early_complication&#8221; is defined as a case of complications occurring within 30 days after surgery, and &#8220;Postop_Chemotherapy&#8221; indicates whether chemotherapy was used after surgery.</p><p>Histopathologic features can be known after a biopsy of the patient&#8217;s tumor following surgery. Here, &#8220;pTNM&#8221; is a variable that integrates the pathological tumor, node, metastasis (TNM) stage and is classified into stages 1, 2, 3, and 4. The TNM stage classification was based on the <italic toggle="yes">AJCC Cancer Staging Manual, Eighth Edition</italic>. &#8220;pT&#8221; (pathological primary tumor) includes Tis, T1, T2, T3, or T4 (T4a, T4b) as the T stage; &#8220;pN&#8221; (pathological regional lymph node) includes N0, N1 (N1a, N1b, N1c), or N2 (N2a, N2b) as the N stage; and &#8220;pM&#8221; (pathological distant metastasis) includes M1 as the M stage.</p><p>In the &#8220;Intraoperative_tumor_location&#8221; variable, colon cancer is classified into cecum, ascending colon, hepatic flexure, transverse colon, splenic flexure, descending colon, sigmoid colon, and rectosigmoid junction cancer according to the location of the primary tumor. In the case of rectal cancer, 0-5 cm of the anal verge (AV) is classified as the lower rectum, 6-10 cm of the AV is the midrectum, and 11-15 cm of the AV is the upper rectum. The final colon cancer dataset consisted of 2091 patients, and the rectal cancer dataset consisted of 1190 patients.</p></sec><sec><title>Ethical Considerations</title><p>This study was reviewed and approved by the Institutional Review Board of the Gil Medical Center (GFIRB 2023-034). EMR data from the Gil Medical Center were used. Overall, data of 3321 patients were retrospectively collected through an iterative chart review conducted by colorectal cancer specialists. Due to its retrospective nature, the study was exempt from requiring informed consent from the participants.</p></sec><sec><title>Data Preprocessing</title><p>The variables were selected in consultation with a clinician. To select and use clinical variables to enhance the explanatory power of the results, pretreatment was performed after discussion with the clinician. Patient exclusion, variable categorization, missing value removal, and variable selection were performed in that order. All continuous variables were normalized to a (0,1) scale using min-max normalization to standardize the color intensity across features. Categorical variables were first one-hot-encoded and then similarly mapped to the image matrix, ensuring consistent scaling across data types during the image generation process. Images were constructed by arranging features sequentially based on the column order in the original dataset. No domain-driven grouping or clustering of related variables was applied to preserve reproducibility and avoid introducing subjective bias. Although the spatial arrangement of semantically related features may potentially affect CNN performance, this aspect was not explored in the study and is left for future investigation.</p><p>Prior to variable selection, patient cases excluded from the analysis were removed. In the case of &#8220;Kidney_disease,&#8221; missing values were deleted. Patients who underwent surgery for recurrent colorectal cancer were also excluded.</p><p>After excluding patients, variable categorization was performed. Age was categorized by the number 65, which is a standard used to divide age. The BMI was categorized as 18 or less and 18 or more and as 25 or less and 25 or more. In addition, &#8220;DM_history,&#8221; &#8220;Pulmonary_disease,&#8221; &#8220;Liver_disease,&#8221; and &#8220;Heart_disease,&#8221; which are variables corresponding to the patient&#8217;s medical history, were categorized by the presence or absence of a medical history.</p><p>Among the perioperative clinical features, &#8220;Initial_CEA,&#8221; which indicates the first CEA level after diagnosis, was categorized according to 5 criteria, and 166 missing values were replaced with the average value. Missing values were removed for &#8220;Early_Complication,&#8221; which indicates complications within 30 days after surgery. In particular, &#8220;Overall_Survival&#8221; was calculated from the date of surgery until the date of death for uncensored cases or until the date of last follow-up for censored cases. In addition, patients were categorized based on 5 years (60 months), which is the criterion for cure.</p><p>Among histopathologic features, missing values were removed and categorized in &#8220;Havested_LN,&#8221; which represents the number of lymph nodes removed from the patient during surgery. If more than 12 pieces were removed, the operation was considered successful, and if less than 12 pieces were removed, the operation was classified as insufficient. Missing values were also removed from &#8220;Positive_LN,&#8221; which indicates the number of metastatic lymph nodes that could be recognized during surgery. The final selected variables are listed in Table S1 in <xref rid="app1" ref-type="supplementary-material">Multimedia Appendix 1</xref>.</p></sec><sec><title>Image Generation Health Care Tabular Data Method</title><p>In this study, we aimed to convert the data from structured data into 2D unstructured image data to use as input to the deep learning model. The variables of the tabular data were visualized in matrix form and developed according to the data. Each variable in Table S1 in <xref rid="app1" ref-type="supplementary-material">Multimedia Appendix 1</xref> was mapped to one column of the heat map and implemented in the form of an image. <xref rid="figure2" ref-type="fig">Figure 2</xref> shows an example of an imaging matrix. The image is in the form of N&#215;N, where N is 5, and the number of variables used for image conversion is 25.</p><fig position="float" id="figure2" orientation="portrait"><label>Figure 2</label><caption><p>Method of generating IGHT. ASA: American Society of Anesthesiologists physical status classification; CEA: carcinoembryonic antigen; DM: diabetes mellitus; IGHT: image generator health care tabular data; LN: lymph nodes; LVI: lymphovascular invasion; pM: pathological distant metastasis; pN: pathological regional lymph node; PNI: perineural invasion; pT: pathological primary tumor; pTNM: pathological tumor, node, metastasis.</p></caption><graphic position="float" orientation="portrait" xlink:href="medinform_v13i1e75022_fig2.jpg"/></fig><p>In this study, we selected 25 clinically relevant variables and reshaped them into a 5&#215;5 matrix to generate 2D images for input into the CNN models. Variables were categorized into three clinically meaningful groups: demographic, perioperative, and histopathologic features. These variables were mapped from left to right and top to bottom in the 5&#215;5 matrix, following the order of these categories. This spatial configuration was carefully designed to enable the deep learning model to effectively capture interactions among clinically related variables. By organizing the input into a square-shaped matrix, the convolutional filters in the CNN architecture could exploit spatial proximities from the input layer, thereby enhancing the model&#8217;s ability to learn joint patterns and improving both performance and interpretability.</p><p>We also considered scenarios where the number of input features may not perfectly form a square matrix. In such cases, possible strategies include (1) zero-padding to fill the remaining space without introducing clinical meaning and (2) modifying the CNN input layer to accept nonsquare (rectangular) input dimensions. Although these were not necessary in this study, they remain relevant for future extensions of this framework.</p></sec><sec><title>Deep Learning Model Prediction</title><p><xref rid="figure3" ref-type="fig">Figure 3</xref> shows a pictorial representation of the VGG16 model, proposed by the Visual Geometry Group (VGG), used in this study. The weights of VGG16 learned using a large dataset called ImageNet were imported and used to classify the imaging matrix. All modeling and analysis were conducted using Google Colaboratory with the following software environment: Python 3.7.13, TensorFlow 2.8.0, Keras 2.8.0, pandas 1.3.5, scikit-learn 1.0.2, NumPy 1.21.5, matplotlib 3.4.3, and seaborn 0.11.2.</p><fig position="float" id="figure3" orientation="portrait"><label>Figure 3</label><caption><p>Pipeline of VGG16 using the IGHT technique. ASA: American Society of Anesthesiologists physical status classification; CEA: carcinoembryonic antigen; DM: diabetes mellitus; IGHT: image generator for health tabular data; LN: lymph nodes; LVI: lymphovascular invasion; pM: pathological distant metastasis; pN: pathological regional lymph node; PNI: perineural invasion; pT: pathological primary tumor; pTNM: pathological tumor, node, metastasis; VGG: Visual Geometry Group.</p></caption><graphic position="float" orientation="portrait" xlink:href="medinform_v13i1e75022_fig3.jpg"/></fig><p>For model interpretation, Grad-CAM was implemented using custom code adapted from tf-keras-vis and compatible TensorFlow visualization utilities. We divided the dataset of 3321 patients with colorectal cancer into two cohorts&#8212;colon cancer (n=2089, 62.9%) and rectal cancer (n=1232, 37.1%)&#8212;and trained separate models for each group. For both cohorts, the data were randomly split into training and testing sets in a 7:3 ratio.</p><p>ANN and CNN models were trained using default hyperparameters. For the VGG16-based model, we adopted a transfer learning approach: the convolutional base of VGG16 pretrained on ImageNet was used (include_top=False), and all layers were set to be trainable. A flattened output was passed through a dense layer with 256 units (rectified linear unit [ReLU] activation), followed by a dropout layer (rate=0.25) and a sigmoid output layer for binary classification. The optimizer used was Adam with a learning rate of 0.01, and the loss function was binary cross-entropy. We also tested the Stochastic Gradient Descent (SGD) optimizer during model development, but it yielded inferior performance compared to Adam. Key hyperparameters, such as optimizer type, learning rate, and dropout rate, were selected based on empirical validation performance, rather than being used as default values.</p><p>Model performance was evaluated using the testing set, and the metrics reported included accuracy, sensitivity, and specificity.</p><sec><title>ANN Model</title><p>After converting the tabular data into an imaging matrix, an ANN was constructed to compare and verify the AI model results. Instead of using the data converted to the imaging matrix (tabular data), the original dataset was used. Using the original dataset, an ANN was used to predict the survival period of patients with colorectal cancer and validate the results. Table S2 in <xref rid="app1" ref-type="supplementary-material">Multimedia Appendix 1</xref> lists the parameters of the baseline model in detail.</p></sec><sec><title>CNN Model</title><p>In this study, after transformation of data into an imaging matrix, a CNN model was applied. The model was constructed to classify the data by dividing the survival period of patients with colorectal cancer by 5 years. The classification performance of the patient survival period was confirmed using the CNN model, which showed good performance in image classification. Table S3 in <xref rid="app1" ref-type="supplementary-material">Multimedia Appendix 1</xref> lists the parameters of the CNN model.</p></sec><sec><title>VGG16 Model</title><p>In this study, transfer learning based on the VGG16 model created by Simonyan and Ziserman in 2014 was performed. The VGG16 model is a structure developed from the existing CNN structure, and because it is convenient to apply, studies using VGG16 are actively being conducted to improve classification performance. Table S4 in <xref rid="app1" ref-type="supplementary-material">Multimedia Appendix 1</xref> shows the structure of VGG16 used in this study. <xref rid="figure4" ref-type="fig">Figure 4</xref> shows a pictorial representation of the transfer learning method used in this study. In this study, the weights of VGG16 learned using a large dataset called ImageNet were imported and used to classify the imaging matrix.</p><fig position="float" id="figure4" orientation="portrait"><label>Figure 4</label><caption><p>Example of Grad-CAM results: (a) original data from the image generator, (b) heat map produced by Grad-CAM, and (c) visualized image generated by Grad-CAM. Grad-CAM: gradient-weighted class activation mapping.</p></caption><graphic position="float" orientation="portrait" xlink:href="medinform_v13i1e75022_fig4.jpg"/></fig></sec></sec><sec><title>Gradient-Weighted Class Activation Mapping</title><p>To demonstrate the models&#8217; potential as a CDSS, we used Grad-CAM [<xref rid="ref26" ref-type="bibr">26</xref>] to visualize important features for individual patient predictions. VGG16 is a CNN-based model that cannot explain the results of a model [<xref rid="ref27" ref-type="bibr">27</xref>]. Therefore, Grad-CAM was used to increase the explanatory potential of the model results. Grad-CAM was used to generate heat maps that highlighted variables with notable influence on the models&#8217; prediction of the survival period of patients with colorectal cancer. The heat maps generated by Grad-CAM are highlighted in red and are a method of displaying the variable area that has a large influence on the prediction. Grad-CAM extracts features using weights extracted from the last convolutional layer of the models used for prediction [<xref rid="ref28" ref-type="bibr">28</xref>].</p><p>An example of converting Grad-CAM is shown in <xref rid="figure4" ref-type="fig">Figure 4</xref>. In the weight of the models trained using patient data (<xref rid="figure4" ref-type="fig">Figure 4</xref>a), the influence of the variable that affected the prediction result was converted into a heat map of the form in <xref rid="figure4" ref-type="fig">Figure 4</xref>b. Next, an image was created in a form that could intuitively determine the influence, as shown in <xref rid="figure4" ref-type="fig">Figure 4</xref>c, overlaid on <xref rid="figure4" ref-type="fig">Figure 4</xref>a.</p></sec><sec><title>Evaluation Metrics</title><p>The predictive performance of the models was evaluated using three key metrics: accuracy, sensitivity, and specificity.</p><p>Accuracy measures the overall proportion of correctly classified instances. Sensitivity (also known as recall) quantifies the proportion of true-positive cases correctly identified by the model, while specificity measures the proportion of true negatives correctly identified.</p><p>These metrics were calculated on the test datasets to assess the generalization performance of the models.</p></sec></sec><sec sec-type="results"><title>Results</title><sec><title>Imaging Matrix Transformation</title><p>The transformation of tabular data into 5&#215;5 imaging matrices using 25 variables from patients with colorectal cancer demonstrated distinctive visual patterns. The heat map representation, where values approaching 1 appear white and those approaching 0 appear black, provided an intuitive visualization of patient data. For survival period prediction, we established a binary classification. <xref rid="figure5" ref-type="fig">Figure 5</xref> shows an example of the imaging matrix created. Class 0 represented patients with a survival period of less than 5 years, while class 1 represented patients with a survival period of more than 5 years.</p><fig position="float" id="figure5" orientation="portrait"><label>Figure 5</label><caption><p>Example of an imaging matrix: (a) class 0 image in colon cancer, (b) class 1 image in colon cancer, (c) class 0 image in rectal cancer, and (d) class 1 image in rectal cancer.</p></caption><graphic position="float" orientation="portrait" xlink:href="medinform_v13i1e75022_fig5.jpg"/></fig></sec><sec><title>Comprehensive Model Performance Evaluation</title><p>We conducted an extensive comparative analysis across three distinct deep learning architectures, each revealing unique strengths in colorectal cancer survival prediction.</p><p>As shown in <xref rid="table1" ref-type="table">Tables 1</xref> and <xref rid="table2" ref-type="table">2</xref>, the accuracy and specificity of VGG16 were the best among the three models. However, the sensitivity of VGG16 was the lowest among the three models, which means that the model mistakenly thought that the actual survival period of a patient was less than 5 years but that the patient had a good prognosis. Compared to VGG16, the ANN and CNN showed higher sensitivities (84%-99%) in identifying patients with poor prognoses well. However, in terms of specificity, VGG16 showed better performance in correctly predicting patients with a 5-year survival period (89.55% for colon cancer and 87.9% for rectal cancer).</p><table-wrap position="float" id="table1" orientation="portrait"><label>Table 1</label><caption><p>Overall survival prediction in patients with colon and rectal cancer using ANN<sup>a</sup>, CNN<sup>b</sup>, and VGG16<sup>c</sup> models.</p></caption><table frame="hsides" rules="groups" width="1000" cellpadding="5" cellspacing="0" border="1"><col width="130" span="1"/><col width="170" span="1"/><col width="190" span="1"/><col width="190" span="1"/><col width="170" span="1"/><col width="150" span="1"/><thead><tr valign="top"><td rowspan="1" colspan="1">Model</td><td rowspan="1" colspan="1">Accuracy (%)</td><td rowspan="1" colspan="1">Sensitivity (%)</td><td rowspan="1" colspan="1">Specificity (%)</td><td rowspan="1" colspan="1">Precision (%)</td><td rowspan="1" colspan="1"><italic toggle="yes">F</italic><sub>1</sub>-score (%)</td></tr></thead><tbody><tr valign="top"><td rowspan="1" colspan="1">ANN</td><td rowspan="1" colspan="1">71.47</td><td rowspan="1" colspan="1">84.00</td><td rowspan="1" colspan="1">54.23</td><td rowspan="1" colspan="1">71.59</td><td rowspan="1" colspan="1">76.86</td></tr><tr valign="top"><td rowspan="1" colspan="1">CNN</td><td rowspan="1" colspan="1">61.90</td><td rowspan="1" colspan="1">99.40</td><td rowspan="1" colspan="1">4.90</td><td rowspan="1" colspan="1">61.58</td><td rowspan="1" colspan="1">76.17</td></tr><tr valign="top"><td rowspan="1" colspan="1">VGG16</td><td rowspan="1" colspan="1">78.23</td><td rowspan="1" colspan="1">40.79</td><td rowspan="1" colspan="1">89.55</td><td rowspan="1" colspan="1">80.55</td><td rowspan="1" colspan="1">54.33</td></tr></tbody></table><table-wrap-foot><fn id="table1fn1"><p><sup>a</sup>ANN: artificial neural network.</p></fn><fn id="table1fn2"><p><sup>b</sup>CNN: convolutional neural network.</p></fn><fn id="table1fn3"><p><sup>c</sup>VGG: Visual Geometry Group.</p></fn></table-wrap-foot></table-wrap><table-wrap position="float" id="table2" orientation="portrait"><label>Table 2</label><caption><p>Overall survival prediction in patients with rectal cancer using ANNa, CNNb, and VGG16c models.</p></caption><table frame="hsides" rules="groups" width="1000" cellpadding="5" cellspacing="0" border="1"><col width="130" span="1"/><col width="170" span="1"/><col width="190" span="1"/><col width="190" span="1"/><col width="170" span="1"/><col width="150" span="1"/><thead><tr valign="top"><td rowspan="1" colspan="1">Model</td><td rowspan="1" colspan="1">Accuracy (%)</td><td rowspan="1" colspan="1">Sensitivity (%)</td><td rowspan="1" colspan="1">Specificity (%)</td><td rowspan="1" colspan="1">Precision (%)</td><td rowspan="1" colspan="1"><italic toggle="yes">F</italic><sub>1</sub>-score (%)</td></tr></thead><tbody><tr valign="top"><td rowspan="1" colspan="1">ANN</td><td rowspan="1" colspan="1">74.78</td><td rowspan="1" colspan="1">87.46</td><td rowspan="1" colspan="1">55.26</td><td rowspan="1" colspan="1">74.14</td><td rowspan="1" colspan="1">80.18</td></tr><tr valign="top"><td rowspan="1" colspan="1">CNN</td><td rowspan="1" colspan="1">61.80</td><td rowspan="1" colspan="1">97.71</td><td rowspan="1" colspan="1">4.93</td><td rowspan="1" colspan="1">61.35</td><td rowspan="1" colspan="1">75.81</td></tr><tr valign="top"><td rowspan="1" colspan="1">VGG16</td><td rowspan="1" colspan="1">76.54</td><td rowspan="1" colspan="1">35.56</td><td rowspan="1" colspan="1">87.90</td><td rowspan="1" colspan="1">78.68</td><td rowspan="1" colspan="1">49.02</td></tr></tbody></table><table-wrap-foot><fn id="table2fn1"><p><sup>a</sup>ANN: artificial neural network.</p></fn><fn id="table2fn2"><p><sup>b</sup>CNN: convolutional neural network.</p></fn><fn id="table2fn3"><p><sup>c</sup>VGG: Visual Geometry Group.</p></fn></table-wrap-foot></table-wrap></sec><sec><title>Gradient-Weighted Class Activation Mapping</title><p>Grad-CAM analysis was used to enhance the interpretability of the VGG16 model predictions. This technique generated heat maps highlighting variables that appeared most influential in the three models&#8217; survival period predictions.</p><sec><title>Analysis of Patients With Colon Cancer</title><p><xref rid="figure6" ref-type="fig">Figure 6</xref> presents examples of heat map conversion results and Grad-CAM application results for patients with colon cancer. The patient has a survival period of more than 5 years and is a successful case of deep learning model prediction. Looking at <xref rid="figure6" ref-type="fig">Figure 6</xref>a, the patient was in an early pTNM stage. Considering this, it can be confirmed that the factor that played a major role in VGG16 predicting the survival of this patient was the pTNM stage. The results of Grad-CAM for patients without liver disease and a moderate ASA grade had an impact on predicting that patients would have longer survival times (<xref rid="figure6" ref-type="fig">Figure 6</xref>d).</p><fig position="float" id="figure6" orientation="portrait"><label>Figure 6</label><caption><p>Visualized results generated by Grad-CAM in patients with colon cancer: (a, c) original data from the image generator and (b, d) visualized image generated by Grad-CAM. Grad-CAM: gradient-weighted class activation mapping.</p></caption><graphic position="float" orientation="portrait" xlink:href="medinform_v13i1e75022_fig6.jpg"/></fig></sec><sec><title>Analysis of Patients With Rectal Cancer</title><p><xref rid="figure7" ref-type="fig">Figure 7</xref> presents examples of heat map conversion results and Grad-CAM application results for patients with rectal cancer. <xref rid="figure7" ref-type="fig">Figures 7</xref>a and 7c are the learned images, and <xref rid="figure7" ref-type="fig">Figures 7</xref>b and 7d show the results of Grad-CAM as a heat map. As shown in the imaging matrix in <xref rid="figure7" ref-type="fig">Figure 7</xref>b, the smoking history, initial CEA level, and pTNM stage were indicated as variables that had a major influence on the prediction of the model: there was no smoking history, the initial CEA level was moderate, and the TNM stage was early. As the patient&#8217;s survival period was well predicted to be more than 5 years, it was found that the Grad-CAM analysis results were consistent with the results of existing clinical studies. As shown in <xref rid="figure7" ref-type="fig">Figure 7</xref>d, age, sex, the ASA grade, and pulmonary disease affected the survival time prediction results. For a patient with a high age, a low ASA grade, and no pulmonary disease, with a well-predicted survival of more than 5 years, because the ASA grade was low, the patient&#8217;s physical condition was good, and there was no pulmonary disease, the survival period seemed to be predicted for a long time. As described earlier, Grad-CAM can be used to determine which variables have the most influence on the prediction of survival time for individual patients and their risk factors.</p><fig position="float" id="figure7" orientation="portrait"><label>Figure 7</label><caption><p>Visualized results generated by Grad-CAM in patients with rectal cancer: (a, c) original data from the image generator and (b, d) visualized image generated by Grad-CAM. Grad-CAM: gradient-weighted class activation mapping.</p></caption><graphic position="float" orientation="portrait" xlink:href="medinform_v13i1e75022_fig7.jpg"/></fig></sec></sec></sec><sec sec-type="discussion"><title>Discussion</title><sec><title>Principal Findings</title><p>In this study, we developed a model to predict the survival period of colorectal cancer using EMR data and investigated which variables contributed to the prediction. In particular, we improved the performance of the model through an innovative approach to convert tabular medical data into image data. The results of the study showed that the VGG16 model achieves the best performance, which suggests a new methodology for developing a CDSS for patients with colorectal cancer in clinical settings.</p><p>Deep learning models performed important clinical predictions on whether the survival period of patients with colorectal cancer who underwent surgery is more than 5 years. If the survival period of a patient with cancer is more than 5 years, it indicates that the patient&#8217;s prognosis is good and the risk of cancer recurrence is low, which can provide clinically helpful information. Furthermore, it shows that doctors can use this as a reference to understand the patient&#8217;s condition and support them in making better decisions.</p><p>In this study, the prediction accuracy of VGG16 was 75%, and that of the CNN was 61%, showing that the VGG16 model, which used more weights through transfer learning, performs better. The reason for the large performance difference between the CNN and VGG16, about 14%, is probably because the amount of data used in this study was less than that used in other deep learning models. However, VGG16 is a transfer learning model trained using a large ImageNET-based dataset and provides the weights obtained as a result. In this study, we used the weights of VGG16 for learning, and we were able to see that the prediction performance was improved by overcoming the limitations of quantitative data volume.</p><p>Given the class imbalance in our dataset and the differing behavior observed across models, a clear understanding of the trade-off between sensitivity and specificity is essential for clinical application. Although oversampling techniques, such as the synthetic minority oversampling technique (SMOTE), were initially considered to mitigate the imbalance, we ultimately decided not to apply them after consultation with clinical experts. It was determined that artificially augmenting the minority classes could compromise the clinical validity and real-world representativeness of colorectal cancer data. Therefore, we maintained the original class distribution and focused on evaluating model performance through sensitivity and specificity, which are more aligned with clinical priorities. To further enhance model interpretability and support clinical decision-making, we applied Grad-CAM to visualize the features contributing to model predictions.</p><p>Our best-performing model, VGG16, exhibited high specificity (88%-90%) but relatively low sensitivity (35%-40%). This indicates that when the model predicts early mortality (nonsurvival within 5 years), it is highly reliable, minimizing false positives in identifying high-risk patients. Such high specificity is valuable in clinical settings where unnecessary aggressive follow-up could impose physical, psychological, and economic burdens. Considering the class distribution (approximately 6:4 for positive to negative outcomes, where &#8220;positive&#8221; refers to patients surviving for more than 5 years), precision and <italic toggle="yes">F</italic><sub>1</sub>-scores provide further insight into model behavior. VGG16, which showed the highest specificity and precision, tended to make more conservative predictions regarding long-term survival, resulting in fewer false positives. From a CDSS perspective, its relatively high <italic toggle="yes">F</italic><sub>1</sub>-score and sensitivity suggest potential usefulness in identifying patients unlikely to survive beyond 5 years. Although the ANN and CNN also demonstrated comparable sensitivity, VGG16 maintained this without compromising specificity, indicating a more balanced performance that may help reduce false alarms in clinical practice.</p><p>The VGG16 model outperformed the ANN model by achieving an accuracy improvement of 6.76% (78.23% vs 71.47%) in colon cancer survival prediction and 1.76% (76.54% vs 74.78%) in rectal cancer survival prediction. Additionally, VGG16 demonstrated markedly higher specificity compared to the ANN (89.55% vs 54.23% for colon cancer and 87.9% vs 55.26% for rectal cancer), indicating improved ability to correctly identify patients with better survival outcomes.</p><p>In contrast, the CNN model showed substantially lower accuracy (61.9% for colon cancer and 61.8% for rectal cancer) and poor specificity (&lt;5%), suggesting limited clinical utility in its current form. These findings highlight that although basic CNN architectures may underperform, deeper networks, such as VGG16, can capture complex patterns to improve prediction reliability.</p><p>However, the lower sensitivity implies that some patients who do die early are not identified by the model (false negatives), potentially missing individuals who could benefit from intensified monitoring or interventions. This limitation is crucial to acknowledge, as underdetection may reduce the model&#8217;s effectiveness in guiding proactive clinical decision-making.</p><p>Conversely, the CNN model showed the opposite pattern, with high sensitivity but low specificity, which would result in many false alarms and potentially excessive interventions.</p><p>Therefore, the choice of model and classification threshold must be carefully tailored to the clinical context and intended use case. For example, a model prioritizing sensitivity may be preferred in screening scenarios to ensure at-risk patients are not missed, whereas one prioritizing specificity may be favored where reducing false positives is paramount.</p><p>Unlike the black-box model, the demand for a white-box model that provides a reason for the result is increasing [<xref rid="ref29" ref-type="bibr">29</xref>]. A white-box model explains the results of an AI model and has recently been attracting attention under the name of XAI [<xref rid="ref30" ref-type="bibr">30</xref>].</p><p>Although our model achieved an accuracy of 75%-78% in predicting 5&#8209;year survival, the existing literature indicates that this level of performance is clinically meaningful. Kos et al [<xref rid="ref31" ref-type="bibr">31</xref>] used various machine learning models (eg, decision tree, stacking ensemble, and SVM) to predict survival rates from 1 to 10 years using large-scale data of patients with colorectal cancer in Australia. In this study, the 5-year survival prediction models showed an AUC of approximately 0.86-0.89 and an accuracy of over 70%, and the performance by cancer stage also showed an excellent predictive power of over 70%. Compared to the prediction accuracy of this study (78.4% for colon cancer and 74.8% for rectal cancer), both studies suggest that machine learning&#8211;based clinical data use is effective for survival prediction, and this study is particularly different in that it converted EMR data into images and applied them to a CNN-based model. Similarly, Gao et al [<xref rid="ref32" ref-type="bibr">32</xref>] compared nine ML models against TNM staging alone (AUC=0.784) and found that most models only modestly outperform staging, often with overlapping CIs.</p><p>These findings suggest that our image&#8209;based deep learning model operates within a comparable performance band, while adding the benefit of interpretable Grad&#8209;CAM visualization. Furthermore, staging&#8209;only approaches often fall short when applied to patients with stage II-III cancer, whose outcomes are less deterministic. Our model showed improved discrimination in this subgroup, indicating its potential to complement staging heuristics in borderline clinical cases.</p><p>Finally, the alignment between Grad&#8209;CAM highlighting (eg, pTNM stage, smoking history, age) and established prognostic factors enhances the model&#8217;s trustworthiness and suitability for integration into explainable CDSSs. To help clinical decision-making, it is better for deep learning models to also provide reasons for their decisions. In this study, Grad-CAM was used to provide XAI in deep learning models. Grad-CAM can answer the reasons for individual patient outcomes, and the survival prediction model shows the possibility of providing tailored medicine as part of a CDSS.</p></sec><sec><title>Clinical Implications</title><p><xref rid="figure6" ref-type="fig">Figures 6</xref> and <xref rid="figure7" ref-type="fig">7</xref> show the results of analyzing the results for some patients with colorectal cancer using Grad-CAM. Among the patients with colon and rectal cancer, we randomly selected 2 patients and examined the heatmap from the Grad-CAM analysis results. As a result, the pTNM stage was found to be the variable that had the greatest influence on the model prediction in both cancer types. However, additional variables that affected each patient were different: age, gender, smoking history, ASA grade, liver disease, pulmonary disease, and initial CEA levels. The results of Grad-CAM analysis for patients in <xref rid="figure5" ref-type="fig">Figure 5</xref>d,e show that the pTNM stage has an influence on the prediction, which is consistent with existing medical knowledge [<xref rid="ref33" ref-type="bibr">33</xref>]. <xref rid="figure6" ref-type="fig">Figure 6</xref> shows that smoking history has a major influence on the prediction by the model. In existing clinical studies, smoking history has been identified as a relevant factor affecting patient survival [<xref rid="ref34" ref-type="bibr">34</xref>]. Since various factors must be considered in order to determine the prognosis of patients with colon cancer, if we conduct an analysis that can identify the influential variables at once, such as Grad-CAM, we will be able to provide better clinical services.</p></sec><sec><title>Limitations and Future Work</title><p>When training a deep learning model using a small amount of data, overfitting may occur. If overfitting occurs, predictions may not work well with data other than trained data. Therefore, if we can collect more multicenter studies or data in future research, we will further increase the reliability of the study by conducting an analysis by stage.</p><p>To leverage the spatial learning capabilities of CNNs, we transformed 25 clinical variables into a 5&#215;5 image for each patient. Each feature was min-max-normalized and assigned to a fixed position in the 5&#215;5 matrix, ensuring consistent spatial encoding across all samples. Although clinical tabular data do not possess an inherent spatial structure, prior studies have shown that imposing a structured layout allows CNNs to effectively capture local feature interactions and complex nonlinear relationships that may be overlooked by traditional models. This image-based representation also enables the application of transfer learning using pretrained CNN architectures.</p><p>Although the variable layout was arbitrarily fixed in this study, future work may explore data-driven arrangements based on feature correlation or clinical relevance to further enhance the model&#8217;s representational capacity.</p><p>The proposed model, although demonstrating strong predictive performance, must address several practical considerations to be implemented as a CDSS. Integration into existing EMR systems would require standardized data pipelines, interoperability, and real-time processing capabilities. Additionally, interpretability remains essential for clinical adoption. In this study, we used Grad-CAM to identify features contributing most to each prediction, providing visual explanations that could be embedded into future CDSS interfaces.</p><p>From a clinical perspective, prediction models for survival outcomes in colorectal cancer can serve as valuable tools, particularly in the postoperative setting. Previous studies have demonstrated the utility of machine learning for guiding surveillance and adjuvant therapy decisions [<xref rid="ref35" ref-type="bibr">35</xref>,<xref rid="ref36" ref-type="bibr">36</xref>]. Our deep learning&#8211;based model offers automated and objective risk stratification using routinely collected clinical data, supporting multidisciplinary teams in identifying high-risk patients. Within clinical workflows, the system could deliver actionable alerts&#8212;such as notifications flagging patients with poor predicted survival&#8212;to prompt timely follow-up or intervention. These alerts, coupled with interpretable Grad-CAM visualizations, can enhance clinical reasoning, build trust, and facilitate shared decision-making by clearly communicating risk to both clinicians and patients.</p><p>Although this study adopted a binary classification approach (predicting 5-year survival), we acknowledge that this represents a simplification of the underlying clinical reality. In oncology, the timing of events such as recurrence or mortality is often just as important as whether the events occur. Therefore, future work will explore modeling survival outcomes in a time-to-event framework using deep learning&#8211;based survival models. Methods such as DeepSurv, a Cox proportional hazards&#8211;based neural network, have shown promise in capturing complex nonlinear relationships, while preserving the structure of survival data [<xref rid="ref37" ref-type="bibr">37</xref>]. These approaches could yield more clinically informative predictions, especially in patient-level prognostication and individualized treatment planning.</p><p>In addition, future enhancements could include the use of more advanced convolutional architectures, such as ResNet or EfficientNet, as well as ensemble learning strategies. These techniques may help improve both performance and generalizability, especially in heterogeneous clinical datasets.</p><p>The dataset used in this study was built through the EMR system of a single institution. For multi-institutional research, if a standardized medical information system for each type of medical institution in Korea is established and advanced, it will be possible to produce generalized prediction results using more patient data. Furthermore, if data construction standards for each institution are established, it is believed that the establishment of a data mart will help Korea&#8217;s data ecosystem and contribute to the generalization of research results.</p><p>The dataset used in this study was limited as it was collected from a single institution. Therefore, the characteristics of the patient group in this study may not reflect the clinical features of patients in other medical institutions. In future research, we will present a generalized model that reflects the clinical features of patients in a multicenter study.</p></sec><sec><title>Conclusion</title><p>XAI research that enhances the interpretability of deep learning model results is actively progressing. In this study, we used the IGHT technique to convert structured data into images, which allowed us to capture relationships between variables effectively. Using this approach, we developed models to predict overall survival, a key indicator for determining the prognosis of patients with colorectal cancer after surgery.</p><p>An ANN model, a CNN model, and transfer learning with a pretrained VGG16 model were used to evaluate predictive performance. Our results suggest that image-based input leads to improved prediction compared to traditional tabular data analysis. Additionally, CNN-based models provide opportunities for enhanced interpretability through techniques such as Grad-CAM.</p></sec></sec></body><back><ack><p>This work was supported by the Gachon University research fund of 2021 (GCU-202110290001).</p></ack><fn-group><fn fn-type="con"><p>Authors' Contributions: S-HO contributed to data cleansing, analysis, modeling, and manuscript writing. WS provided clinical direction and assisted the manuscript. J-HB was responsible for data collection, curation, interpretation, and verification. YL designed and supervised the research and critically reviewed the final manuscript. All authors had access to the data, reviewed the study results, and approved the final manuscript.</p></fn><fn fn-type="COI-statement"><p>Conflicts of Interest: None declared.</p></fn></fn-group><app-group><supplementary-material id="app1" position="float" content-type="local-data" orientation="portrait"><label>Multimedia Appendix 1</label><p>Feature overview of the dataset, parameters of the artificial neural network (ANN) model, parameters of the convolutional neural network (CNN) model, and parameters of the Visual Geometry Group (VGG)16 model.</p><media xlink:href="medinform_v13i1e75022_app1.docx" xlink:title="DOCX File , 15 KB" id="d100e781" position="anchor" orientation="portrait"/></supplementary-material></app-group><glossary><title>Abbreviations</title><def-list><def-item><term id="abb1">AI</term><def><p>artificial intelligence</p></def></def-item><def-item><term id="abb2">ANN</term><def><p>artificial neural network</p></def></def-item><def-item><term id="abb3">ASA</term><def><p>American Society of Anesthesiologists physical status classification</p></def></def-item><def-item><term id="abb4">AUC</term><def><p>area under the curve</p></def></def-item><def-item><term id="abb5">AV</term><def><p>anal verge</p></def></def-item><def-item><term id="abb6">CDSS</term><def><p>clinical decision support system</p></def></def-item><def-item><term id="abb7">CEA</term><def><p>carcinoembryonic antigen</p></def></def-item><def-item><term id="abb8">CNN</term><def><p>convolutional neural network</p></def></def-item><def-item><term id="abb9">DM</term><def><p>diabetes mellitus</p></def></def-item><def-item><term id="abb10">EMR</term><def><p>electronic medical record</p></def></def-item><def-item><term id="abb11">Grad-CAM</term><def><p>gradient-weighted class activation mapping</p></def></def-item><def-item><term id="abb12">IGHT</term><def><p>image generator health care tabular data</p></def></def-item><def-item><term id="abb13">IGTD</term><def><p>image-guided tabular data</p></def></def-item><def-item><term id="abb14">pM</term><def><p>pathological distant metastasis</p></def></def-item><def-item><term id="abb15">pN</term><def><p>pathological regional lymph node</p></def></def-item><def-item><term id="abb16">pT</term><def><p>pathological primary tumor</p></def></def-item><def-item><term id="abb17">pTNM</term><def><p>pathological tumor, node, metastasis</p></def></def-item><def-item><term id="abb18">PNI</term><def><p>perineural invasion</p></def></def-item><def-item><term id="abb19">LN</term><def><p>lymph nodes</p></def></def-item><def-item><term id="abb20">LVI</term><def><p>lymphovascular invasion</p></def></def-item><def-item><term id="abb21">SVM</term><def><p>support vector machine</p></def></def-item><def-item><term id="abb22">TNM</term><def><p>tumor, node, metastasis</p></def></def-item><def-item><term id="abb23">VGG</term><def><p>Visual Geometry Group</p></def></def-item><def-item><term id="abb24">WFO</term><def><p>Watson for Oncology</p></def></def-item><def-item><term id="abb25">XAI</term><def><p>explainable artificial intelligence</p></def></def-item></def-list></glossary><ref-list><ref id="ref1"><label>1</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>Y</given-names></name><name name-style="western"><surname>Park</surname><given-names>Y</given-names></name><name name-style="western"><surname>Park</surname><given-names>J</given-names></name><name name-style="western"><surname>Yi</surname><given-names>B</given-names></name></person-group><article-title>Association between electronic medical record system adoption and healthcare information technology infrastructure</article-title><source>Healthc Inform Res</source><year>2018</year><month>10</month><volume>24</volume><issue>4</issue><fpage>327</fpage><lpage>334</lpage><comment><ext-link xlink:href="https://europepmc.org/abstract/MED/30443421" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.4258/hir.2018.24.4.327</pub-id><pub-id pub-id-type="medline">30443421</pub-id><pub-id pub-id-type="pmid">30443421</pub-id><pub-id pub-id-type="pmcid">PMC6230536</pub-id></element-citation></ref><ref id="ref2"><label>2</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hlatky</surname><given-names>MA</given-names></name><name name-style="western"><surname>Kazi</surname><given-names>DS</given-names></name></person-group><article-title>PCSK9 inhibitors: economics and policy</article-title><source>J Am Coll Cardiol</source><year>2017</year><month>11</month><day>28</day><volume>70</volume><issue>21</issue><fpage>2677</fpage><lpage>2687</lpage><comment><ext-link xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0735-1097(17)41049-7" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1016/j.jacc.2017.10.001</pub-id><pub-id pub-id-type="medline">29169476</pub-id><pub-id pub-id-type="pii">S0735-1097(17)41049-7</pub-id><pub-id pub-id-type="pmid">29169476</pub-id></element-citation></ref><ref id="ref3"><label>3</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>X</given-names></name><name name-style="western"><surname>Qiu</surname><given-names>H</given-names></name><name name-style="western"><surname>Wang</surname><given-names>L</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X</given-names></name></person-group><article-title>Predicting colorectal cancer survival using time-to-event machine learning: retrospective cohort study</article-title><source>J Med Internet Res</source><year>2023</year><month>10</month><day>26</day><volume>25</volume><fpage>e44417</fpage><comment><ext-link xlink:href="https://www.jmir.org/2023//e44417/" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.2196/44417</pub-id><pub-id pub-id-type="medline">37883174</pub-id><pub-id pub-id-type="pii">v25i1e44417</pub-id><pub-id pub-id-type="pmid">37883174</pub-id><pub-id pub-id-type="pmcid">PMC10636616</pub-id></element-citation></ref><ref id="ref4"><label>4</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yau</surname><given-names>STY</given-names></name><name name-style="western"><surname>Hung</surname><given-names>CT</given-names></name><name name-style="western"><surname>Leung</surname><given-names>EYM</given-names></name><name name-style="western"><surname>Lee</surname><given-names>A</given-names></name><name name-style="western"><surname>Yeoh</surname><given-names>EK</given-names></name></person-group><article-title>Survival tree analysis of interactions among factors associated with colorectal cancer risk in patients with type 2 diabetes: retrospective cohort study</article-title><source>JMIR Public Health Surveill</source><year>2025</year><month>04</month><day>29</day><volume>11</volume><fpage>e62756</fpage><comment><ext-link xlink:href="https://publichealth.jmir.org/2025//e62756/" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.2196/62756</pub-id><pub-id pub-id-type="medline">40300170</pub-id><pub-id pub-id-type="pii">v11i1e62756</pub-id><pub-id pub-id-type="pmid">40300170</pub-id><pub-id pub-id-type="pmcid">PMC12054970</pub-id></element-citation></ref><ref id="ref5"><label>5</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>X</given-names></name><name name-style="western"><surname>Ren</surname><given-names>S</given-names></name><name name-style="western"><surname>Mao</surname><given-names>X</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S</given-names></name><name name-style="western"><surname>Chen</surname><given-names>E</given-names></name><name name-style="western"><surname>He</surname><given-names>Y</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Y</given-names></name></person-group><article-title>Association between risk factors and major cancers: explainable machine learning approach</article-title><source>JMIR Cancer</source><year>2025</year><month>05</month><day>02</day><volume>11</volume><fpage>e62833</fpage><comment><ext-link xlink:href="https://cancer.jmir.org/2025//e62833/" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.2196/62833</pub-id><pub-id pub-id-type="medline">40315870</pub-id><pub-id pub-id-type="pii">v11i1e62833</pub-id><pub-id pub-id-type="pmid">40315870</pub-id><pub-id pub-id-type="pmcid">PMC12064211</pub-id></element-citation></ref><ref id="ref6"><label>6</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Timp</surname><given-names>S</given-names></name><name name-style="western"><surname>Varela</surname><given-names>C</given-names></name><name name-style="western"><surname>Karssemeijer</surname><given-names>N</given-names></name></person-group><article-title>Computer-aided diagnosis with temporal analysis to improve radiologists' interpretation of mammographic mass lesions</article-title><source>IEEE Trans Inf Technol Biomed</source><year>2010</year><month>05</month><volume>14</volume><issue>3</issue><fpage>803</fpage><lpage>8</lpage><pub-id pub-id-type="doi">10.1109/TITB.2010.2043296</pub-id><pub-id pub-id-type="medline">20403792</pub-id><pub-id pub-id-type="pmid">20403792</pub-id></element-citation></ref><ref id="ref7"><label>7</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>J</given-names></name><name name-style="western"><surname>Ju</surname><given-names>J</given-names></name><name name-style="western"><surname>Guo</surname><given-names>L</given-names></name><name name-style="western"><surname>Ji</surname><given-names>B</given-names></name><name name-style="western"><surname>Shi</surname><given-names>S</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z</given-names></name><name name-style="western"><surname>Gao</surname><given-names>S</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>X</given-names></name><name name-style="western"><surname>Tian</surname><given-names>G</given-names></name><name name-style="western"><surname>Liang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>P</given-names></name></person-group><article-title>Prediction of HER2-positive breast cancer recurrence and metastasis risk from histopathological images and clinical information via multimodal deep learning</article-title><source>Comput Struct Biotechnol J</source><year>2022</year><volume>20</volume><fpage>333</fpage><lpage>342</lpage><comment><ext-link xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S2001-0370(21)00537-7" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1016/j.csbj.2021.12.028</pub-id><pub-id pub-id-type="medline">35035786</pub-id><pub-id pub-id-type="pii">S2001-0370(21)00537-7</pub-id><pub-id pub-id-type="pmid">35035786</pub-id><pub-id pub-id-type="pmcid">PMC8733169</pub-id></element-citation></ref><ref id="ref8"><label>8</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Althammer</surname><given-names>S</given-names></name><name name-style="western"><surname>Tan</surname><given-names>TH</given-names></name><name name-style="western"><surname>Spitzm&#252;ller</surname><given-names>Andreas</given-names></name><name name-style="western"><surname>Rognoni</surname><given-names>L</given-names></name><name name-style="western"><surname>Wiestler</surname><given-names>T</given-names></name><name name-style="western"><surname>Herz</surname><given-names>T</given-names></name><name name-style="western"><surname>Widmaier</surname><given-names>M</given-names></name><name name-style="western"><surname>Rebelatto</surname><given-names>MC</given-names></name><name name-style="western"><surname>Kaplon</surname><given-names>H</given-names></name><name name-style="western"><surname>Damotte</surname><given-names>D</given-names></name><name name-style="western"><surname>Alifano</surname><given-names>M</given-names></name><name name-style="western"><surname>Hammond</surname><given-names>SA</given-names></name><name name-style="western"><surname>Dieu-Nosjean</surname><given-names>M</given-names></name><name name-style="western"><surname>Ranade</surname><given-names>K</given-names></name><name name-style="western"><surname>Schmidt</surname><given-names>G</given-names></name><name name-style="western"><surname>Higgs</surname><given-names>BW</given-names></name><name name-style="western"><surname>Steele</surname><given-names>KE</given-names></name></person-group><article-title>Automated image analysis of NSCLC biopsies to predict response to anti-PD-L1 therapy</article-title><source>J Immunother Cancer</source><year>2019</year><month>05</month><day>06</day><volume>7</volume><issue>1</issue><fpage>121</fpage><comment><ext-link xlink:href="https://jitc.bmj.com/lookup/pmidlookup?view=long&amp;pmid=31060602" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1186/s40425-019-0589-x</pub-id><pub-id pub-id-type="medline">31060602</pub-id><pub-id pub-id-type="pii">10.1186/s40425-019-0589-x</pub-id><pub-id pub-id-type="pmid">31060602</pub-id><pub-id pub-id-type="pmcid">PMC6501300</pub-id></element-citation></ref><ref id="ref9"><label>9</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>Y</given-names></name><name name-style="western"><surname>Brettin</surname><given-names>T</given-names></name><name name-style="western"><surname>Xia</surname><given-names>F</given-names></name><name name-style="western"><surname>Partin</surname><given-names>Ar</given-names></name><name name-style="western"><surname>Shukla</surname><given-names>M</given-names></name><name name-style="western"><surname>Yoo</surname><given-names>H</given-names></name><name name-style="western"><surname>Evrard</surname><given-names>YA</given-names></name><name name-style="western"><surname>Doroshow</surname><given-names>JH</given-names></name><name name-style="western"><surname>Stevens</surname><given-names>RL</given-names></name></person-group><article-title>Converting tabular data into images for deep learning with convolutional neural networks</article-title><source>Sci Rep</source><year>2021</year><month>05</month><day>31</day><volume>11</volume><issue>1</issue><fpage>11325</fpage><comment><ext-link xlink:href="https://doi.org/10.1038/s41598-021-90923-y" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1038/s41598-021-90923-y</pub-id><pub-id pub-id-type="medline">34059739</pub-id><pub-id pub-id-type="pii">10.1038/s41598-021-90923-y</pub-id><pub-id pub-id-type="pmid">34059739</pub-id><pub-id pub-id-type="pmcid">PMC8166880</pub-id></element-citation></ref><ref id="ref10"><label>10</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lara-Abelenda</surname><given-names>FJ</given-names></name><name name-style="western"><surname>Chushig-Muzo</surname><given-names>D</given-names></name><name name-style="western"><surname>Peiro-Corbacho</surname><given-names>P</given-names></name><name name-style="western"><surname>G&#243;mez-Mart&#237;nez</surname><given-names>V</given-names></name><name name-style="western"><surname>W&#228;gner</surname><given-names>AM</given-names></name><name name-style="western"><surname>Granja</surname><given-names>C</given-names></name><name name-style="western"><surname>Soguero-Ruiz</surname><given-names>C</given-names></name></person-group><article-title>Transfer learning for a tabular-to-image approach: a case study for cardiovascular disease prediction</article-title><source>J Biomed Inform</source><year>2025</year><month>05</month><volume>165</volume><fpage>104821</fpage><comment><ext-link xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S1532-0464(25)00050-4" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1016/j.jbi.2025.104821</pub-id><pub-id pub-id-type="medline">40209918</pub-id><pub-id pub-id-type="pii">S1532-0464(25)00050-4</pub-id><pub-id pub-id-type="pmid">40209918</pub-id></element-citation></ref><ref id="ref11"><label>11</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sharma</surname><given-names>A</given-names></name><name name-style="western"><surname>Vans</surname><given-names>E</given-names></name><name name-style="western"><surname>Shigemizu</surname><given-names>D</given-names></name><name name-style="western"><surname>Boroevich</surname><given-names>KA</given-names></name><name name-style="western"><surname>Tsunoda</surname><given-names>T</given-names></name></person-group><article-title>DeepInsight: a methodology to transform a non-image data to an image for convolution neural network architecture</article-title><source>Sci Rep</source><year>2019</year><month>08</month><day>06</day><volume>9</volume><issue>1</issue><fpage>11399</fpage><comment><ext-link xlink:href="https://doi.org/10.1038/s41598-019-47765-6" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1038/s41598-019-47765-6</pub-id><pub-id pub-id-type="medline">31388036</pub-id><pub-id pub-id-type="pii">10.1038/s41598-019-47765-6</pub-id><pub-id pub-id-type="pmid">31388036</pub-id><pub-id pub-id-type="pmcid">PMC6684600</pub-id></element-citation></ref><ref id="ref12"><label>12</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>DH</given-names></name><name name-style="western"><surname>Jung</surname><given-names>HY</given-names></name><name name-style="western"><surname>Kim</surname><given-names>MH</given-names></name><name name-style="western"><surname>Lim</surname><given-names>ME</given-names></name><name name-style="western"><surname>Kim</surname><given-names>DH</given-names></name><name name-style="western"><surname>Han</surname><given-names>YW</given-names></name><name name-style="western"><surname>Lee</surname><given-names>YW</given-names></name><name name-style="western"><surname>Choi</surname><given-names>JH</given-names></name><name name-style="western"><surname>Kim</surname><given-names>SH</given-names></name></person-group><article-title>Trends of clinical decision support system (CDSS)</article-title><source>Electron Telecommun Trends</source><year>2016</year><volume>31</volume><issue>4</issue><fpage>77</fpage><lpage>85</lpage><comment><ext-link xlink:href="https://koreascience.or.kr/article/JAKO201652057195696.view" ext-link-type="uri"/></comment></element-citation></ref><ref id="ref13"><label>13</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Somashekhar</surname><given-names>SP</given-names></name><name name-style="western"><surname>Sep&#250;lveda</surname><given-names>M-j</given-names></name><name name-style="western"><surname>Puglielli</surname><given-names>S</given-names></name><name name-style="western"><surname>Norden</surname><given-names>AD</given-names></name><name name-style="western"><surname>Shortliffe</surname><given-names>EH</given-names></name><name name-style="western"><surname>Rohit Kumar</surname><given-names>C</given-names></name><name name-style="western"><surname>Rauthan</surname><given-names>A</given-names></name><name name-style="western"><surname>Arun Kumar</surname><given-names>N</given-names></name><name name-style="western"><surname>Patil</surname><given-names>P</given-names></name><name name-style="western"><surname>Rhee</surname><given-names>K</given-names></name><name name-style="western"><surname>Ramya</surname><given-names>Y</given-names></name></person-group><article-title>Watson for Oncology and breast cancer treatment recommendations: agreement with an expert multidisciplinary tumor board</article-title><source>Ann Oncol</source><year>2018</year><month>02</month><day>01</day><volume>29</volume><issue>2</issue><fpage>418</fpage><lpage>423</lpage><comment><ext-link xlink:href="https://linkinghub.elsevier.com/retrieve/pii/S0923-7534(19)35072-0" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1093/annonc/mdx781</pub-id><pub-id pub-id-type="medline">29324970</pub-id><pub-id pub-id-type="pii">S0923-7534(19)35072-0</pub-id><pub-id pub-id-type="pmid">29324970</pub-id></element-citation></ref><ref id="ref14"><label>14</label><element-citation publication-type="webpage"><article-title>Gachon University Gil Medical Center adopts IBM Watson for Oncology trained by Memorial Sloan Kettering, marking Watson's first deployment in Korea</article-title><source>IBM UK Newsroom</source><year>2016</year><month>9</month><day>7</day><date-in-citation content-type="access-date">2022-08-14</date-in-citation><comment><ext-link xlink:href="https://tinyurl.com/mr3bccxd" ext-link-type="uri">https://tinyurl.com/mr3bccxd</ext-link></comment></element-citation></ref><ref id="ref15"><label>15</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>KA</given-names></name><name name-style="western"><surname>Kim</surname><given-names>CH</given-names></name><name name-style="western"><surname>Baek</surname><given-names>JH</given-names></name><name name-style="western"><surname>Shim</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Ahn</surname><given-names>SM</given-names></name><name name-style="western"><surname>Ahn</surname><given-names>HK</given-names></name><name name-style="western"><surname>Lee</surname><given-names>U</given-names></name><name name-style="western"><surname>Lee</surname><given-names>SH</given-names></name></person-group><article-title>Concordance assessment and satisfaction of medical professionals for the artificial intelligence Watson</article-title><source>J Health Technol Assess</source><year>2019</year><volume>7</volume><issue>2</issue><fpage>112</fpage><lpage>118</lpage><comment><ext-link xlink:href="https://www.kahta.or.kr/journal-1/vol.7-no.2-%3A-dec.-2019" ext-link-type="uri"/></comment></element-citation></ref><ref id="ref16"><label>16</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>W</given-names></name><name name-style="western"><surname>Ahn</surname><given-names>SM</given-names></name><name name-style="western"><surname>Chung</surname><given-names>J</given-names></name><name name-style="western"><surname>Kim</surname><given-names>KO</given-names></name><name name-style="western"><surname>Kwon</surname><given-names>KA</given-names></name><name name-style="western"><surname>Kim</surname><given-names>Y</given-names></name><name name-style="western"><surname>Sym</surname><given-names>S</given-names></name><name name-style="western"><surname>Shin</surname><given-names>D</given-names></name><name name-style="western"><surname>Park</surname><given-names>I</given-names></name><name name-style="western"><surname>Lee</surname><given-names>U</given-names></name><name name-style="western"><surname>Baek</surname><given-names>J</given-names></name></person-group><article-title>Assessing concordance with Watson for Oncology, a cognitive computing decision support system for colon cancer treatment in Korea</article-title><source>JCO Clin Cancer Inform</source><year>2018</year><month>12</month><issue>2</issue><fpage>1</fpage><lpage>8</lpage><comment><ext-link xlink:href="https://pubmed.ncbi.nlm.nih.gov/30652564/" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1200/cci.17.00109</pub-id><pub-id pub-id-type="pmid">30652564</pub-id></element-citation></ref><ref id="ref17"><label>17</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>J</given-names></name><name name-style="western"><surname>Baek</surname><given-names>J</given-names></name><name name-style="western"><surname>Sym</surname><given-names>SJ</given-names></name><name name-style="western"><surname>Lee</surname><given-names>KY</given-names></name><name name-style="western"><surname>Lee</surname><given-names>Y</given-names></name></person-group><article-title>A data-driven approach to a chemotherapy recommendation model based on deep learning for patients with colorectal cancer in Korea</article-title><source>BMC Med Inform Decis Mak</source><year>2020</year><month>09</month><day>22</day><volume>20</volume><issue>1</issue><fpage>241</fpage><comment><ext-link xlink:href="https://bmcmedinformdecismak.biomedcentral.com/articles/10.1186/s12911-020-01265-0" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1186/s12911-020-01265-0</pub-id><pub-id pub-id-type="medline">32962726</pub-id><pub-id pub-id-type="pii">10.1186/s12911-020-01265-0</pub-id><pub-id pub-id-type="pmid">32962726</pub-id><pub-id pub-id-type="pmcid">PMC7510149</pub-id></element-citation></ref><ref id="ref18"><label>18</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kwon</surname><given-names>H</given-names></name><name name-style="western"><surname>Park</surname><given-names>J</given-names></name><name name-style="western"><surname>Lee</surname><given-names>Y</given-names></name></person-group><article-title>Stacking ensemble technique for classifying breast cancer</article-title><source>Healthc Inform Res</source><year>2019</year><month>10</month><volume>25</volume><issue>4</issue><fpage>283</fpage><lpage>288</lpage><comment><ext-link xlink:href="https://europepmc.org/abstract/MED/31777671" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.4258/hir.2019.25.4.283</pub-id><pub-id pub-id-type="medline">31777671</pub-id><pub-id pub-id-type="pmid">31777671</pub-id><pub-id pub-id-type="pmcid">PMC6859259</pub-id></element-citation></ref><ref id="ref19"><label>19</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Oh</surname><given-names>SH</given-names></name><name name-style="western"><surname>Baek</surname><given-names>JH</given-names></name><name name-style="western"><surname>Kang</surname><given-names>UG</given-names></name></person-group><article-title>Classification models for chemotherapy recommendation using LGBM for the patients with colorectal cancer</article-title><source>J Korea Soc Comput Inf</source><year>2021</year><volume>26</volume><issue>7</issue><fpage>9</fpage><lpage>17</lpage><comment><ext-link xlink:href="https://journal.kci.go.kr/jksci/archive/articleView?artiId=ART002740822" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.9708/jksci.2021.26.07.009</pub-id></element-citation></ref><ref id="ref20"><label>20</label><element-citation publication-type="webpage"><article-title>Data visualization tools for exploring the global cancer burden in 2022</article-title><source>International Agency for Research on Cancer</source><year>2019</year><date-in-citation content-type="access-date">2022-09-13</date-in-citation><comment><ext-link xlink:href="https://gco.iarc.fr/today/home" ext-link-type="uri">https://gco.iarc.fr/today/home</ext-link></comment></element-citation></ref><ref id="ref21"><label>21</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sung</surname><given-names>H</given-names></name><name name-style="western"><surname>Ferlay</surname><given-names>J</given-names></name><name name-style="western"><surname>Siegel</surname><given-names>RL</given-names></name><name name-style="western"><surname>Laversanne</surname><given-names>M</given-names></name><name name-style="western"><surname>Soerjomataram</surname><given-names>I</given-names></name><name name-style="western"><surname>Jemal</surname><given-names>A</given-names></name><name name-style="western"><surname>Bray</surname><given-names>F</given-names></name></person-group><article-title>Global cancer statistics 2020: GLOBOCAN estimates of incidence and mortality worldwide for 36 cancers in 185 countries</article-title><source>CA Cancer J Clin</source><year>2021</year><month>05</month><volume>71</volume><issue>3</issue><fpage>209</fpage><lpage>249</lpage><comment><ext-link xlink:href="https://onlinelibrary.wiley.com/doi/10.3322/caac.21660" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.3322/caac.21660</pub-id><pub-id pub-id-type="medline">33538338</pub-id><pub-id pub-id-type="pmid">33538338</pub-id></element-citation></ref><ref id="ref22"><label>22</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>EO</given-names></name><name name-style="western"><surname>Eom</surname><given-names>A</given-names></name><name name-style="western"><surname>Song</surname><given-names>R</given-names></name><name name-style="western"><surname>Chae</surname><given-names>YR</given-names></name><name name-style="western"><surname>Lam</surname><given-names>P</given-names></name></person-group><article-title>[Factors influencing quality of life in patients with gastrointestinal neoplasms]</article-title><source>J Korean Acad Nurs</source><year>2008</year><month>10</month><volume>38</volume><issue>5</issue><fpage>649</fpage><lpage>55</lpage><pub-id pub-id-type="doi">10.4040/jkan.2008.38.5.649</pub-id><pub-id pub-id-type="medline">19114753</pub-id><pub-id pub-id-type="pii">200810649</pub-id><pub-id pub-id-type="pmid">19114753</pub-id></element-citation></ref><ref id="ref23"><label>23</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Goel</surname><given-names>MK</given-names></name><name name-style="western"><surname>Khanna</surname><given-names>P</given-names></name><name name-style="western"><surname>Kishore</surname><given-names>J</given-names></name></person-group><article-title>Understanding survival analysis: Kaplan-Meier estimate</article-title><source>Int J Ayurveda Res</source><year>2010</year><month>10</month><volume>1</volume><issue>4</issue><fpage>274</fpage><lpage>8</lpage><comment><ext-link xlink:href="https://europepmc.org/abstract/MED/21455458" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.4103/0974-7788.76794</pub-id><pub-id pub-id-type="medline">21455458</pub-id><pub-id pub-id-type="pmid">21455458</pub-id><pub-id pub-id-type="pmcid">PMC3059453</pub-id></element-citation></ref><ref id="ref24"><label>24</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cox</surname><given-names>DR</given-names></name></person-group><article-title>Regression models and life&#8208;tables</article-title><source>J R Stat Soc Series B Stat Methodol</source><year>1972</year><month>1</month><volume>34</volume><issue>2</issue><fpage>187</fpage><lpage>202</lpage><comment><ext-link xlink:href="https://academic.oup.com/jrsssb/article/34/2/187/7027194" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1111/j.2517-6161.1972.tb00899.x</pub-id></element-citation></ref><ref id="ref25"><label>25</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yeom</surname><given-names>CH</given-names></name><name name-style="western"><surname>Choi</surname><given-names>YS</given-names></name><name name-style="western"><surname>Hong</surname><given-names>YS</given-names></name><name name-style="western"><surname>Park</surname><given-names>YG</given-names></name><name name-style="western"><surname>Lee</surname><given-names>HR</given-names></name></person-group><article-title>Prediction of life expectancy for terminally ill cancer patients based on clinical parameters</article-title><source>J Hosp Palliat Care</source><year>2002</year><month>11</month><day>30</day><volume>5</volume><issue>2</issue><fpage>111</fpage><lpage>124</lpage><comment><ext-link xlink:href="https://koreascience.kr/article/JAKO200223514845312.page#:~:text=Conclusions%20%3A%20In%20terminally%20ill%20cancer%20patients%2C%20we,of%20life%20expectancy%20in%20terminally%20ill%20cancer%20patients" ext-link-type="uri"/></comment></element-citation></ref><ref id="ref26"><label>26</label><element-citation publication-type="confproc"><person-group person-group-type="author"><name name-style="western"><surname>Selvaraju</surname><given-names>RR</given-names></name><name name-style="western"><surname>Cogswell</surname><given-names>M</given-names></name><name name-style="western"><surname>Das</surname><given-names>A</given-names></name><name name-style="western"><surname>Vedantam</surname><given-names>R</given-names></name><name name-style="western"><surname>Parikh</surname><given-names>D</given-names></name><name name-style="western"><surname>Batra</surname><given-names>D</given-names></name></person-group><article-title>Grad-CAM: visual explanations from deep networks via gradient-based localization</article-title><year>2017</year><conf-name>IEEE International Conference on Computer Vision (ICCV)</conf-name><conf-date>2017</conf-date><conf-loc>Venice, Italy</conf-loc><fpage>618</fpage><lpage>626</lpage><comment><ext-link xlink:href="https://ieeexplore.ieee.org/document/8237336" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1109/ICCV.2017.74</pub-id></element-citation></ref><ref id="ref27"><label>27</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>An</surname><given-names>J</given-names></name><name name-style="western"><surname>Joe</surname><given-names>I</given-names></name></person-group><article-title>Attention map-guided visual explanations for deep neural networks</article-title><source>Appl Sci</source><year>2022</year><month>04</month><day>11</day><volume>12</volume><issue>8</issue><fpage>3846</fpage><comment><ext-link xlink:href="https://www.mdpi.com/2076-3417/12/8/3846#:~:text=In%20this%20paper%2C%20we%20propose%20an%20attention-map-guided%20visual,compare%20our%20approach%20with%20other%20state-of-the-art%20XAI%20methods" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.3390/app12083846</pub-id></element-citation></ref><ref id="ref28"><label>28</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Selvaraju</surname><given-names>RR</given-names></name><name name-style="western"><surname>Das</surname><given-names>A</given-names></name><name name-style="western"><surname>Vedantam</surname><given-names>R</given-names></name><name name-style="western"><surname>Cogswell</surname><given-names>M</given-names></name><name name-style="western"><surname>Parikh</surname><given-names>D</given-names></name><name name-style="western"><surname>Batra</surname><given-names>D</given-names></name></person-group><article-title>Grad-CAM: why did you say that?</article-title><source>arXiv. Preprint posted online 2016. [doi: 10.48550/arXiv.1611.07450]</source><comment><ext-link xlink:href="https://arxiv.org/abs/1611.07450" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.48550/arXiv.1611.07450</pub-id></element-citation></ref><ref id="ref29"><label>29</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Preece</surname><given-names>A</given-names></name><name name-style="western"><surname>Harborne</surname><given-names>D</given-names></name><name name-style="western"><surname>Braines</surname><given-names>D</given-names></name><name name-style="western"><surname>Tomsett</surname><given-names>R</given-names></name><name name-style="western"><surname>Chakraborty</surname><given-names>S</given-names></name></person-group><article-title>Stakeholders in explainable AI</article-title><source>arXiv. Preprint posted online 2018. [doi: 10.48550/arXiv.1810.00184]</source><pub-id pub-id-type="doi">10.48550/arXiv.1810.00184</pub-id></element-citation></ref><ref id="ref30"><label>30</label><element-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Gunning</surname><given-names>D</given-names></name></person-group><article-title>Explainable artificial intelligence (XAI)</article-title><source>Defense Advanced Research Projects Agency (DARPA)</source><year>2017</year><date-in-citation content-type="access-date">2025-08-06</date-in-citation><comment><ext-link xlink:href="https://www.darpa.mil/research/programs/explainable-artificial-intelligence" ext-link-type="uri">https://www.darpa.mil/research/programs/explainable-artificial-intelligence</ext-link></comment></element-citation></ref><ref id="ref31"><label>31</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kos</surname><given-names>FT</given-names></name><name name-style="western"><surname>Cecen Kaynak</surname><given-names>S</given-names></name><name name-style="western"><surname>Akt&#252;rk Esen</surname><given-names>S</given-names></name><name name-style="western"><surname>Arslan</surname><given-names>H</given-names></name><name name-style="western"><surname>Uncu</surname><given-names>D</given-names></name></person-group><article-title>Comparison of different machine learning models for predicting long-term overall survival in non-metastatic colorectal cancers</article-title><source>Cureus</source><year>2024</year><month>12</month><volume>16</volume><issue>12</issue><fpage>e75713</fpage><pub-id pub-id-type="doi">10.7759/cureus.75713</pub-id><pub-id pub-id-type="medline">39811215</pub-id><pub-id pub-id-type="pmid">39811215</pub-id><pub-id pub-id-type="pmcid">PMC11730731</pub-id></element-citation></ref><ref id="ref32"><label>32</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gao</surname><given-names>P</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>X</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Z</given-names></name><name name-style="western"><surname>Song</surname><given-names>Y</given-names></name><name name-style="western"><surname>Tong</surname><given-names>L</given-names></name><name name-style="western"><surname>Xu</surname><given-names>Y</given-names></name><name name-style="western"><surname>Yue</surname><given-names>Z</given-names></name><name name-style="western"><surname>Xu</surname><given-names>H</given-names></name></person-group><article-title>Which is a more accurate predictor in colorectal survival analysis? Nine data mining algorithms vs. the TNM staging system</article-title><source>PLoS One</source><year>2012</year><volume>7</volume><issue>7</issue><fpage>e42015</fpage><comment><ext-link xlink:href="https://dx.plos.org/10.1371/journal.pone.0042015" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1371/journal.pone.0042015</pub-id><pub-id pub-id-type="medline">22848691</pub-id><pub-id pub-id-type="pii">PONE-D-12-08028</pub-id><pub-id pub-id-type="pmid">22848691</pub-id><pub-id pub-id-type="pmcid">PMC3404978</pub-id></element-citation></ref><ref id="ref33"><label>33</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Weiser</surname><given-names>MR</given-names></name></person-group><article-title>AJCC 8th edition: colorectal cancer</article-title><source>Ann Surg Oncol</source><year>2018</year><month>06</month><volume>25</volume><issue>6</issue><fpage>1454</fpage><lpage>1455</lpage><pub-id pub-id-type="doi">10.1245/s10434-018-6462-1</pub-id><pub-id pub-id-type="medline">29616422</pub-id><pub-id pub-id-type="pii">10.1245/s10434-018-6462-1</pub-id><pub-id pub-id-type="pmid">29616422</pub-id></element-citation></ref><ref id="ref34"><label>34</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhu</surname><given-names>Y</given-names></name><name name-style="western"><surname>Yang</surname><given-names>SR</given-names></name><name name-style="western"><surname>Wang</surname><given-names>PP</given-names></name><name name-style="western"><surname>Savas</surname><given-names>S</given-names></name><name name-style="western"><surname>Wish</surname><given-names>T</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>J</given-names></name><name name-style="western"><surname>Green</surname><given-names>R</given-names></name><name name-style="western"><surname>Woods</surname><given-names>M</given-names></name><name name-style="western"><surname>Sun</surname><given-names>Z</given-names></name><name name-style="western"><surname>Roebothan</surname><given-names>B</given-names></name><name name-style="western"><surname>Squires</surname><given-names>J</given-names></name><name name-style="western"><surname>Buehler</surname><given-names>S</given-names></name><name name-style="western"><surname>Dicks</surname><given-names>E</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>J</given-names></name><name name-style="western"><surname>Mclaughlin</surname><given-names>JR</given-names></name><name name-style="western"><surname>Parfrey</surname><given-names>PS</given-names></name><name name-style="western"><surname>Campbell</surname><given-names>PT</given-names></name></person-group><article-title>Influence of pre-diagnostic cigarette smoking on colorectal cancer survival: overall and by tumour molecular phenotype</article-title><source>Br J Cancer</source><year>2014</year><month>03</month><day>04</day><volume>110</volume><issue>5</issue><fpage>1359</fpage><lpage>66</lpage><comment><ext-link xlink:href="https://europepmc.org/abstract/MED/24448365" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1038/bjc.2014.6</pub-id><pub-id pub-id-type="medline">24448365</pub-id><pub-id pub-id-type="pii">bjc20146</pub-id><pub-id pub-id-type="pmid">24448365</pub-id><pub-id pub-id-type="pmcid">PMC3950884</pub-id></element-citation></ref><ref id="ref35"><label>35</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bibault</surname><given-names>JE</given-names></name><name name-style="western"><surname>Giraud</surname><given-names>P</given-names></name><name name-style="western"><surname>Housset</surname><given-names>M</given-names></name><name name-style="western"><surname>Durdux</surname><given-names>C</given-names></name><name name-style="western"><surname>Taieb</surname><given-names>J</given-names></name><name name-style="western"><surname>Berger</surname><given-names>A</given-names></name><name name-style="western"><surname>Coriat</surname><given-names>R</given-names></name><name name-style="western"><surname>Chaussade</surname><given-names>S</given-names></name><name name-style="western"><surname>Dousset</surname><given-names>B</given-names></name><name name-style="western"><surname>Nordlinger</surname><given-names>B</given-names></name><name name-style="western"><surname>Burgun</surname><given-names>A</given-names></name></person-group><article-title>Deep learning and radiomics predict complete response after neo-adjuvant chemoradiation for locally advanced rectal cancer</article-title><source>Sci Rep</source><year>2018</year><month>08</month><day>22</day><volume>8</volume><issue>1</issue><fpage>12611</fpage><comment><ext-link xlink:href="https://doi.org/10.1038/s41598-018-30657-6" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1038/s41598-018-30657-6</pub-id><pub-id pub-id-type="medline">30135549</pub-id><pub-id pub-id-type="pii">10.1038/s41598-018-30657-6</pub-id><pub-id pub-id-type="pmid">30135549</pub-id><pub-id pub-id-type="pmcid">PMC6105676</pub-id></element-citation></ref><ref id="ref36"><label>36</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jiang</surname><given-names>F</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Zhi</surname><given-names>H</given-names></name><name name-style="western"><surname>Dong</surname><given-names>Y</given-names></name><name name-style="western"><surname>Li</surname><given-names>H</given-names></name><name name-style="western"><surname>Ma</surname><given-names>S</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Dong</surname><given-names>Q</given-names></name><name name-style="western"><surname>Shen</surname><given-names>H</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name></person-group><article-title>Artificial intelligence in healthcare: past, present and future</article-title><source>Stroke Vasc Neurol</source><year>2017</year><month>12</month><volume>2</volume><issue>4</issue><fpage>230</fpage><lpage>243</lpage><comment><ext-link xlink:href="https://svn.bmj.com/lookup/pmidlookup?view=long&amp;pmid=29507784" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1136/svn-2017-000101</pub-id><pub-id pub-id-type="medline">29507784</pub-id><pub-id pub-id-type="pii">svn-2017-000101</pub-id><pub-id pub-id-type="pmid">29507784</pub-id><pub-id pub-id-type="pmcid">PMC5829945</pub-id></element-citation></ref><ref id="ref37"><label>37</label><element-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Katzman</surname><given-names>JL</given-names></name><name name-style="western"><surname>Shaham</surname><given-names>U</given-names></name><name name-style="western"><surname>Cloninger</surname><given-names>A</given-names></name><name name-style="western"><surname>Bates</surname><given-names>J</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>T</given-names></name><name name-style="western"><surname>Kluger</surname><given-names>Y</given-names></name></person-group><article-title>DeepSurv: personalized treatment recommender system using a Cox proportional hazards deep neural network</article-title><source>BMC Med Res Methodol</source><year>2018</year><month>02</month><day>26</day><volume>18</volume><issue>1</issue><fpage>24</fpage><comment><ext-link xlink:href="https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/s12874-018-0482-1" ext-link-type="uri"/></comment><pub-id pub-id-type="doi">10.1186/s12874-018-0482-1</pub-id><pub-id pub-id-type="medline">29482517</pub-id><pub-id pub-id-type="pii">10.1186/s12874-018-0482-1</pub-id><pub-id pub-id-type="pmid">29482517</pub-id><pub-id pub-id-type="pmcid">PMC5828433</pub-id></element-citation></ref></ref-list></back></article>
        
    </metadata>
</record>
    </GetRecord>

</OAI-PMH>