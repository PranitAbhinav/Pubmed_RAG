


<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
    <responseDate>2025-09-09T13:47:18Z</responseDate>
    <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:12405547" metadataPrefix="pmc">https://pmc.ncbi.nlm.nih.gov/api/oai/v1/mh/</request>
    
    <GetRecord>
        <record>
    <header>
    <identifier>oai:pubmedcentral.nih.gov:12405547</identifier>
    <datestamp>2025-09-04</datestamp>
    
        
        <setSpec>scirep</setSpec>
        
    
        
        <setSpec>pmc-open</setSpec>
        
    
</header>
    <metadata>
        
        <article xmlns="https://jats.nlm.nih.gov/ns/archiving/1.4/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.4/ https://jats.nlm.nih.gov/archiving/1.4/xsd/JATS-archivearticle1-4.xsd" article-type="research-article" xml:lang="en" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-id journal-id-type="pmc-domain-id">1579</journal-id><journal-id journal-id-type="pmc-domain">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12405547</article-id><article-id pub-id-type="pmcid-ver">PMC12405547.1</article-id><article-id pub-id-type="pmcaid">12405547</article-id><article-id pub-id-type="pmcaiid">12405547</article-id><article-id pub-id-type="pmid">40897789</article-id><article-id pub-id-type="doi">10.1038/s41598-025-16923-4</article-id><article-id pub-id-type="publisher-id">16923</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Precision diagnosis of citrus leaf diseases using image enhancement and nonlinear fuzzy ranking ensemble approach NLFuRBe</article-title></title-group><contrib-group><contrib contrib-type="author" equal-contrib="yes"><name name-style="western"><surname>Kaur</surname><given-names initials="B">Bobbinpreet</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name name-style="western"><surname>Gupta</surname><given-names initials="SK">Shashi Kant</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" equal-contrib="yes"><name name-style="western"><surname>Janarthan</surname><given-names initials="M">Midhunchakkaravarthy</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name name-style="western"><surname>Alsekait</surname><given-names initials="DM">Deema Mohammed</given-names></name><address><email>Dmalsekait@pnu.edu.sa</email></address><xref ref-type="aff" rid="Aff3">3</xref></contrib><contrib contrib-type="author" corresp="yes" equal-contrib="yes"><name name-style="western"><surname>AbdElminaam</surname><given-names initials="DS">Diaa Salama</given-names></name><address><email>diaa.salama@miuegypt.edu.eg</email></address><xref ref-type="aff" rid="Aff4">4</xref><xref ref-type="aff" rid="Aff5">5</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02yd50j87</institution-id><institution-id institution-id-type="GRID">grid.512179.9</institution-id><institution-id institution-id-type="ISNI">0000 0004 1781 393X</institution-id><institution>Lincoln University College, </institution></institution-wrap>Petaling Jaya-47301, Selangor, Malaysia </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/057d6z539</institution-id><institution-id institution-id-type="GRID">grid.428245.d</institution-id><institution-id institution-id-type="ISNI">0000 0004 1765 3753</institution-id><institution> Centre for Research Impact &amp; Outcome Chitkara University Institute of Engineering and Technology, </institution><institution> Chitkara University, Rajpura, 140401, </institution></institution-wrap>Punjab, India </aff><aff id="Aff3"><label>3</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/05b0cyh02</institution-id><institution-id institution-id-type="GRID">grid.449346.8</institution-id><institution-id institution-id-type="ISNI">0000 0004 0501 7602</institution-id><institution>Department of Information Technology, College of Computer and Information Sciences, </institution><institution>Princess Nourah bint Abdulrahman University, </institution></institution-wrap>P.O. Box 84428, 11671 Riyadh, Saudi Arabia </aff><aff id="Aff4"><label>4</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03tn5ee41</institution-id><institution-id institution-id-type="GRID">grid.411660.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 0621 2741</institution-id><institution>Faculty of Computers and Artificial Intelligence, </institution><institution>Benha University, </institution></institution-wrap>Benha, Egypt </aff><aff id="Aff5"><label>5</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/001drnv35</institution-id><institution-id institution-id-type="GRID">grid.449338.1</institution-id><institution-id institution-id-type="ISNI">0000 0004 0645 5794</institution-id><institution>Jadara Research Center, Jadara University, </institution></institution-wrap>21110 Irbid, Jordan </aff></contrib-group><pub-date pub-type="epub"><day>2</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type="pmc-issue-id">478255</issue-id><elocation-id>32296</elocation-id><history><date date-type="received"><day>15</day><month>6</month><year>2025</year></date><date date-type="accepted"><day>20</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-04 00:25:59.930"><day>04</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri content-type="pmc-pdf" xlink:href="41598_2025_Article_16923.pdf"/><abstract id="Abs1"><p id="Par1">Citrus fruits, especially lemons, play a vital economic and nutritional role worldwide but are increasingly threatened by a wide range of diseases that diminish yield quality and quantity. Traditional manual and automated methods for disease detection requires domain expert, ample observation time, and is often ineffective during early infection stages. This paper presents a novel automated approach for the symptom based detection and classification of citrus leaf diseases using a nonlinear Fuzzy Rank-Based Ensemble (NL-FuRBE) methodology, enhanced by image quality improvement techniques. The study emphasizes the significance of timely disease diagnosis in citrus crops, which are vital for global food security and economic stability. The methodology begins with image quality enhancement through Vector-Valued Anisotropic Diffusion (VAD) and morphological filtering, evaluated using PSNR, SSIM, and NIQE metrics to ensure optimal visual clarity for classifier input. The core ensemble integrates three deep learning (DL) architectures&#8211;VGG19, AlexNet, and Xception&#8211;using a fuzzy rank-based scoring mechanism built on nonlinear transformations (exponential, tanh, and sigmoid functions) to address prediction uncertainty and model bias. A comprehensive dataset of lemon leaf diseases, consisting of 1354 images across nine classes, was utilized for training and evaluation. Experimental results using five-fold cross-validation demonstrate that the proposed model achieves superior performance with an average accuracy of 96.51%, outperforming conventional ensemble and state-of-the-art approaches. The results validate the proposed NL-FuRBE as an effective, automated, and cost-efficient tool for precision agriculture and early disease diagnosis in citrus farming.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>AlexNet</kwd><kwd>Citrus diseases</kwd><kwd>Morphological filter</kwd><kwd>VEctor Valued anisotropic diffusion</kwd><kwd>Convolutional Layers</kwd><kwd>Classification</kwd><kwd>Deep Learning</kwd><kwd>Ensemble</kwd><kwd>Feature Extraction Methods</kwd><kwd>FUZZY fusion</kwd><kwd>Disease symptoms</kwd><kwd>Image enhancement</kwd><kwd>Greening</kwd><kwd>Anthracnose</kwd><kwd>VGG-19</kwd><kwd>Xception</kwd><kwd>Image Processing</kwd><kwd>Computer aided diagnosis</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Electrical and electronic engineering</kwd><kwd>Plant physiology</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100004242</institution-id><institution>Princess Nourah Bint Abdulrahman University</institution></institution-wrap></funding-source><award-id>PNURSP2025R435</award-id><award-id>PNURSP2025R435</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Agricultural and horticultural research aims towards enhancing the food and fruit production and quality while reducing costs and maximizing profits. Fruit trees significantly contribute to a state&#8217;s economic prosperity. The citrus plant, rich in vitamin C and widely used in the Indian subcontinent, is among the best-recognized fruit species in the citrus family. Citrus fruits and leaves are utilized to manufacture jams, candies, confectionary, and other agricultural goods that promote human health. Timely diagnosis of infections is crucial for enhancing plant yield, a job that presents significant challenges<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. With around 150 million metric tons yearly production, citrus fruits covers the major commercially important fruit crops grown worldwide. The statistics of citrus production are shown in table <xref rid="Tab1" ref-type="table">1</xref>.Especially important in China, Brazil, the United States, India, and nations in the Mediterranean basin, citrus farming is a vital aspect of their agricultural economy. Many viruses and pathogens able to destroy whole orchards affect the viability and output of citrus trees, therefore compromising food security and resulting significant financial losses. Timely interventions and crop health management depend on correct diagnosis and fast disease identification<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR3">3</xref></sup>. Because of their quick spread and sometimes asymptomatic first phases, citrus diseases&#8211;including black spot, citrus canker, Huanglongbing (HLB or citrus greening), scab, and many fungal infections&#8211;cause great difficulties. Conventional methods of disease detection have mostly relied on laboratory-based molecular diagnostics instruments or hand-examining skilled experts.<table-wrap id="Tab1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Statistics of citrus production globally (Production in million tons, loss in USD Billions)<sup><xref ref-type="bibr" rid="CR10">10</xref>&#8211;<xref ref-type="bibr" rid="CR12">12</xref></sup>.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1"><bold>Country</bold></th><th align="left" colspan="1" rowspan="1"><bold>Prod.</bold></th><th align="left" colspan="1" rowspan="1"><bold>Major Diseases</bold></th><th align="left" colspan="1" rowspan="1"><bold>Infected Fruits (%)</bold></th><th align="left" colspan="1" rowspan="1"><bold>Loss</bold></th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">USA (Florida, California)</td><td align="left" colspan="1" rowspan="1">15.8</td><td align="left" colspan="1" rowspan="1">HLB (Citrus Greening), Citrus Canker</td><td align="left" colspan="1" rowspan="1">40&#8211;50% (HLB in FL)</td><td align="left" colspan="1" rowspan="1">$4.5</td></tr><tr><td align="left" colspan="1" rowspan="1">Brazil</td><td align="left" colspan="1" rowspan="1">18.7</td><td align="left" colspan="1" rowspan="1">Citrus Black Spot, HLB</td><td align="left" colspan="1" rowspan="1">25&#8211;30%</td><td align="left" colspan="1" rowspan="1">$3.2</td></tr><tr><td align="left" colspan="1" rowspan="1">China</td><td align="left" colspan="1" rowspan="1">43.0</td><td align="left" colspan="1" rowspan="1">HLB, Citrus Yellow Shoot</td><td align="left" colspan="1" rowspan="1">20&#8211;25%</td><td align="left" colspan="1" rowspan="1">$5.0</td></tr><tr><td align="left" colspan="1" rowspan="1">India</td><td align="left" colspan="1" rowspan="1">12.5</td><td align="left" colspan="1" rowspan="1">CTV, HLB</td><td align="left" colspan="1" rowspan="1">15&#8211;20%</td><td align="left" colspan="1" rowspan="1">$1.8</td></tr><tr><td align="left" colspan="1" rowspan="1">Mexico</td><td align="left" colspan="1" rowspan="1">8.2</td><td align="left" colspan="1" rowspan="1">Citrus Canker, HLB</td><td align="left" colspan="1" rowspan="1">18&#8211;22%</td><td align="left" colspan="1" rowspan="1">$1.4</td></tr><tr><td align="left" colspan="1" rowspan="1">Spain</td><td align="left" colspan="1" rowspan="1">6.9</td><td align="left" colspan="1" rowspan="1">HLB, Black Spot</td><td align="left" colspan="1" rowspan="1">10&#8211;15%</td><td align="left" colspan="1" rowspan="1">$1.0</td></tr></tbody></table></table-wrap></p><p id="Par3">While these approaches have helped the sector for decades, they are labor-intensive, time-consuming, subjective, and often find diseases only after significant development, therefore causing lasting crop harm<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>. The prevention and treatment of crop diseases have consistently been a critical concern in agriculture, evolving from traditional manual techniques to instrumental detection and, more recently, to advanced intelligence technologies. Disease recognition technology has experienced multiple phases of advancement<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. Contemporary research emphasizes the identification, prevention, and management of prevalent diseases, although there is an absence of systematic research data and efficient identification techniques for developing or geographically unique disease species.<fig id="Fig1" position="float" orientation="portrait"><label>Fig. 1</label><caption><p>Layout for citrus disease detection using deep learning.</p></caption><graphic id="MO1" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig1_HTML.jpg"/></fig></p><p id="Par4">The diagnosis of categories of citrus diseases presently depends on symptomatic observation (either through images or manual field examination) and laboratory testing; nevertheless, these procedures frequently exhibit delays. Laborious, time-consuming, and requiring professional involvement are conventional disease detection methods that are more prone to erroneous detection. Deep learning is a disruptive technology in agriculture that, via data-driven decision-making and automation, is changing many aspects of farming. Deep learning, applied with artificial intelligence (AI), enables researchers, agronomists, and farmers to improve crop quality, lower costs, and increase output<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. Deep learning techniques have lately been a reliable and exact choice for early identification of citrus illnesses. Image-based disease detection has shown remarkable performance for convolutional neural networks (CNNs), transfer learning, and hybrid deep learning models<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>, therefore enabling automated, scalable, accurate diagnosis. These systems can effectively identify patterns and symptoms using large databases of images of citrus leaf, therefore helping farmers and agricultural experts to quickly reduce disease spread<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>. The generalized layout of citrus disease detection using deep learning is shown in Fig. <xref rid="Fig1" ref-type="fig">1</xref>.</p><p id="Par5">The development of artificial intelligence and computer vision has revolutionized plant pathology by offering efficient means of automated, fast, and exact disease diagnosis. Deep learning systems have become powerful tools for picture analysis-based plant disease diagnosis, since they are quite good at identifying small visual patterns connected to different diseases. From traditional Convolutional Neural Networks (CNNs) to advanced Vision Transformers and self-supervised learning models, the evolution of deep learning architectures has steadily raised the precision and dependability of citrus disease detection systems<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>.</p></sec><sec id="Sec2"><title>Motivation</title><p id="Par6">Citrus fruits, which are one of the most consumed sources of vitamin C, are considered economically and nutritionally significant globally. These fruits fields are prone to the increase in number of diseases due to various biotic and abiotic factors. Timely detection and identification is necessary for reducing production loss, and promoting sustainable agricultural practices. Nonetheless, manual diagnosis by specialists is labor-intensive, susceptible to errors, and frequently unattainable in remote agricultural regions. Deep learning models have a lot of potential when it comes to classifying plant diseases, but they often fail due to the anomalies present in the images used for training the model. Also the problems like class imbalance, noise present in the input samples leads to the poor classifier performance. To address these issues, an automated, highly accurate disease detection system is required which can considerably reduce the financial and production losses. This work introduces a novel ensemble learning framework that integrates improved image quality, transfer learning, and a fuzzy rank-based decision fusion method to boost classification robustness and reliability.</p></sec><sec id="Sec3"><title>Major contributions</title><p id="Par7">The novelty of this research lies in its integration of advanced image pre-processing (including noise minimization and enhancement), and a nonlinear fuzzy ranking mechanism deployed for citrus leaf disease classification. Unlike traditional ensemble models that rely on direct class probabilities or majority voting, the proposed ensemble framework employs a fuzzy rank-based fusion strategy where classifier outputs are transformed using nonlinear functions (exponential and hyperbolic tangent) to capture subtle variations in confidence scores. This unique transformation enables more accurate decision-making for disease detection. The major contributions of the work presented in this article include:<list list-type="bullet"><list-item><p id="Par8">The quality of images available in the dataset is enhanced in order to improve the classifier accuracy.</p></list-item><list-item><p id="Par9">Design of an efficient denoising mechanism for better PSNR and improved image quality.</p></list-item><list-item><p id="Par10">Developed an ensemble architecture that utilizes various base classifiers and consolidates the individual decisions through a novel fuzzy rank assignment logic.</p></list-item><list-item><p id="Par11">Implementation of a nonlinear Fuzzy Ranking Mechanism for class probability scoring, where the class probabilities of each base learner undergo nonlinear transformations based on the exponential and hyperbolic tangent (tanh) functions. These transformed scores are then mapped to corresponding rank values.</p></list-item><list-item><p id="Par12">The proposed approach is resistant to biased decisions of individual classifiers, as it effectively provides final predictions based on a robust ensemble strategy.</p></list-item><list-item><p id="Par13">The proposed method is automated, computationally efficient, and cost-effective, ensuring the ecological and economic significance of citrus plants and their yields remains maintained.</p></list-item><list-item><p id="Par14">The proposed work outperforms existing methods on a newly developed Lemon disease dataset [31].</p></list-item></list></p></sec><sec id="Sec4"><title>Literature review</title><p id="Par15">Accurate and automated detection techniques have been developed in response to citrus crops&#8217; increasing sensitivity to diseases such citrus canker, black spot, and Huanglongbing (HLB). Conventional manual examination methods produce inconsistent and delayed diagnosis by tedious, time-consuming, often subjective nature. Because deep learning can independently extract hierarchical features from complex image data, it has become a powerful tool in agricultural image analysis in recent years. Emphasizing model designs, datasets, pre-processing techniques, and performance measures, this section reviews present development in citrus disease detection with deep learning models. The convolutional neural network (CNN)<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> can autonomously extract image features that are more expressive and resilient. By autonomously acquiring hierarchical feature representations inside the image, it can enhance comprehension of the image structure and augment classification accuracy. Diverse IoT-enabled gadgets and sensors are extensively employed to capture images of fruit. Typically, hyperspectral functionality yields 2D statistics over time and examines three-dimensional data comprehensively. Based on the filtering mechanism, multispectral images and imaging systems are categorized into two types: optical filtration and digitally adjustable filters<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. The authors in<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> employed transfer learning on the PlantVillage dataset to identify 26 types of illnesses. They attained a classification accuracy of 99.35% with AlexNet and GoogleNet. Citrus fruits such as lime and orange, exhibiting pharmacological degradation, possess diminished economic viability. The scientist presents a methodology for image analysis to identify disease kinds in X-ray images of lemons and oranges. The authors in<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> employed various networks, such as AlexNet, VGG16, GoogLeNet, MobileNetV2, and SqueezeNet, which were trained by transfer learning, in their investigation of tomato leaf disease identification. The experimental findings indicated that the VGG16 network achieved the greatest recognition accuracy of 99.17%. Image enhancement can be utilized to identify and categorize diseases more economically. The suggested approach implements an advanced Image Principal Component Analysis to reduce the dimensionality of orange images. The Edge identification module in the proposed model primarily pulls seven features from the orange fruit picture dataset. A deep learning algorithm is subsequently utilized to identify the circumstances of orange fruit at the initial stages of growth<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>. The authors in<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> employed fine-tuned Caffenet networks to identify several crop leaf diseases, attaining an average accuracy of 96.3%. The authors in<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> employed a CNN model with eight hidden layers, training it on the PlantVillage dataset and ultimately attaining an accuracy of 98.4%. The authors in<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> employed a transfer learning approach to identify six illness categories and healthy tomato photos utilizing pre-trained AlexNet and VGG16 models, achieving accuracies of 97.29% and 97.49%, respectively. The authors proposed<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> enhanced parameter utilization by introducing a weakly densenet-16 network and used a cross-channel feature fusion technique, achieving 93.33% accuracy on a citrus pest and disease dataset. Recurrent Neural Networks (RNNs) are a category of neural network models that incorporate the immediately preceding step in the process output as direct input in the first stage. The predominant applications of RNNs are sequence classification, sentiment analysis, image processing, and video analysis<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. The authors in<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> created a transformer model that effectively classified tomato leaf images (including both regular and complex backgrounds) into 13 categories, with an accuracy of 93.51%. It necessitates reduced storage capacity and abbreviated training duration, and can be integrated with IoT, drones, and other apparatus for real-time surveillance. The authors proposed<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> discuss many types of convolutional neural networks (CNNs) that were presented. Methods such as Self-Structured Convolutional Neural Networks (SSCNN) and MobileNet were employed for citrus leaf segmentation. The database was initially created utilizing smartphones in this study. Subsequently, both comprehensive models underwent training utilizing identical datasets of citrus plants. The Mobile Net CNN training achieves a peak accuracy of 98%, with a verification accuracy of 92% at the 10th iteration. The authors proposed<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> proposed a method for identifying citrus diseases with deep metric learning, specifically tailored for sparse data. This framework analyzes data utilizing resource-limited devices such as mobile phones. It integrates a network patch utilizing class action with distinct modules (focus, collection, and fundamental neural network classes) to identify different citrus diseases. Implement a deep metric-based composition by partitioning the leaf into segments. The precision of both categories was 95.04%. Classification accuracy was employed as a metric for evaluation, with findings given following five-fold validation. The authors proposed<sup><xref ref-type="bibr" rid="CR27">27</xref></sup> thoroughly assessed the utilization of IoT and DL models for the monitoring and categorization of plant diseases. The benefits and drawbacks of various architectures were examined, and deficiencies in current research were recognized. Their performance on publicly accessible datasets was analyzed to establish a benchmark for identifying the ideal model. In summary, deep learning, characterized by automatic feature extraction, advanced semantic comprehension, robust generalization capabilities, streamlined processing, and excellent scalability, has demonstrated exceptional effectiveness in picture classification. Deep learning, as a prevalent contemporary approach, offers robust theoretical foundations and practical insights for the categorization of citrus leaf diseases<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>. A contemporary AlexNet architecture employing deep learning has been effectively applied for disease detection. A 2 stage Deep CNN model was proposed in<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>. The major novelty s extraction of target area using region proposal network followed by classification into a subsequent disease class. The value of achieved accuracy in detection is 94.37%.Particularly targeting Greening disease detection, the authors in<sup><xref ref-type="bibr" rid="CR30">30</xref></sup> developed a faster RCNN architecture which achieved an Average Precision of 84.13%. Table <xref rid="Tab2" ref-type="table">2</xref> portrays the shortcomings of the existing methods for citrus disease detection.<table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Addressing shortcomings of existing citrus disease detection algorithms.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Sr. No.</th><th align="left" colspan="1" rowspan="1">Shortcomings of Existing Algorithms</th><th align="left" colspan="1" rowspan="1">Proposed Work&#8217;s Solution</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">Manual feature extraction or shallow models limit generalization<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup></td><td align="left" colspan="1" rowspan="1">Uses deep transfer learning models (AlexNet, VGG19, Xception) that automatically learn hierarchical, robust features</td></tr><tr><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">Overfitting due to small or imbalanced datasets<sup><xref ref-type="bibr" rid="CR21">21</xref>,<xref ref-type="bibr" rid="CR26">26</xref></sup></td><td align="left" colspan="1" rowspan="1">Implements data augmentation and five-fold cross-validation to improve generalization and reduce bias</td></tr><tr><td align="left" colspan="1" rowspan="1">3</td><td align="left" colspan="1" rowspan="1">Performance degradation due to image quality issues<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup></td><td align="left" colspan="1" rowspan="1">Applies VAD and Top-Hat/Bottom-Hat filters for image quality enhancement measured in terms of subjective and objective evaluation metrics.</td></tr><tr><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">The application of Single-deep learning model is prone to failure on specific disease types<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup></td><td align="left" colspan="1" rowspan="1">Employs ensemble learning using three diverse models, increasing robustness across diverse disease classes.</td></tr><tr><td align="left" colspan="1" rowspan="1">5</td><td align="left" colspan="1" rowspan="1">Lack of interpretability or sensitivity analysis of errors<sup><xref ref-type="bibr" rid="CR27">27</xref>,<xref ref-type="bibr" rid="CR30">30</xref></sup></td><td align="left" colspan="1" rowspan="1">Performs class-wise performance evaluation for pre-defined as well as ensemble deep learning models.</td></tr><tr><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">Limited disease types or outdated datasets used<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR29">29</xref></sup></td><td align="left" colspan="1" rowspan="1">Trained and evaluated on a recent publicly available citrus disease dataset covering multiple categories</td></tr></tbody></table></table-wrap></p><sec id="Sec5"><title>Research gaps</title><p id="Par16">Although basic machine learning and deep learning approaches have demonstrated efficacy and widespread application in crop disease prediction, several prior studies encountered challenges in enhancing classification accuracy rates to a significant degree. Furthermore, most research employs standard CNNs (e.g., VGG, ResNet) without customizing them to the distinct visual characteristics of citrus diseases in terms of agricultural imagery.<table-wrap id="Tab3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Research questions and corresponding rationales.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Research Question</th><th align="left" colspan="1" rowspan="1">Rationale for Question Design</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1"><bold>RQ1.</bold> What is the impact of image noise on the performance of deep learning-based citrus disease detection systems?</td><td align="left" colspan="1" rowspan="1">Images captured for citrus disease detection are often affected by various types of noise and distortions. Investigating the impact of such noise is essential to understand how model performance degrades.</td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>RQ2.</bold> How effective are deep learning and ensemble models in accurately detecting and classifying citrus leaf diseases from real-world images?</td><td align="left" colspan="1" rowspan="1">Individual deep learning models (such as CNNs, ResNet, or MobileNet) have demonstrated good accuracy. Ensemble models, which combine predictions from multiple classifiers, offer the potential for improved performance.</td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>RQ3.</bold> To what extent does the proposed model outperform state-of-the-art deep learning methods in detecting citrus diseases?</td><td align="left" colspan="1" rowspan="1">To determine the comparative efficacy of the proposed ensemble approach by benchmarking it against recognized models using quantitative metrics.</td></tr></tbody></table></table-wrap></p><p id="Par17">Also inadequate parameter and layer selection in the neural network model has resulted in performance loss. Noise present in the images still remains an under-explored challenge thereby posing an impact on model accuracy. The proposed approach in the first stage caters the impact of noise by developing noise-aware architectures or denoising models for pre-processing The proposed Fuzzy rank based ensemble model employs integration of four base learners based upon their individual rankings for the classification of citrus diseases in both fruit and leaf images. A comparison with base learner&#8217;s performance and proposed ensemble model is also presented.</p></sec><sec id="Sec6"><title>Rationale for research question design</title><p id="Par18">The study conducted in this work aims to thoroughly examine the practical obstacles and technical deficiencies in the application of deep learning for citrus disease identification. Considering the economic importance of citrus crops and the rising incidence of diseases like HLB, Citrus Canker, and Black Spot, it is essential to establish reliable and precise detection systems. Considering the survey conducted for the existing state of the art techniques to detect diseases prevailing in citrus fruits the questions were designed and are listed in table <xref rid="Tab3" ref-type="table">3</xref></p></sec></sec><sec id="Sec7"><title>Dataset and proposed method</title><p id="Par19">The primary finding of this study is that automated systems for detecting citrus diseases can achieve significantly greater accuracy and reliability in real-time scenarios when employing effective ensemble approaches and enhanced picture pre-processing techniques. Due to the efficacy of NL-FuRBE, it establishes a new benchmark for detecting citrus diseases and provides precision farmers with an economical, scalable method for early disease identification. This method can be enhanced in the future to incorporate mixed datasets and real-time field operations utilising low-power devices. This would enhance integrated agricultural management and provide sufficient food availability for the global population. The overall framework of the proposed methodology demonstrated through Fig. <xref rid="Fig2" ref-type="fig">2</xref> and step by step process is represented through Algorithm&#160;4.<fig id="Fig2" position="float" orientation="portrait"><label>Fig. 2</label><caption><p>Flow diagram for the proposed methodology.</p></caption><graphic id="MO2" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig2_HTML.jpg"/></fig></p><sec id="Sec8"><title>Dataset and its acquisition</title><p id="Par20">The prevalence of diseases in fruits may lead to widespread loss of the yields. To control the spreading of disease it is important to detect the disease at its onset. <fig id="Fig3" position="float" orientation="portrait"><label>Fig. 3</label><caption><p>Sample images from different categories of diseases<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>.</p></caption><graphic id="MO3" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig3_HTML.jpg"/></fig></p><p id="Par21">The design of an automated system for disease detection requires the image dataset to train the deep learning model. The proposed model is trained using public available lemon disease dataset<sup><xref ref-type="bibr" rid="CR31">31</xref></sup> released online in February 2025, consisting of 1354 raw images with 9 class- 8 types of diseases (anthracnose, bacterial blight, citrus canker, curl virus, deficiency leaf, dry leaf, healthy leaf, sooty mould, and spider mites) and Healthy class.This latest dataset chosen for the work reflects the current trends and broader disease types coverage and is captured under wider range of conditions. The sample images belonging to each disease category are shown in Fig. <xref rid="Fig3" ref-type="fig">3</xref>.</p></sec><sec id="Sec9"><title>Data pre-processing and balancing</title><p id="Par22">Pre-processing and data balancing are essential elements of deep learning, guaranteeing effective model training and applicability in diverse situations. Some classes may be outnumbered by datasets in comparison to others. Data balancing strategies, such oversampling minority classes, under sampling majority classes, or employing SMOTE to create synthetic data, can address this issue. Resizing, normalizing, flipping or rotating images, and noise reduction are all operations that enhance the quality of input during pre-processing. The amalgamation of these operations improves the model&#8217;s performance, reduces its bias, and enables accurate identification of all classes. The principal benefits of these strategies include adaptability to new data, less overfitting, enhanced consistency in performance across all classes, and increased model accuracy. This research involves the oversampling of minority classes. Figure <xref rid="Fig4" ref-type="fig">4</xref> illustrates a strong class imbalance regarding disease prevailing in citrus fruits, with a highly varying number of images per class. The preconfigured models employed on this imbalanced dataset will exhibit suboptimal performance regarding disease detection in citrus fruits, resulting in diminished accuracy and skewed predictions. Figure <xref rid="Fig5" ref-type="fig">5</xref> illustrates the balanced count for each class, facilitating equitable learning across all classes to enhance prediction and accuracy. The images from each class are reduced to a resolution of 128x128x3 due to varying original resolutions. Random transformations like as orientation, scaling, and flipping are applied to the training dataset. Images in the training dataset are randomly rotated by &#177;40 degrees and moved horizontally and vertically by 20%, respectively. The training dataset has a 20% variance in magnification adjustments. Furthermore, horizontal flipping of images is executed while preserving their intrinsic significance. During the execution of operations on images, such as rotation, scaling, and flipping, certain pixels may stay unoccupied. The nearest pixel value is utilized to address these gaps.<fig id="Fig4" position="float" orientation="portrait"><label>Fig. 4</label><caption><p>Sample images from different categories of diseases<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>.</p></caption><graphic id="MO4" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig4_HTML.jpg"/></fig><fig id="Fig5" position="float" orientation="portrait"><label>Fig. 5</label><caption><p>Sample images from different categories of diseases<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>.</p></caption><graphic id="MO5" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig5_HTML.jpg"/></fig></p></sec><sec id="Sec10"><title>Edge preserving image noise removal</title><sec id="Sec11"><title>Vector valued-anisotropic diffusion(VAD)</title><p id="Par23">In the diagnosis of citrus diseases by deep learning, image quality is crucial for the precise identification and classification of illness symptoms. Images obtained in actual agricultural settings frequently exhibit numerous types of noise, including motion blur, variable lighting, shadows, and background clutter, which might mask essential disease indicators such as lesions, spots, or discolouration. Denoising is an essential pre-processing technique that improves image quality by eliminating undesired distortions. Denoising approaches enhance the visual quality of input data, allowing deep learning models to extract more significant and consistent features, thus augmenting classification accuracy and robustness. Furthermore, denoising enhances the generalizability of models across various field circumstances and imaging devices, such as smartphones and drones. Incorporating classical filters (e.g., Gaussian or median filters) into the pre-processing stage can substantially improve model performance, particularly in resource-constrained or noisy settings. These conventional filters smoothen the image globally thereby blurring of edges. Figure <xref rid="Fig6" ref-type="fig">6</xref> shows the output using VAD for sample image taken from dataset.<fig id="Fig6" position="float" orientation="portrait"><label>Fig. 6</label><caption><p>Original and enhanced image.</p></caption><graphic id="MO6" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig6_HTML.jpg"/></fig></p><p id="Par24">Also each colour channel is treated separately leading to colour artefacts. While applying filters for denoising the major factor to be considered is preservance of edges. In such context Vector Valued Anisotropic Diffusion method minimizes the noise and also preserve the edges in the image<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>. Each pixel is treated as an independent vector comprising of R, G, B vectors in combined form. The smoothing process is guided using the combined gradient magnitude. Diffusion denotes a method that enhances an image by reducing the disparity in intensity levels among adjacent pixels<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>. Isotropic diffusion uniformly smooths in all directions, resulting in edge blurring. This can be controlled by applying modified approach where the diffusion is varied along different directions. The process is thus known as Anisotropic diffusion<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>.</p><p id="Par25">The operation of this filter is governed by the following key components:<list list-type="bullet"><list-item><p id="Par26"><bold>Diffusion tensor</bold>: This tensor represented through a matrix provides the information of rate and direction of diffusion at a particular point. Expressed using the anisotropic diffusion PDE: <disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e791">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \frac{\partial I}{\partial t} = \nabla \cdot \left( \textbf{D}(I) \nabla I \right) \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ1.gif"/></alternatives></disp-formula><inline-formula id="IEq1"><alternatives><tex-math id="d33e797">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{D}(I)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq1.gif"/></alternatives></inline-formula> represents diffusion tensor a symmetric square matrix, <inline-formula id="IEq2"><alternatives><tex-math id="d33e803">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\frac{\partial I}{\partial t}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq2.gif"/></alternatives></inline-formula> represents derivative of Image intensity value, <inline-formula id="IEq3"><alternatives><tex-math id="d33e809">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\nabla I$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq3.gif"/></alternatives></inline-formula> represents intensity gradient</p></list-item></list><list list-type="bullet"><list-item><p id="Par27"><bold>Structure tensor</bold>: This tensor stores the local gradient information across all channels on the similar instant of time. The information in this vector can be utilised to extract information of edge strength and edge direction. <disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="d33e821">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \textbf{S}_t = \sigma * \left( \sum _{c=1}^{f} \begin{bmatrix} I_{c,x}^2 &amp; I_{c,x} I_{c,y} \\ I_{c,x} I_{c,y} &amp; I_{c,y}^2 \end{bmatrix} \right) \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ2.gif"/></alternatives></disp-formula> Where: <inline-formula id="IEq4"><alternatives><tex-math id="d33e828">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{S}_t$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq4.gif"/></alternatives></inline-formula> represents structure tensor, <inline-formula id="IEq5"><alternatives><tex-math id="d33e834">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq5.gif"/></alternatives></inline-formula>represents Gaussian convolution kernel, <inline-formula id="IEq6"><alternatives><tex-math id="d33e840">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_{c,x}, I_{c,y}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq6.gif"/></alternatives></inline-formula>: Spatial gradients of channel <inline-formula id="IEq7"><alternatives><tex-math id="d33e846">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq7.gif"/></alternatives></inline-formula>. In case of R, G,B image the number of channels <inline-formula id="IEq8"><alternatives><tex-math id="d33e853">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq7.gif"/></alternatives></inline-formula>=3.</p></list-item></list>The amount of diffusion is varied across the pixels over the image and the amount is varied as soon as an edge is encountered. This process reduces the diffusion effect on the sharp edges thereby preserving the edge information<sup><xref ref-type="bibr" rid="CR35">35</xref>,<xref ref-type="bibr" rid="CR36">36</xref></sup>. The gradient for each individual channel is calculated through Eq. (<xref rid="Equ1" ref-type="disp-formula">1</xref>). The resultant image can be calculated by Eq. (<xref rid="Equ3" ref-type="disp-formula">3</xref>) by combining R G B channels.<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="d33e873">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} I_c(x, y) \leftarrow I_c(x, y) + t \cdot \nabla \cdot \left( \textbf{D}(I) \nabla I_c \right) \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ3.gif"/></alternatives></disp-formula>where <inline-formula id="IEq9"><alternatives><tex-math id="d33e881">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I_c(x, y)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq9.gif"/></alternatives></inline-formula> represents intensity value of individual channel.</p><p id="Par28">
<fig position="anchor" id="Figa" orientation="portrait"><label>Algorithm 1</label><caption><p>Vector valued anisotropic diffusion vad.filter().</p></caption><graphic position="anchor" id="MO7" orientation="portrait" xlink:href="41598_2025_16923_Figa_HTML.jpg"/></fig>
</p></sec><sec id="Sec12"><title>Morphological- filter module</title><p id="Par29">Top-hat and bottom-hat filters(THBH) are morphological methods employed to enhance features in an image by highlighting minor aspects and details such as bright spots, dark spots, and textures. For image quality enhancement considerations, morphological filter fromed by amalgamation of two filters using elementary morphological operations i.e. Top hat filter and Bottom hat filter is a suitable option. The formation of morphological filter module is elaborated in Fig. <xref rid="Fig8" ref-type="fig">8</xref>. The top hatted version is computed by subtracting the resultant of opening operation between original image and kernel from the original image. Figure <xref rid="Fig7" ref-type="fig">7</xref> shows the output using THBH for sample image taken from dataset.<fig id="Fig7" position="float" orientation="portrait"><label>Fig. 7</label><caption><p>Original vs morphological filtered image.</p></caption><graphic id="MO8" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig7_HTML.jpg"/></fig></p><p id="Par30">This solves the problems relating to background illumination and perform enhancement of the bright objects with respect to a dark background<sup><xref ref-type="bibr" rid="CR37">37</xref></sup>. The bottom-hatted variant is a dual filter of top-hat filtering. The filtered picture is derived by subtracting the original image from the result of applying a closure operation on the original image using a kernel. This will enhance the picture contrast for effective lesion segmentation. Moreover, the sharp noise peaks are removed. The structuring element(K) plays an important role in the operation of this filter<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>. For top hatted image the features lesser than K are removed from the image and the features greater than k are enhanced. Whereas in bottom hat version the smaller feature elements are preserved. The combined resultant image is thus enhanced<sup><xref ref-type="bibr" rid="CR39">39</xref></sup>. The erosion process entails the removal of specific pixels based on the attributes of the kernel. <italic toggle="yes">K</italic>, and is implemented as given in Eq.&#160;(<xref rid="Equ4" ref-type="disp-formula">4</xref>).<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="d33e935">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} I(m,n) \ominus K = \{ Z \mid K_Z \subseteq I(r,c) \} \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ4.gif"/></alternatives></disp-formula>Where <italic toggle="yes">I</italic>(<italic toggle="yes">r</italic>,&#160;<italic toggle="yes">c</italic>) is the original input image and <italic toggle="yes">K</italic> is the kernel or structuring element.</p><p id="Par31">Through the process of dilation, more pixels are added to the object boundaries depending upon the characteristics of the kernel <italic toggle="yes">K</italic>, and is implemented as given in Eq.&#160;(<xref rid="Equ5" ref-type="disp-formula">5</xref>).<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="d33e962">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} I(r,c) \oplus K = \{ Z \mid (K_Z^\wedge \cap I(m,n)) \subseteq I(r,c) \} \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ5.gif"/></alternatives></disp-formula>An operation termed <bold>Opening</bold> is constituted by the sequential application of erosion and dilation, utilizing a same kernel element. The outcome is the refinement of contours and the elimination of narrow peaks. It is implemented as:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="d33e972">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} I(r,c) \circ K = (I(r,c) \ominus K) \oplus K \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ6.gif"/></alternatives></disp-formula>A composite operation termed <bold>Closing</bold> is constituted by a sequence of dilation succeeded by erosion, employing an identical kernel element. The outcome is the refinement of contours and the eradication of minor depressions. It is implemented as:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="d33e983">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} I(r,c) \bullet K = (I(r,c) \oplus K) \ominus K \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ7.gif"/></alternatives></disp-formula><fig id="Fig8" position="float" orientation="portrait"><label>Fig. 8</label><caption><p>Morphological filter module.</p></caption><graphic id="MO9" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig8_HTML.jpg"/></fig></p><p id="Par32">
<fig position="anchor" id="Figb" orientation="portrait"><label>Algorithm 2</label><caption><p>Top-hat and bottom-hat filtering with disk-shaped structuring rlement thbh_filter().</p></caption><graphic position="anchor" id="MO10" orientation="portrait" xlink:href="41598_2025_16923_Figb_HTML.jpg"/></fig>
</p></sec><sec id="Sec13"><title>Selection of an efficient image enhancement filter</title><p id="Par33">In the previous section, we have discussed the two categories of filters in order to remove noise from the images as well as preserving or enhancing the image quality. The subjective evaluation of image may not guarantee the effectiveness of the proposed image enhancement filter. Therefore, an objective evaluation is required to be conducted so as validate the performance of the Image enhancement filter<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. The objective evaluation has been conducted by evaluating the standard image quality metrics listed in table <xref rid="Tab4" ref-type="table">4</xref>.<table-wrap id="Tab4" position="float" orientation="portrait"><label>Table 4</label><caption><p>Comparison of image quality metrics with their mathematical formulations and interpretations.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Name of Metric</th><th align="left" colspan="1" rowspan="1">Mathematical Equation</th><th align="left" colspan="1" rowspan="1">Significance</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1"><bold>PSNR</bold> (Peak Signal-to-Noise Ratio)</td><td align="left" colspan="1" rowspan="1"><inline-formula id="IEq10"><alternatives><tex-math id="d33e1047">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {PSNR (dB)} = 10 \log _{10} \left( \frac{R_I^2}{\text {MSE}} \right)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq10.gif"/></alternatives></inline-formula>
<italic toggle="yes">Where MSE is the mean squared error.</italic></td><td align="left" colspan="1" rowspan="1">Higher value indicates better image strength over noise, implying better enhancement.</td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>SSIM</bold> (Structural Similarity Index)</td><td align="left" colspan="1" rowspan="1"><inline-formula id="IEq11"><alternatives><tex-math id="d33e1065">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {SSIM}(x, y) = \frac{(2\mu _x \mu _y + C_1)(2\sigma _{xy} + C_2)}{(\mu _x^2 + \mu _y^2 + C_1)(\sigma _x^2 + \sigma _y^2 + C_2)}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq11.gif"/></alternatives></inline-formula>
<italic toggle="yes">Where: </italic><inline-formula id="IEq12"><alternatives><tex-math id="d33e1073">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu _x, \mu _y$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq12.gif"/></alternatives></inline-formula> are means; <inline-formula id="IEq13"><alternatives><tex-math id="d33e1079">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma _x^2, \sigma _y^2$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq13.gif"/></alternatives></inline-formula> are variances; <inline-formula id="IEq14"><alternatives><tex-math id="d33e1085">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma _{xy}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq14.gif"/></alternatives></inline-formula> is covariance; constants: <inline-formula id="IEq15"><alternatives><tex-math id="d33e1091">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C_1 = (K_1L)^2, C_2 = (K_2L)^2$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq15.gif"/></alternatives></inline-formula>.</td><td align="left" colspan="1" rowspan="1">Measures luminance, contrast, and structure similarity. Higher SSIM indicates better perceptual image quality.</td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>NIQE</bold> (Natural Image Quality Evaluator)</td><td align="left" colspan="1" rowspan="1"><inline-formula id="IEq16"><alternatives><tex-math id="d33e1107">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {NIQE} = \sqrt{(\mu _d - \mu _p)^T \left( \frac{\Sigma _d + \Sigma _p}{2} \right) ^{-1} (\mu _d - \mu _p)}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq16.gif"/></alternatives></inline-formula>
<italic toggle="yes">Where: </italic><inline-formula id="IEq17"><alternatives><tex-math id="d33e1115">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu _d, \Sigma _d$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq17.gif"/></alternatives></inline-formula> and <inline-formula id="IEq18"><alternatives><tex-math id="d33e1121">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\mu _p, \Sigma _p$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq18.gif"/></alternatives></inline-formula> are the means and covariances of distorted and pristine image features respectively.</td><td align="left" colspan="1" rowspan="1">No-reference metric. Lower NIQE scores imply higher perceptual image quality.</td></tr></tbody></table></table-wrap></p><p id="Par34">The choice of an effective image enhancement filter is vital for enhancing visual quality and maintaining key image attributes. To objectively measure and compare the efficacy of enhancement techniques, three widely recognized image quality evaluation metrics are utilized: Peak Signal-to-Noise Ratio (PSNR), Structural Similarity Index Measure (SSIM), and Natural Image Quality Evaluator (NIQE). PSNR measures the ratio of the highest possible signal power to the noise power impacting the image, with elevated values signifying superior noise reduction. SSIM evaluates perceptual similarity of pictures by analyzing brightness, contrast, and structure, so effectively assessing structural fidelity<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. NIQE, a no-reference metric, evaluates image quality by analyzing statistical deviations from natural scene statistics, with lower values indicating superior perceptual quality<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Through the concurrent analysis of these measures, filters may be evaluated with more reliability, facilitating the selection of the enhancing technique that minimizes distortion while maximizing perceptual clarity and naturalness.<table-wrap id="Tab5" position="float" orientation="portrait"><label>Table 5</label><caption><p>Comparison of image quality metrics for THBH Filter and VAD method.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Disease Image Category</th><th align="left" colspan="3" rowspan="1">Top Hat Bottom Hat Filter</th><th align="left" colspan="3" rowspan="1">Vector-Valued Anisotropic Diffusion</th></tr><tr><th align="left" colspan="1" rowspan="1"/><th align="left" colspan="1" rowspan="1">PSNR</th><th align="left" colspan="1" rowspan="1">SSIM</th><th align="left" colspan="1" rowspan="1">NIQE</th><th align="left" colspan="1" rowspan="1">PSNR</th><th align="left" colspan="1" rowspan="1">SSIM</th><th align="left" colspan="1" rowspan="1">NIQE</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Anthracnose00005</td><td align="left" colspan="1" rowspan="1">28.2277</td><td align="left" colspan="1" rowspan="1">0.9588</td><td align="left" colspan="1" rowspan="1">8.2514</td><td align="left" colspan="1" rowspan="1">29.7000</td><td align="left" colspan="1" rowspan="1">0.9589</td><td align="left" colspan="1" rowspan="1">5.0717</td></tr><tr><td align="left" colspan="1" rowspan="1">bacterial_blight00003</td><td align="left" colspan="1" rowspan="1">21.8589</td><td align="left" colspan="1" rowspan="1">0.7443</td><td align="left" colspan="1" rowspan="1">9.3133</td><td align="left" colspan="1" rowspan="1">22.5717</td><td align="left" colspan="1" rowspan="1">0.7776</td><td align="left" colspan="1" rowspan="1">6.6862</td></tr><tr><td align="left" colspan="1" rowspan="1">Citrus_Canker00009</td><td align="left" colspan="1" rowspan="1">25.6710</td><td align="left" colspan="1" rowspan="1">0.9186</td><td align="left" colspan="1" rowspan="1">7.5630</td><td align="left" colspan="1" rowspan="1">24.2778</td><td align="left" colspan="1" rowspan="1">0.9047</td><td align="left" colspan="1" rowspan="1">5.5003</td></tr><tr><td align="left" colspan="1" rowspan="1">curl_virus00007</td><td align="left" colspan="1" rowspan="1">32.4988</td><td align="left" colspan="1" rowspan="1">0.9516</td><td align="left" colspan="1" rowspan="1">7.4150</td><td align="left" colspan="1" rowspan="1">31.8224</td><td align="left" colspan="1" rowspan="1">0.9444</td><td align="left" colspan="1" rowspan="1">4.4148</td></tr><tr><td align="left" colspan="1" rowspan="1">Deficiency_leaf00009</td><td align="left" colspan="1" rowspan="1">22.9131</td><td align="left" colspan="1" rowspan="1">0.8605</td><td align="left" colspan="1" rowspan="1">6.3553</td><td align="left" colspan="1" rowspan="1">22.8427</td><td align="left" colspan="1" rowspan="1">0.8660</td><td align="left" colspan="1" rowspan="1">3.8767</td></tr><tr><td align="left" colspan="1" rowspan="1">Dry_leaf00007</td><td align="left" colspan="1" rowspan="1">23.3556</td><td align="left" colspan="1" rowspan="1">0.8858</td><td align="left" colspan="1" rowspan="1">7.6135</td><td align="left" colspan="1" rowspan="1">22.4526</td><td align="left" colspan="1" rowspan="1">0.8739</td><td align="left" colspan="1" rowspan="1">6.0716</td></tr><tr><td align="left" colspan="1" rowspan="1">sooty_mould00002</td><td align="left" colspan="1" rowspan="1">19.8858</td><td align="left" colspan="1" rowspan="1">0.8211</td><td align="left" colspan="1" rowspan="1">8.0138</td><td align="left" colspan="1" rowspan="1">22.3137</td><td align="left" colspan="1" rowspan="1">0.8542</td><td align="left" colspan="1" rowspan="1">4.4302</td></tr><tr><td align="left" colspan="1" rowspan="1">spider_mites00004</td><td align="left" colspan="1" rowspan="1">24.5846</td><td align="left" colspan="1" rowspan="1">0.7980</td><td align="left" colspan="1" rowspan="1">6.7255</td><td align="left" colspan="1" rowspan="1">24.8009</td><td align="left" colspan="1" rowspan="1">0.8241</td><td align="left" colspan="1" rowspan="1">4.6680</td></tr><tr><td align="left" colspan="1" rowspan="1">Healthy_leaf00006</td><td align="left" colspan="1" rowspan="1">29.3078</td><td align="left" colspan="1" rowspan="1">0.9491</td><td align="left" colspan="1" rowspan="1">7.3364</td><td align="left" colspan="1" rowspan="1">29.2762</td><td align="left" colspan="1" rowspan="1">0.9526</td><td align="left" colspan="1" rowspan="1">5.4856</td></tr></tbody></table></table-wrap></p><p id="Par35">To obtain the enhanced images, we have done implementation through MATLAB. The rigorous testing of variants of VAD and THBH filters in order to obtain the optimal values of quality metrics including PSNR, SSIM and NIQE<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. For implementation of VAD the testing was done considering varying number of iterations and the optimal values of metrics were obtained with iterations=3. At each iteration, directional gradients (North, South, East, West) were computed for each color channel, followed by gradient magnitude calculation and application of the diffusion process to preserve edges while smoothing noise.For conductivity function, the value is calculated using exponential function. Similarly, the testing of THBH filter was done considering different shapes of structuring elements like Disk, Diamond, cube and rectangle with different sizes of radii. The optimal performance was obtained through the disk shaped kernel with radii=3. Table <xref rid="Tab5" ref-type="table">5</xref> shows the comparitive analysis metric wise for VAD and THBH filter. The implemntation results demosterated in this table shows the values obtained for sample image taken from each image class.</p><p id="Par36">
<fig position="anchor" id="Figc" orientation="portrait"><label>Algorithm 3</label><caption><p>Enhanced image selection based on image quality metrics.</p></caption><graphic position="anchor" id="MO11" orientation="portrait" xlink:href="41598_2025_16923_Figc_HTML.jpg"/></fig>
</p><p id="Par37">From the value of metrics obtained through implementation, the results obtained for VAD filter satisfies the selection criteria designed in algorithm 3. The testing has been done on sample images belonging to each class of disease and healthy images. In numerous disease categories, the VAD filter produces either elevated or equivalent PSNR and SSIM, while consistently attaining reduced NIQE values, indicating enhanced perceptual quality. For example, in the instance of Anthracnose00005, VAD attained a PSNR of 29.7 and NIQE of 5.0717, in contrast to 28.2277 and 8.2514 for the Top Hat Bottom Hat technique, signifying superior signal fidelity and naturalness. Comparable trends are observed in bacterial_blight00003, curl_virus00007, and sooty_mould00002, wherein VAD markedly enhances NIQE scores while preserving competitive PSNR and SSIM. In several instances, such as Citrus_Canker00009 and Dry_leaf00007, the Top Hat Bottom Hat technique demonstrated marginally superior PSNR or SSIM; nonetheless, it remained inferior in NIQE. This suggests that although both techniques can improve image features, VAD consistently provides a superior equilibrium of objective quality and perceptual realism, rendering it a more reliable option for enhancing images of plant diseases.</p></sec></sec><sec id="Sec14"><title>Design of fuzzy based ensemble model:NL-FuRBE</title><p id="Par38">Fusion in ensemble learning is the technique for aggregating predictions from several base learners into a single output. This stage is essential, as how well the outputs of the base models are merged determines the usefulness of an ensemble in addition to their diversity and correctness. While fusing the predictions of multiple base learners, the underlying problem is giving uniform priority to all the base learners, which results in lower classification accuracy. This section presents a brief overview of the utilized base learners and their related technical aspects, which are later utilized to develop the proposed model based on fuzzy-based ranking. Each base learner generates the value of a confidence score indicating the probability of disease presence. Multiple customized base learners ensure robustness and increased accuracy. The fuzzy-ensemble method subsequently integrates these confidence scores through fuzzy set theory, considering the uncertainty and variability inherent in the scores<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. The resultant fuzzy ensemble calculated score serves as the deciding criterion for disease diagnosis. This method seeks to enhance the accuracy and reliability of citrus disease detection by leveraging the advantages of various classifiers and integrating uncertainty into the final decision-making process. The weights of the early layers of three base learners are fixed, while the subsequent layers are optimized on the Lemon leaf disease dataset utilized. <list list-type="order"><list-item><p id="Par39">VGG19</p><p id="Par40">VGG19 A high-in-depth CNN model, introduced by Simonyan and Zisserman, consisting of 19 weight layers. Out of 19 layers- 16 are the convolutional layers while three are the fully connected layers. Each convolutional block is followed by a max-pooling layer, and the entire network utilizes ReLU activation functions to introduce nonlinearity. The uniform architecture, combined with its depth, enables VGG-19 to capture rich hierarchical features, making it highly suitable for tasks such as object recognition, feature extraction, and transfer learning.</p></list-item><list-item><p id="Par41">AlexNet</p><p id="Par42">AlexNet is a renowned neural network architecture primarily developed by Alex Krizhevsky. It was renowned for competing in the ImageNet Large Scale Visual Recognition Challenge in 2012 and securing the championship. The network attains a 15.3% error rate, which is remarkable since it is 10.8 percentage points lower than the second-place competitor. The principle of AlexNet is that a model&#8217;s depth is crucial for its performance. Consequently, it was developed in CUDA to leverage GPU capabilities for enhanced training speed. Although AlexNet is not the inaugural network trained on a GPU, it has had a significant impact in this domain. It stimulates greater research than training on GPUs and is regarded as one of the most significant articles in the domain of image recognition.</p></list-item><list-item><p id="Par43">Xception</p><p id="Par44">The Xception architecture, designed by Chollet et al., is derived from the Inception v3 design, maintaining an equivalent number of model parameters while utilizing them more efficiently. They demonstrated that pointwise convolutions and depthwise separable convolutions represent the two extremes of a discrete spectrum, with inception modules positioned centrally. Consequently, they substituted the inception modules with depthwise separable convolutions, resulting in enhanced classification performance without increasing computational costs.</p></list-item></list>To leverage the strengths of multiple base models and enhance the robustness and accuracy of our predictions, we introduce a novel NL-FuRBE (nonlinear Fuzzy Rank-Based Ensemble). A Fuzzy Rank-Based Ensemble is an advanced fusion methodology that integrates the advantages of many classifiers or models by allocating fuzzy ranks according to their efficacy or confidence, instead of depending exclusively on definitive voting or averaging. It is particularly advantageous in situations such as disease detection, where model uncertainty and overlapping class borders frequently occur. This approach considers the prediction scores of each base classifier for every individual test case independently.</p><p id="Par45">
<fig position="anchor" id="Figd" orientation="portrait"><label>Algorithm 4</label><caption><p>Multi-class citrus leaf disease detection.</p></caption><graphic position="anchor" id="MO12" orientation="portrait" xlink:href="41598_2025_16923_Figd_HTML.jpg"/></fig>
</p><sec id="Sec15"><title>Base model training</title><p id="Par46">The diverse base learners employed are denoted as <inline-formula id="IEq19"><alternatives><tex-math id="d33e1376">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BM_1, BM_2, \ldots , BM_N$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq19.gif"/></alternatives></inline-formula>. In our proposed methodology, we have employed VGG-19, AlexNet, and Xception as the base learners. Each base model <inline-formula id="IEq20"><alternatives><tex-math id="d33e1382">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BM_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq20.gif"/></alternatives></inline-formula> is trained independently on the training dataset for citrus leaf diseases (<inline-formula id="IEq21"><alternatives><tex-math id="d33e1388">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$CD_{train}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq21.gif"/></alternatives></inline-formula>) to learn the underlying patterns and relationships within the data. Upon completion of the training phase, each base model <inline-formula id="IEq22"><alternatives><tex-math id="d33e1394">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BM_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq20.gif"/></alternatives></inline-formula> generates prediction scores for a given input sample <inline-formula id="IEq23"><alternatives><tex-math id="d33e1400">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_x$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq23.gif"/></alternatives></inline-formula> belonging to the test dataset (<inline-formula id="IEq24"><alternatives><tex-math id="d33e1407">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$CD_{test}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq24.gif"/></alternatives></inline-formula>). the complete steps are shown in Algorithm&#160;4.</p><p id="Par47">Let the prediction score of each base model <inline-formula id="IEq25"><alternatives><tex-math id="d33e1415">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BM_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq20.gif"/></alternatives></inline-formula> for individual disease or healthy class <italic toggle="yes">c</italic> be denoted as <inline-formula id="IEq26"><alternatives><tex-math id="d33e1424">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$P_{i,c}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq26.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq27"><alternatives><tex-math id="d33e1430">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$c \in \{1, 2, \ldots , C\}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq27.gif"/></alternatives></inline-formula> and <italic toggle="yes">C</italic> is the total number of classes. <italic toggle="yes">I</italic> represents the number of base models. In our dataset, there are a total of nine different classes. Therefore, the total prediction sets will include values as: <inline-formula id="IEq28"><alternatives><tex-math id="d33e1443">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{bmatrix} BM_1\\ BM_2 \\ BM_3 \end{bmatrix} \rightarrow \begin{bmatrix} P_{1,1}, P_{1,2}, P_{1,3}, \ldots , P_{1,9}\\ P_{2,1}, P_{2,2}, P_{2,3}, \ldots , P_{2,9}\\ P_{3,1}, P_{3,2}, P_{3,3}, \ldots , P_{3,9} \end{bmatrix}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq28.gif"/></alternatives></inline-formula></p></sec><sec id="Sec16"><title>Fuzzy ranking of predictions</title><p id="Par48">For each test sample <inline-formula id="IEq29"><alternatives><tex-math id="d33e1452">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_x$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq23.gif"/></alternatives></inline-formula>, the prediction scores from each base model <inline-formula id="IEq30"><alternatives><tex-math id="d33e1458">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BM_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq20.gif"/></alternatives></inline-formula> are subjected to a fuzzy ranking process through three different nonlinear functions. Let <inline-formula id="IEq31"><alternatives><tex-math id="d33e1464">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{(1)}_{i,c}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq31.gif"/></alternatives></inline-formula>, <inline-formula id="IEq32"><alternatives><tex-math id="d33e1470">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{(2)}_{i,c}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq32.gif"/></alternatives></inline-formula>, and <inline-formula id="IEq33"><alternatives><tex-math id="d33e1476">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{(3)}_{i,c}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq33.gif"/></alternatives></inline-formula> denote the fuzzy ranks generated by the nonlinear functions for each base model <inline-formula id="IEq34"><alternatives><tex-math id="d33e1483">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BM_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq20.gif"/></alternatives></inline-formula> corresponding to all classes in the dataset. The equations for the nonlinear functions are given as:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="d33e1489">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} R^{(1)}_{i,c}&amp;= 1 - \tanh \left( \frac{(P_c - 1)^2}{2}\right) \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ8.gif"/></alternatives></disp-formula><disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="d33e1495">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} R^{(2)}_{i,c}&amp;= 1 - \exp \left( -\frac{(P_c - 1)^2}{2}\right) \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ9.gif"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="d33e1501">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} R^{(3)}_{i,c}&amp;= \frac{1}{1 + e^{(-P_c)}} \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ10.gif"/></alternatives></disp-formula>Equation (<xref rid="Equ8" ref-type="disp-formula">8</xref>) representa a hyperbolic tangent function which is a symmetric function centered around 1. The rank value is calculated based on prediction value- lower the prediction score lower is the rank allocated.</p><p id="Par49">Equation (<xref rid="Equ9" ref-type="disp-formula">9</xref>) represents an exponential function which assigns rank based on the prediction scores of each base classifier. As soon as the predicted score approach towards 1, the minimum rank value is assigned to a particular classifier for a particular class label.</p><p id="Par50">Equation (<xref rid="Equ10" ref-type="disp-formula">10</xref>) represents a sigmoid function. The sigmoid is suitable for smoother transitions from higher to lower weights. This will also generates the ranks for each individual classifier based on prediction score. These ranks combined together forms the base for aggregate rank generation.</p></sec><sec id="Sec17"><title>Ensemble aggregation using fuzzy ranks</title><p id="Par51">To obtain the final ensemble prediction for a test sample <inline-formula id="IEq35"><alternatives><tex-math id="d33e1525">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_x$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq23.gif"/></alternatives></inline-formula>, the fuzzy ranks generated by all base models <inline-formula id="IEq36"><alternatives><tex-math id="d33e1531">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BM_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq20.gif"/></alternatives></inline-formula> are aggregated. For each class <italic toggle="yes">c</italic>, we calculate an aggregated fuzzy rank <inline-formula id="IEq37"><alternatives><tex-math id="d33e1540">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$R^{\text {fused}}_c$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq37.gif"/></alternatives></inline-formula> by combining the ranks from all <italic toggle="yes">N</italic> base models.</p><p id="Par52">The fused rank score is given by:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="d33e1551">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} R^{\text {fused}}_c = \sum _{i=1}^{N} \left( R^{(1)}_{i,c} \times R^{(2)}_{i,c} \times R^{(3)}_{i,c} \right) \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ11.gif"/></alternatives></disp-formula>The final predicted class <inline-formula id="IEq38"><alternatives><tex-math id="d33e1558">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{y}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq38.gif"/></alternatives></inline-formula> for the test sample <inline-formula id="IEq39"><alternatives><tex-math id="d33e1564">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S_x$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq23.gif"/></alternatives></inline-formula> is the class with the smallest aggregated fuzzy rank:<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="d33e1570">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \hat{y} = \arg \min _{c \in \{1,2,\ldots ,C\}} \left( R^{\text {fused}}_c \right) \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ12.gif"/></alternatives></disp-formula>The class <inline-formula id="IEq40"><alternatives><tex-math id="d33e1577">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{y}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq38.gif"/></alternatives></inline-formula> with the maximum aggregated fuzzy rank is considered the final prediction of the proposed methodology. This decision rule prioritizes the class that receives consistently high confidence and low uncertainty scores across the ensemble of base models thereby improving the classification accuracy.</p></sec></sec></sec><sec id="Sec18"><title>Experimental results</title><sec id="Sec19"><title>Experimental set-up</title><p id="Par53">The experiments conducted in this study utilize the Python framework with Google Colab as the integrated development environment. The implementation system is configured with 8 GB of RAM and utilizes with hardware accelerator v2-8 TPU. The study aims to classify citrus disease images available through publically available dataset as Anthracnose, Bacterial Blight, Canker, curl virus, Deficiency leaf, Dry leaf, Sooty mould, Spider mites, Healthy employing consistent hyper parameters for the training of three utilized transfer learning models: AlexNet, VGG19 and Xception. Subsequent to input pre-processing through various filters, the images of different categories of diseases are ultimately scaled to 128 x 128 and subsequently utilized in transfer learning models for feature extraction. To split the data into train, test and validation set 5-fold cross validation is used. To evaluate the effectiveness of our proposed model on the dataset, we implemented the model using hyperparameters, as shown in Table <xref rid="Tab6" ref-type="table">6</xref>. Also, the same settings have been used to evaluate the performance of individual classifiers.<table-wrap id="Tab6" position="float" orientation="portrait"><label>Table 6</label><caption><p>Hyperparameter settings for training the models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Hyperparameters</th><th align="left" colspan="1" rowspan="1">Values</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Image Width</td><td align="left" colspan="1" rowspan="1">128</td></tr><tr><td align="left" colspan="1" rowspan="1">Image Height</td><td align="left" colspan="1" rowspan="1">128</td></tr><tr><td align="left" colspan="1" rowspan="1">Optimizers</td><td align="left" colspan="1" rowspan="1">Adam</td></tr><tr><td align="left" colspan="1" rowspan="1">Learning rate</td><td align="left" colspan="1" rowspan="1">0.001</td></tr><tr><td align="left" colspan="1" rowspan="1">Batch size</td><td align="left" colspan="1" rowspan="1">32</td></tr><tr><td align="left" colspan="1" rowspan="1">Epochs</td><td align="left" colspan="1" rowspan="1">25</td></tr><tr><td align="left" colspan="1" rowspan="1">Loss function</td><td align="left" colspan="1" rowspan="1">categorical crossentropy</td></tr><tr><td align="left" colspan="1" rowspan="1">Activation functions</td><td align="left" colspan="1" rowspan="1">Relu, softmax</td></tr><tr><td align="left" colspan="1" rowspan="1">Dropout</td><td align="left" colspan="1" rowspan="1">0.5</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec20"><title>Performance indicators</title><p id="Par54">To access the performance of proposed NL-FuRBE method, the performance metrics considered includes- Accuracy, F1-Score, Recall and Precision. Through these metrics we are going to present the tabular results and access the performance of proposed methodology w.r.t existing methods. These metrics access the model&#8217;s performance quantitatively for the given dataset.<disp-formula id="Equ13"><alternatives><tex-math id="d33e1656">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {Accuracy}= &amp; \frac{\sum (T_{pos} , T_{neg})}{\sum (T_{pos} , T_{neg} , F_{pos} , F_{neg})} \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ13.gif"/></alternatives></disp-formula><disp-formula id="Equ14"><alternatives><tex-math id="d33e1661">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {Precision (Specificity)}= &amp; \frac{\sum T_{pos}}{\sum (T_{pos} , F_{pos})} \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ14.gif"/></alternatives></disp-formula><disp-formula id="Equ15"><alternatives><tex-math id="d33e1666">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {Recall (Sensitivity)}= &amp; \frac{\sum T_{pos}}{\sum (T_{pos} , F_{neg})} \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ15.gif"/></alternatives></disp-formula><disp-formula id="Equ16"><alternatives><tex-math id="d33e1671">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {F1-Score}= &amp; 2 \times \frac{ (\text {Precision} \times \text {Recall})}{\text {Precision} + \text {Recall}} \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_16923_Article_Equ16.gif"/></alternatives></disp-formula>where <inline-formula id="IEq41"><alternatives><tex-math id="d33e1677">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T_{pos} , T_{neg} , F_{pos} , F_{neg}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq41.gif"/></alternatives></inline-formula> stands for True Positives, True Negatives, False Positives, False Negatives respectively.</p></sec></sec><sec id="Sec21"><title>Results and discussions</title><p id="Par55">
<fig id="Fig9" position="float" orientation="portrait"><label>Fig. 9</label><caption><p>Accuracy and loss for Alexnet with conventional ensemble approach.</p></caption><graphic id="MO13" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig9_HTML.jpg"/></fig>
<fig id="Fig10" position="float" orientation="portrait"><label>Fig. 10</label><caption><p>Accuracy and loss for VGG-19 with conventional ensemble approach.</p></caption><graphic id="MO14" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig10_HTML.jpg"/></fig>
<fig id="Fig11" position="float" orientation="portrait"><label>Fig. 11</label><caption><p>Accuracy and loss for Xception with conventional ensemble approach.</p></caption><graphic id="MO15" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig11_HTML.jpg"/></fig>
<fig id="Fig12" position="float" orientation="portrait"><label>Fig. 12</label><caption><p>Confusion matrix for Alexnet and VGG-19 with conventional ensemble approach.</p></caption><graphic id="MO16" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig12_HTML.jpg"/></fig>
<fig id="Fig13" position="float" orientation="portrait"><label>Fig. 13</label><caption><p>Confusion matrix for Xception and ensemble with conventional ensemble approach.</p></caption><graphic id="MO17" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig13_HTML.jpg"/></fig>
</p><p id="Par56">This section presents the results obtained after implementation, considering the mentioned experimental setup. We have tested the two methodologies&#8211;standard fuzzy-rank-based ensemble and the proposed model explained in the previous section. A comparative analysis considering both methods is also presented.</p><sec id="Sec22"><title>Results using base learners with fuzzy ensemble model</title><p id="Par57">The standard fuzzy rank-based ensemble is trained using the available dataset and the results are shown and presented in this section. Initially 3 base learners, AlexNet, VGG19 and Xception models were trained and a total of 20 epochs were used. Figure <xref rid="Fig9" ref-type="fig">9</xref> shows the accuracy and loss value for individual AlexNet with the ensemble approach. Accuracy, F1 score, recall, and precision achieved by the AlexNet classifier with the fuzzy ensemble model are 79.5%, 82.4%, 81.4%, and 83.5%, respectively. Figure <xref rid="Fig10" ref-type="fig">10</xref> shows the accuracy and loss value for individual VGG-19 with the ensemble approach. Accuracy, F1 score, recall, and precision achieved by the AlexNet classifier with the fuzzy ensemble model are 85.7%, 90.0%, 88.8.4%, and 91.2%, respectively. Figure <xref rid="Fig11" ref-type="fig">11</xref> shows the accuracy and loss value for individual Xception with the ensemble approach. Accuracy, F1 score, recall, and precision achieved by the AlexNet classifier with the fuzzy ensemble model are 88.3%, 91.5%, 91.3%, and 91.7%, respectively. The confusion matrix illustrates the performance of a Multi-class classification model for different disease variants available in the dataset. Figure <xref rid="Fig12" ref-type="fig">12</xref>, Fig. <xref rid="Fig13" ref-type="fig">13</xref>, show the confusion matrix for the base learners and ensemble model with conventional approach.</p></sec><sec id="Sec23"><title>Results for the proposed model</title><p id="Par58">
<fig id="Fig14" position="float" orientation="portrait"><label>Fig. 14</label><caption><p>Accuracy and loss for Alexnet with proposed approach.</p></caption><graphic id="MO18" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig14_HTML.jpg"/></fig>
<fig id="Fig15" position="float" orientation="portrait"><label>Fig. 15</label><caption><p>Accuracy and loss for VGG-19 with proposed approach.</p></caption><graphic id="MO19" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig15_HTML.jpg"/></fig>
<fig id="Fig16" position="float" orientation="portrait"><label>Fig. 16</label><caption><p>Accuracy and loss for Xception with proposed approach.</p></caption><graphic id="MO20" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig16_HTML.jpg"/></fig>
<fig id="Fig17" position="float" orientation="portrait"><label>Fig. 17</label><caption><p>Confusion matrix for Alexnet and VGG-19 with proposed approach.</p></caption><graphic id="MO21" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig17_HTML.jpg"/></fig>
<fig id="Fig18" position="float" orientation="portrait"><label>Fig. 18</label><caption><p>Confusion matrix for Xception and ensemble with proposed approach.</p></caption><graphic id="MO22" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig18_HTML.jpg"/></fig>
</p><p id="Par59">The proposed fuzzy rank-based ensemble is trained using the enhanced dataset (refer to algorithms <xref rid="Sec11" ref-type="sec">1</xref> and <xref rid="Sec13" ref-type="sec">3</xref>) and the results are shown and presented in this section. Initially, 3 base learners, AlexNet, VGG19 and Xception models were trained, and a total of 20 epochs were used. The proposed approach outperforms the conventional approach, depicting the advantage of applying Image quality improvement before providing the dataset for the classifier training. The image quality was assessed using standard metrics and the results shown support the relevance of applying image quality enhancement to the original dataset. Table <xref rid="Tab8" ref-type="table">8</xref> shows the model performance for the lemon disease dataset corresponding to the fifth fold. The whole enhanced dataset obtained after applying enhancement is split into five parts so that a 5-fold cross-validation can be done. The first four folds of the dataset were used for training in the first fold. The last fold tested all the base learners and the suggested fuzzy-rank-based ensemble model. After 20 epochs, Fig. <xref rid="Fig14" ref-type="fig">14</xref> shows the accuracy and loss of the AlexNet classifier with the proposed methodology. Accuracy, F1 score, recall, and precision achieved by the AlexNet classifier with the fuzzy ensemble model are 85.2%, 86.3%, 85.8% and 86.9% respectively. Figure <xref rid="Fig15" ref-type="fig">15</xref> shows the accuracy curve, and loss curve of the VGG-19 classifier with the fuzzy ensemble model. Accuracy, F1 score, recall, and precision achieved by the VGG-19 classifier with the fuzzy ensemble model are 90.3%, 91.7%, 91.6%, and 91.7%, respectively. For the Xception classifier, Fig. <xref rid="Fig16" ref-type="fig">16</xref> shows the accuracy and loss curve with the fuzzy ensemble model. Accuracy, F1 score, recall, and precision achieved by the AlexNet classifier with the fuzzy ensemble model are 96.5%, 96.4%, 96.4%, and 96.5%, respectively.</p><p id="Par60">Table <xref rid="Tab7" ref-type="table">7</xref> compare the performance of four classifiers&#8211;AlexNet, VGG, Xception, and an ensemble model&#8211;under two approaches: fuzzy rank-based methodology and the proposed methodology. Four key evaluation metrics are reported: Accuracy, F1 Score, Recall, and Precision. Figure <xref rid="Fig17" ref-type="fig">17</xref>, Fig. <xref rid="Fig18" ref-type="fig">18</xref>, show the confusion matrix for the individual learners and ensemble model with proposed approach.</p><p id="Par61">The results evidently demonstrate that the Ensemble model outperforms the individual models across all evaluation metrics. AlexNet shows the lowest performance across all metrics, with an accuracy of 85.2% and F1 Score of 86.3%, suggesting its limitations in capturing the complexity of the disease patterns. VGG markedly surpassed AlexNet, attaining an accuracy of 90.3% and an F1 score of 91.7%, because to its deeper architecture that facilitates enhanced feature extraction. Xception somewhat surpassed VGG, achieving an accuracy of 92.2% and a recall of 92.3%, demonstrating robust sensitivity in accurately detecting positive instances. Table <xref rid="Tab7" ref-type="table">7</xref> presents metric wise comparison for conventional and proposed methodology considering individual base learners and ensembled version.<table-wrap id="Tab7" position="float" orientation="portrait"><label>Table 7</label><caption><p>Performance comparison between conventional and proposed methodologies.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" rowspan="2" colspan="1">Model</th><th align="left" colspan="4" rowspan="1">Conventional Fuzzy Rank based Methodology</th><th align="left" colspan="4" rowspan="1">Proposed Methodology</th></tr><tr><th align="left" colspan="1" rowspan="1">Accuracy</th><th align="left" colspan="1" rowspan="1">F1 Score</th><th align="left" colspan="1" rowspan="1">Recall</th><th align="left" colspan="1" rowspan="1">Precision</th><th align="left" colspan="1" rowspan="1">Accuracy</th><th align="left" colspan="1" rowspan="1">F1 Score</th><th align="left" colspan="1" rowspan="1">Recall</th><th align="left" colspan="1" rowspan="1">Precision</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">AlexNet</td><td align="left" colspan="1" rowspan="1">79.52</td><td align="left" colspan="1" rowspan="1">82.42</td><td align="left" colspan="1" rowspan="1">81.41</td><td align="left" colspan="1" rowspan="1">83.52</td><td align="left" colspan="1" rowspan="1">85.26</td><td align="left" colspan="1" rowspan="1">86.31</td><td align="left" colspan="1" rowspan="1">85.84</td><td align="left" colspan="1" rowspan="1">86.91</td></tr><tr><td align="left" colspan="1" rowspan="1">VGG</td><td align="left" colspan="1" rowspan="1">85.75</td><td align="left" colspan="1" rowspan="1">90.00</td><td align="left" colspan="1" rowspan="1">88.81</td><td align="left" colspan="1" rowspan="1">91.22</td><td align="left" colspan="1" rowspan="1">90.34</td><td align="left" colspan="1" rowspan="1">91.79</td><td align="left" colspan="1" rowspan="1">91.60</td><td align="left" colspan="1" rowspan="1">91.71</td></tr><tr><td align="left" colspan="1" rowspan="1">Xception</td><td align="left" colspan="1" rowspan="1">88.36</td><td align="left" colspan="1" rowspan="1">91.54</td><td align="left" colspan="1" rowspan="1">91.32</td><td align="left" colspan="1" rowspan="1">91.75</td><td align="left" colspan="1" rowspan="1">92.25</td><td align="left" colspan="1" rowspan="1">91.82</td><td align="left" colspan="1" rowspan="1">92.30</td><td align="left" colspan="1" rowspan="1">91.41</td></tr><tr><td align="left" colspan="1" rowspan="1">Ensemble</td><td align="left" colspan="1" rowspan="1">91.46</td><td align="left" colspan="1" rowspan="1">92.23</td><td align="left" colspan="1" rowspan="1">92.26</td><td align="left" colspan="1" rowspan="1">92.36</td><td align="left" colspan="1" rowspan="1">96.51</td><td align="left" colspan="1" rowspan="1">96.43</td><td align="left" colspan="1" rowspan="1">96.49</td><td align="left" colspan="1" rowspan="1">96.55</td></tr></tbody></table></table-wrap><table-wrap id="Tab8" position="float" orientation="portrait"><label>Table 8</label><caption><p>Class-wise performance metrics for different classes- Fold 5.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Fold no.</th><th align="left" colspan="1" rowspan="1">Model</th><th align="left" colspan="1" rowspan="1">Class</th><th align="left" colspan="1" rowspan="1">Precision</th><th align="left" colspan="1" rowspan="1">Recall</th><th align="left" colspan="1" rowspan="1">F1-score</th><th align="left" colspan="1" rowspan="1">Accuracy</th></tr></thead><tbody><tr><td align="left" rowspan="36" colspan="1">5</td><td align="left" rowspan="9" colspan="1">AlexNet</td><td align="left" colspan="1" rowspan="1">Spider Mites</td><td align="left" colspan="1" rowspan="1">0.86</td><td align="left" colspan="1" rowspan="1">0.68</td><td align="left" colspan="1" rowspan="1">0.79</td><td align="left" rowspan="9" colspan="1">85.26</td></tr><tr><td align="left" colspan="1" rowspan="1">Sooty Mould</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.81</td><td align="left" colspan="1" rowspan="1">0.83</td></tr><tr><td align="left" colspan="1" rowspan="1">Healthy Leaf</td><td align="left" colspan="1" rowspan="1">0.79</td><td align="left" colspan="1" rowspan="1">0.97</td><td align="left" colspan="1" rowspan="1">0.89</td></tr><tr><td align="left" colspan="1" rowspan="1">Curl Virus</td><td align="left" colspan="1" rowspan="1">0.93</td><td align="left" colspan="1" rowspan="1">0.86</td><td align="left" colspan="1" rowspan="1">0.84</td></tr><tr><td align="left" colspan="1" rowspan="1">Anthracnose</td><td align="left" colspan="1" rowspan="1">0.51</td><td align="left" colspan="1" rowspan="1">0.88</td><td align="left" colspan="1" rowspan="1">0.85</td></tr><tr><td align="left" colspan="1" rowspan="1">Citrus Canker</td><td align="left" colspan="1" rowspan="1">0.95</td><td align="left" colspan="1" rowspan="1">0.85</td><td align="left" colspan="1" rowspan="1">0.85</td></tr><tr><td align="left" colspan="1" rowspan="1">Bacterial Blight</td><td align="left" colspan="1" rowspan="1">0.91</td><td align="left" colspan="1" rowspan="1">0.84</td><td align="left" colspan="1" rowspan="1">0.88</td></tr><tr><td align="left" colspan="1" rowspan="1">Deficiency Leaf</td><td align="left" colspan="1" rowspan="1">0.86</td><td align="left" colspan="1" rowspan="1">0.83</td><td align="left" colspan="1" rowspan="1">0.84</td></tr><tr><td align="left" colspan="1" rowspan="1">Dry Leaf</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">1.00</td></tr><tr><td align="left" rowspan="9" colspan="1">VGG-19</td><td align="left" colspan="1" rowspan="1">Spider Mites</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.75</td><td align="left" colspan="1" rowspan="1">0.98</td><td align="left" rowspan="9" colspan="1">90.34</td></tr><tr><td align="left" colspan="1" rowspan="1">Sooty Mould</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.76</td><td align="left" colspan="1" rowspan="1">0.84</td></tr><tr><td align="left" colspan="1" rowspan="1">Healthy Leaf</td><td align="left" colspan="1" rowspan="1">0.70</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.84</td></tr><tr><td align="left" colspan="1" rowspan="1">Curl Virus</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.87</td><td align="left" colspan="1" rowspan="1">0.87</td></tr><tr><td align="left" colspan="1" rowspan="1">Anthracnose</td><td align="left" colspan="1" rowspan="1">0.65</td><td align="left" colspan="1" rowspan="1">0.89</td><td align="left" colspan="1" rowspan="1">0.78</td></tr><tr><td align="left" colspan="1" rowspan="1">Citrus Canker</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.97</td><td align="left" colspan="1" rowspan="1">0.99</td></tr><tr><td align="left" colspan="1" rowspan="1">Bacterial Blight</td><td align="left" colspan="1" rowspan="1">0.96</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.98</td></tr><tr><td align="left" colspan="1" rowspan="1">Deficiency Leaf</td><td align="left" colspan="1" rowspan="1">0.96</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.98</td></tr><tr><td align="left" colspan="1" rowspan="1">Dry Leaf</td><td align="left" colspan="1" rowspan="1">0.98</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">1.00</td></tr><tr><td align="left" rowspan="9" colspan="1">Xception</td><td align="left" colspan="1" rowspan="1">Spider Mites</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.76</td><td align="left" colspan="1" rowspan="1">0.86</td><td align="left" rowspan="9" colspan="1">92.25</td></tr><tr><td align="left" colspan="1" rowspan="1">Sooty Mould</td><td align="left" colspan="1" rowspan="1">0.80</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.82</td></tr><tr><td align="left" colspan="1" rowspan="1">Healthy Leaf</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.97</td><td align="left" colspan="1" rowspan="1">0.97</td></tr><tr><td align="left" colspan="1" rowspan="1">Curl Virus</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.85</td><td align="left" colspan="1" rowspan="1">0.92</td></tr><tr><td align="left" colspan="1" rowspan="1">Anthracnose</td><td align="left" colspan="1" rowspan="1">0.82</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.91</td></tr><tr><td align="left" colspan="1" rowspan="1">Citrus Canker</td><td align="left" colspan="1" rowspan="1">0.82</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">1.00</td></tr><tr><td align="left" colspan="1" rowspan="1">Bacterial Blight</td><td align="left" colspan="1" rowspan="1">0.97</td><td align="left" colspan="1" rowspan="1">0.86</td><td align="left" colspan="1" rowspan="1">0.99</td></tr><tr><td align="left" colspan="1" rowspan="1">Deficiency Leaf</td><td align="left" colspan="1" rowspan="1">0.85</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.80</td></tr><tr><td align="left" colspan="1" rowspan="1">Dry Leaf</td><td align="left" colspan="1" rowspan="1">0.96</td><td align="left" colspan="1" rowspan="1">0.86</td><td align="left" colspan="1" rowspan="1">1.00</td></tr><tr><td align="left" rowspan="9" colspan="1"><bold>Proposed Ensemble</bold></td><td align="left" colspan="1" rowspan="1">Spider Mites</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.91</td><td align="left" colspan="1" rowspan="1">0.99</td><td align="left" rowspan="9" colspan="1">96.51</td></tr><tr><td align="left" colspan="1" rowspan="1">Sooty Mould</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.95</td><td align="left" colspan="1" rowspan="1">0.97</td></tr><tr><td align="left" colspan="1" rowspan="1">Healthy Leaf</td><td align="left" colspan="1" rowspan="1">0.96</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.96</td></tr><tr><td align="left" colspan="1" rowspan="1">Curl Virus</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.85</td><td align="left" colspan="1" rowspan="1">0.92</td></tr><tr><td align="left" colspan="1" rowspan="1">Anthracnose</td><td align="left" colspan="1" rowspan="1">0.77</td><td align="left" colspan="1" rowspan="1">0.98</td><td align="left" colspan="1" rowspan="1">0.86</td></tr><tr><td align="left" colspan="1" rowspan="1">Citrus Canker</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.98</td><td align="left" colspan="1" rowspan="1">1.00</td></tr><tr><td align="left" colspan="1" rowspan="1">Bacterial Blight</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">1.00</td></tr><tr><td align="left" colspan="1" rowspan="1">Deficiency Leaf</td><td align="left" colspan="1" rowspan="1">0.96</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">0.98</td></tr><tr><td align="left" colspan="1" rowspan="1">Dry Leaf</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">1.00</td><td align="left" colspan="1" rowspan="1">1.00</td></tr></tbody></table></table-wrap><table-wrap id="Tab9" position="float" orientation="portrait"><label>Table 9</label><caption><p>Inference time comparison of different models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Model</th><th align="left" colspan="1" rowspan="1">Total Inference time (in seconds)</th><th align="left" colspan="1" rowspan="1">Inference time per sample (in seconds)</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">AlexNet</td><td align="left" colspan="1" rowspan="1">0.601311</td><td align="left" colspan="1" rowspan="1">0.002018</td></tr><tr><td align="left" colspan="1" rowspan="1">VGG</td><td align="left" colspan="1" rowspan="1">2.770031</td><td align="left" colspan="1" rowspan="1">0.009295</td></tr><tr><td align="left" colspan="1" rowspan="1">Xception</td><td align="left" colspan="1" rowspan="1">4.968212</td><td align="left" colspan="1" rowspan="1">0.016672</td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>Proposed Ensemble</bold></td><td align="left" colspan="1" rowspan="1"><bold>0.05095</bold></td><td align="left" colspan="1" rowspan="1"><bold>0.000171</bold></td></tr></tbody></table></table-wrap></p><p id="Par62">Table <xref rid="Tab8" ref-type="table">8</xref> shows the metric values obtained during 5th fold for different categories of diseases for individual as well as ensemble model. The Ensemble model, integrating the advantages of separate models, attained superior performance across all metrics: Accuracy (96.5%), F1 Score (96.4%), Recall (96.4%), and Precision (96.5%). This illustrates that the ensemble method successfully alleviates the shortcomings of individual models, resulting in enhanced and more generalizable performance. AlexNet showed a marked improvement, with accuracy increasing from 79.5% to 85.2%, F1 score from 82.4% to 86.3%, and similar gains in recall (81.4% to 85.8%) and precision (83.5% to 86.9%). This improvement reflects the benefit of the proposed model enhancements even for earlier-generation architectures. VGG displayed notable gains as well. Accuracy rose from 85.8% to 90.3%, and F1 score improved from 90.0% to 91.7%. The model also saw increases in recall (88.8% to 91.6%) and precision (91.2% to 91.7%), highlighting the improved generalization capability of the proposed approach. Xception, already a high-performing model under the fuzzy rank-based methodology (accuracy of 88.4%, F1 of 91.5%), further benefited from the proposed framework. Accuracy reached 92.2%, and recall rose to 92.3%, while maintaining strong precision (91.4%) and F1 score (91.8%). The ensemble model outperformed all individual architectures in both the approaches. Under the conventional method, it achieved an accuracy of 91.5% and F1 score of 92.2%, whereas under the proposed methodology, it reached 96.5% accuracy, 96.4% F1 score, and identical recall and precision values (96.4% and 96.5%, respectively). A systematic error was observed in the classification of Anthracnose, where the model exhibited high recall (0.98) but low precision (0.77), indicating frequent mis-classification of other disease classes as Anthracnose. This points to a consistent confusion pattern, arising due to visual similarities of the disease symptoms with classes such as Curl Virus or Deficiency Leaf.</p><p id="Par63">Table <xref rid="Tab9" ref-type="table">9</xref> shows the values of inference time per sample obtained for the independent and ensemble classifiers. The results indicate that the proposed ensemble achieves the lowest total inference time of 0.05095 seconds, which corresponds to 0.000171 seconds per sample. This is significantly faster than all other individual models, with AlexNet, VGG, and Xception requiring 0.601311, 2.770031, and 4.968212 seconds in total, respectively. The considerable reduction in inference time for the proposed ensemble highlights its efficiency, making it more suitable for real-time or large-scale deployment without compromising responsiveness.</p><p id="Par64">The sensitivity analysis<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> of the proposed work and the pre-defined models shows that In Fold-5, the proposed ensemble model consistently matches or outperforms AlexNet, VGG-19, and Xception in sensitivity (recall) across all classes. It achieves 91% for Spider Mites (vs. 68% (AlexNet), 75% (VGG-19), 76%(Xception), 95% for Sooty Mould (vs. 81% (AlexNet), 76% (VGG-19), 100% (Xception)), and 100% for Healthy Leaf, Bacterial Blight, Deficiency Leaf, and Dry Leaf. For Curl Virus, Anthracnose, and Citrus Canker, the ensemble records 85%, 98%, and 98% respectively, outperforming the predefined models. These results highlight the ensemble&#8217;s superior ability to detect true positive cases across all citrus leaf disease categories.</p><p id="Par65">To statistically evaluate the performance gains of the proposed NL-FuRBE model over predefined models, we have applied paired t-tests across 5 folds. The average accuracy of each model over 5 folds subjected to paired t-test. The results show significant improvements (p &lt; 0.05) for all comparisons, confirming the robustness of the observed gains. Table <xref rid="Tab10" ref-type="table">10</xref> shows the results of statistical analysis based on paired t-test.<table-wrap id="Tab10" position="float" orientation="portrait"><label>Table 10</label><caption><p>Statistical significance testing of proposed model against individual models.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Comparison</th><th align="left" colspan="1" rowspan="1">t-value</th><th align="left" colspan="1" rowspan="1">p-value</th><th align="left" colspan="1" rowspan="1">Statistically Significant</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Ensemble vs AlexNet</td><td align="left" colspan="1" rowspan="1">25.73</td><td align="left" colspan="1" rowspan="1"><inline-formula id="IEq42"><alternatives><tex-math id="d33e2424">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.36\times 10^{-5}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq42.gif"/></alternatives></inline-formula></td><td align="left" colspan="1" rowspan="1">Yes</td></tr><tr><td align="left" colspan="1" rowspan="1">Ensemble vs VGG</td><td align="left" colspan="1" rowspan="1">77.81</td><td align="left" colspan="1" rowspan="1"><inline-formula id="IEq43"><alternatives><tex-math id="d33e2438">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$1.64\times 10^{-7}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq43.gif"/></alternatives></inline-formula></td><td align="left" colspan="1" rowspan="1">Yes</td></tr><tr><td align="left" colspan="1" rowspan="1">Ensemble vs Xception</td><td align="left" colspan="1" rowspan="1">3.74</td><td align="left" colspan="1" rowspan="1">0.0201</td><td align="left" colspan="1" rowspan="1">Yes</td></tr></tbody></table></table-wrap></p><p id="Par66">The application of image enhancement techniques before model training markedly enhanced the overall performance of the proposed method. The pre-processing stage improved image contrast, minimized noise, and emphasized essential characteristics, hence providing the model with superior inputs that facilitated more efficient feature extraction and classification.The enhanced images promoted superior convergence during training, increased accuracy, and higher resilience in prediction results, consequently affirming the significance of image enhancement as an essential phase in the model development process. The results demonstrate that, while individual models such as Xception exhibit robust performance, ensembling is an effective method for enhancing reliability and accuracy in practical applications, especially where high precision and recall are essential.</p></sec><sec id="Sec24"><title>Ablation study</title><p id="Par67">To examine the importance of individual stages in the proposed system, we carried out an ablation study<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>. The major goal is to examine the performance in terms of metric values while the dataset is subjected to a pre-processing stage i.e., image denoising and enhancement by application of suitable spatial and morphological datasets. The assessment is carried out to highlight the importance of dataset pre-processing and ensembled model development.<table-wrap id="Tab11" position="float" orientation="portrait"><label>Table 11</label><caption><p>Ablation study results for different models with and without preprocessing.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Metric</th><th align="left" colspan="2" rowspan="1">Ensemble Model</th><th align="left" colspan="2" rowspan="1">AlexNet (independent)</th><th align="left" colspan="2" rowspan="1">VGG (independent)</th><th align="left" colspan="2" rowspan="1">Xception (independent)</th><th align="left" colspan="1" rowspan="1">Variance</th></tr><tr><th align="left" colspan="1" rowspan="1"/><th align="left" colspan="1" rowspan="1">Without Pre-processing</th><th align="left" colspan="1" rowspan="1">With Pre-processing</th><th align="left" colspan="1" rowspan="1">Without Pre-processing</th><th align="left" colspan="1" rowspan="1">With Pre-processing</th><th align="left" colspan="1" rowspan="1">Without Pre-processing</th><th align="left" colspan="1" rowspan="1">With Preprocessing</th><th align="left" colspan="1" rowspan="1">Without Pre-processing</th><th align="left" colspan="1" rowspan="1">With Pre-processing</th><th align="left" colspan="1" rowspan="1"/></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1"><bold>Accuracy</bold></td><td align="left" colspan="1" rowspan="1">91.5</td><td align="left" colspan="1" rowspan="1">96.5</td><td align="left" colspan="1" rowspan="1">79.5</td><td align="left" colspan="1" rowspan="1">85.2</td><td align="left" colspan="1" rowspan="1">85.8</td><td align="left" colspan="1" rowspan="1">90.3</td><td align="left" colspan="1" rowspan="1">88.4</td><td align="left" colspan="1" rowspan="1">92.2</td><td align="left" colspan="1" rowspan="1">26.9821</td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>F1 Score</bold></td><td align="left" colspan="1" rowspan="1">92.2</td><td align="left" colspan="1" rowspan="1">96.4</td><td align="left" colspan="1" rowspan="1">82.4</td><td align="left" colspan="1" rowspan="1">86.3</td><td align="left" colspan="1" rowspan="1">90.0</td><td align="left" colspan="1" rowspan="1">91.7</td><td align="left" colspan="1" rowspan="1">91.5</td><td align="left" colspan="1" rowspan="1">91.8</td><td align="left" colspan="1" rowspan="1">17.8527</td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>Recall</bold></td><td align="left" colspan="1" rowspan="1">92.2</td><td align="left" colspan="1" rowspan="1">96.4</td><td align="left" colspan="1" rowspan="1">81.4</td><td align="left" colspan="1" rowspan="1">85.8</td><td align="left" colspan="1" rowspan="1">88.8</td><td align="left" colspan="1" rowspan="1">91.6</td><td align="left" colspan="1" rowspan="1">91.3</td><td align="left" colspan="1" rowspan="1">92.3</td><td align="left" colspan="1" rowspan="1">21.1964</td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>Precision</bold></td><td align="left" colspan="1" rowspan="1">92.3</td><td align="left" colspan="1" rowspan="1">96.5</td><td align="left" colspan="1" rowspan="1">83.5</td><td align="left" colspan="1" rowspan="1">86.9</td><td align="left" colspan="1" rowspan="1">91.2</td><td align="left" colspan="1" rowspan="1">91.7</td><td align="left" colspan="1" rowspan="1">91.7</td><td align="left" colspan="1" rowspan="1">91.4</td><td align="left" colspan="1" rowspan="1">15.0286</td></tr></tbody></table></table-wrap></p><p id="Par68">
<fig id="Fig19" position="float" orientation="portrait"><label>Fig. 19</label><caption><p>Ablation study- Metric based quantitative evaluation.</p></caption><graphic id="MO23" position="float" orientation="portrait" xlink:href="41598_2025_16923_Fig19_HTML.jpg"/></fig>
</p><p id="Par69">A comparative analysis depicting the performance of individual models and ensemble model for two scenarios- with pre-processing and without pre-processing is shown in Table <xref rid="Tab11" ref-type="table">11</xref>. The efficacy of the proposed model NL-FuRBE was evaluated and ablation was performed considering with VAD, THBH, and without these filters.The results in Table <xref rid="Tab11" ref-type="table">11</xref> demonstrate that the inclusion of pre-processing consistently improves metric values - Accuracy, F1 Score, and Recall for all the models. The most substantial improvements in accuracy were observed in the Ensemble Model (91.5 to 96.5, <inline-formula id="IEq44"><alternatives><tex-math id="d33e2625">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq44.gif"/></alternatives></inline-formula> = +5.0) and AlexNet (79.5 to 85.2, <inline-formula id="IEq45"><alternatives><tex-math id="d33e2631">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Delta$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_16923_Article_IEq44.gif"/></alternatives></inline-formula>= +5.7). The other two metrics F1 Score and Recall shows the similar gains, with Ensemble and AlexNet showing the largest relative gains. Figure <xref rid="Fig19" ref-type="fig">19</xref> gives the comparative representation for various metric values and variance value for the individual models and ensemble model.The findings together affirm that how well the pre-processing of the images works to improve the model&#8217;s ability to predict and categorize the various types of citrus diseases correctly and improving the robustness, generalization capacity, and stability of the designed system.</p></sec><sec id="Sec25"><title>Comparison of proposed methodology with state-of-the-art techniques</title><p id="Par70">The Table <xref rid="Tab12" ref-type="table">12</xref> presents a comparative analysis of Proposed methodology with existing methods. The proposed method, which uses an enhanced fuzzy rank-based ensemble on a dedicated 2025 lemon disease dataset, achieved a strong accuracy of 96.51%.<table-wrap id="Tab12" position="float" orientation="portrait"><label>Table 12</label><caption><p>Comparison of methods for citrus and lemon disease detection.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Studies</th><th align="left" colspan="1" rowspan="1">Methods</th><th align="left" colspan="1" rowspan="1">Dataset</th><th align="left" colspan="1" rowspan="1">Performance</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Butt et.al<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>.</td><td align="left" colspan="1" rowspan="1">DenseNet, AlexNet</td><td align="left" colspan="1" rowspan="1">citrus fruits and leaves dataset (2019)</td><td align="left" colspan="1" rowspan="1">Accuracy = 99.6%</td></tr><tr><td align="left" colspan="1" rowspan="1">Rahman et.al<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>.</td><td align="left" colspan="1" rowspan="1">2 stage deep CNN</td><td align="left" colspan="1" rowspan="1">citrus fruits and leaves dataset (2019)</td><td align="left" colspan="1" rowspan="1">Accuracy = 94.37%</td></tr><tr><td align="left" colspan="1" rowspan="1">Dong et.al<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>.</td><td align="left" colspan="1" rowspan="1">Faster RCNN</td><td align="left" colspan="1" rowspan="1">in-field leaf dataset</td><td align="left" colspan="1" rowspan="1">Average Precision = 84.13%</td></tr><tr><td align="left" colspan="1" rowspan="1">Janarthan et.al<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>.</td><td align="left" colspan="1" rowspan="1">Deep metric learning-based framework</td><td align="left" colspan="1" rowspan="1">citrus fruits and leaves dataset (2019)</td><td align="left" colspan="1" rowspan="1">Accuracy = 95.04%</td></tr><tr><td align="left" colspan="1" rowspan="1">Mehmood et.al<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>.</td><td align="left" colspan="1" rowspan="1">Ensemble method</td><td align="left" colspan="1" rowspan="1">fruit imagery datasets</td><td align="left" colspan="1" rowspan="1">Accuracy = 99%</td></tr><tr><td align="left" colspan="1" rowspan="1"><bold>Proposed</bold></td><td align="left" colspan="1" rowspan="1">Enhanced Fuzzy rank based ensemble</td><td align="left" colspan="1" rowspan="1">Lemon disease dataset (2025)</td><td align="left" colspan="1" rowspan="1">Accuracy = 96.51%</td></tr></tbody></table></table-wrap></p><p id="Par71">The fuzzy rank mechanism effectively prioritizes and integrates classification outputs, enhancing decision reliability under uncertainty, which is common in early-stage plant disease symptoms. The proposed approach&#8217;s competitive performance demonstrates that a fuzzy logic-based ensemble model, when combined with recent, domain-specific datasets, can outperform traditional deep learning models in specific agricultural contexts. This highlights the importance of tailored model architectures and up-to-date datasets in improving precision agriculture applications.This demonstrates its effectiveness and suitability for lemon-specific disease detection, outperforming many existing models on modern datasets and showing promise for future real-time applications.</p></sec></sec><sec id="Sec26"><title>Conclusion and future work</title><p id="Par72">This paper introduces NL-FuRBE, a novel nonlinear Fuzzy Rank-Based Ensemble approach for the precise diagnosis of citrus leaf diseases. The recently devised method employs sophisticated image improvement techniques, particularly Vector-Valued Anisotropic Diffusion (VAD), to improve and denoise photos. This renders the images objectively superior, as evidenced by values of PSNR, SSIM, and NIQE obtained. The proposed model effectively mitigates classifier bias and uncertainty in disease classification by integrating predictions from established transfer learning-based deep learning models (AlexNet, VGG19, and Xception) via a nonlinear fuzzy ranking system. Experimental results on the lemon leaf disease dataset indicate that NL-FuRBE attains an accuracy of 96.51%, an F1 Score of 96.4%, a recall value of 96.4%, and precision values of 96.5%, significantly surpassing both traditional ensembles and cutting-edge approaches. The ensemble model achieved an accuracy of 96.5%, an F1 Score of 96.4%, and recall and precision values of 96.4% and 96.5%, respectively. This study&#8217;s principal finding indicates that the application of effective ensemble techniques, together with enhanced image pre-processing, significantly improves the accuracy and reliability of automated citrus disease detection systems in real-time situations. The efficacy of NL-FuRBE establishes a new benchmark for detecting citrus diseases and provides a cost-effective method for early disease diagnosis in precision agriculture. This method may be extended in the future to incorporate multimodal datasets and real-time field operations utilising resource-constrained devices. This will enhance integrated crop management and contribute to global food security initiatives.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>Bobbinpreet Kaur, Shashi Kant Gupta, Midhunchakkaravarthy Janarthan, Deema Mohammed Alsekait, and Diaa Salama AbdElminaam:&#160;These authors contributed equally to this work.</p></fn></fn-group><ack><title>Acknowledgements</title><p>The authors would like to acknowledge the support of Princess Nourah bint Abdulrahman University Researchers Supporting Project number (PNURSP2025R435), Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>Bobbinpreet Kaur contributed to the conception and design. Shashi Kant Gupta and Midhunchakkaravarthy Janarthan contributed to the collection and assembly of data. Deema Mohammed Alsekait contributed to the development of methodology. Diaa Salama AbdElminaam contributed to the data analysis and interpretation. All authors read and approved the final manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>This research was funded by Princess Nourah bint Abdulrahman University Researchers Supporting Project number (PNURSP2025R435), Princess Nourah bint Abdulrahman University, Riyadh, Saudi Arabia.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The dataset used during the current study are available in the Mendeley Data repository under the title &#8220;Comprehensive Lemon Leaf Disease Dataset for Advanced Detection and Sustainable Agriculture&#8221; (DOI: 10.17632/44nrn4593f.1), <ext-link ext-link-type="uri" xlink:href="https://data.mendeley.com/datasets/44nrn4593f/1">https://data.mendeley.com/datasets/44nrn4593f/1</ext-link>.</p></notes><notes><title>Declarations</title><notes id="FPar3" notes-type="COI-statement"><title>Competing interests</title><p id="Par73">The authors declare no competing interests.</p></notes><notes id="FPar1"><title>Ethical approval</title><p id="Par74">This study exclusively utilizes publicly accessible Lemon disease dataset. The utilized datasets exclude any private information or identifiable data, thereby negating the requirement for ethical approval. The generative AI and AI-assisted technologies have not been utilized in the writing process. That generative AI and AI-assisted technologies have not been used to create or alter images.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Khan</surname><given-names>MA</given-names></name><name name-style="western"><surname>Akram</surname><given-names>T</given-names></name><name name-style="western"><surname>Sharif</surname><given-names>M</given-names></name><etal/></person-group><article-title>Detection and classification of citrus diseases in agriculture based on optimized weighted segmentation and feature selection</article-title><source>Comput. Electron. Agric.</source><year>2020</year><volume>170</volume><fpage>105354</fpage></element-citation><mixed-citation id="mc-CR1" publication-type="journal">Khan, M. A. et al. Detection and classification of citrus diseases in agriculture based on optimized weighted segmentation and feature selection. <italic toggle="yes">Comput. Electron. Agric.</italic><bold>170</bold>, 105354 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Mukhametzyanov, R. R., Brusenko, S. V., Khezhev, A. M., Kelemetov, E. M. &amp; Kirillova, S. S. Changing the global production and trade of citrus fruits. In <italic toggle="yes">Sustainable Development of the Agrarian Economy Based on Digital Technologies and Smart Innovations</italic>, pp. 19-24. Cham: Springer Nature Switzerland, (2024).</mixed-citation></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Naqvi, S. A. M. H. Diagnosis and management of pre and post-harvest diseases of citrus fruit. In <italic toggle="yes">Diseases of Fruits and Vegetables Volume I: Diagnosis and Management</italic>, pp. 339-359. Dordrecht: Springer Netherlands, (2004).</mixed-citation></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saleem</surname><given-names>MH</given-names></name><name name-style="western"><surname>Potgieter</surname><given-names>J</given-names></name><name name-style="western"><surname>Arif</surname><given-names>KM</given-names></name></person-group><article-title>Plant disease detection and classification by deep learning</article-title><source>Plants</source><year>2020</year><volume>9</volume><issue>11</issue><fpage>1302</fpage><pub-id pub-id-type="pmid">31683734</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/plants8110468</pub-id><pub-id pub-id-type="pmcid">PMC6918394</pub-id></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Saleem, M. H., Potgieter, J. &amp; Arif, K. M. Plant disease detection and classification by deep learning. <italic toggle="yes">Plants</italic><bold>9</bold>(11), 1302 (2020).<pub-id pub-id-type="pmid">31683734</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/plants8110468</pub-id><pub-id pub-id-type="pmcid">PMC6918394</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kurup</surname><given-names>SM</given-names></name><name name-style="western"><surname>Amaraja</surname><given-names>VS</given-names></name><name name-style="western"><surname>Kumar</surname><given-names>NR</given-names></name></person-group><article-title>Ensemble deep learning approach for citrus leaf disease classification</article-title><source>J. Plant Dis. Prot.</source><year>2021</year><volume>128</volume><issue>4</issue><fpage>929</fpage><lpage>943</lpage></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Kurup, S. M., Amaraja, V. S. &amp; Kumar, N. R. Ensemble deep learning approach for citrus leaf disease classification. <italic toggle="yes">J. Plant Dis. Prot.</italic><bold>128</bold>(4), 929&#8211;943 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Song</surname><given-names>C</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>D</given-names></name></person-group><article-title>Deep learning-based object detection improvement for tomato disease</article-title><source>IEEE Access</source><year>2021</year><volume>9</volume><fpage>24248</fpage><lpage>24257</lpage></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Zhang, Y., Song, C. &amp; Zhang, D. Deep learning-based object detection improvement for tomato disease. <italic toggle="yes">IEEE Access</italic><bold>9</bold>, 24248&#8211;24257 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Osorio</surname><given-names>K</given-names></name><name name-style="western"><surname>Puerto</surname><given-names>A</given-names></name><name name-style="western"><surname>Pedraza</surname><given-names>C</given-names></name><etal/></person-group><article-title>A multi-core deep learning framework for early diagnosis of citrus greening disease using progressive learning techniques</article-title><source>Appl. Soft Comput.</source><year>2021</year><volume>98</volume><fpage>106859</fpage></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Osorio, K. et al. A multi-core deep learning framework for early diagnosis of citrus greening disease using progressive learning techniques. <italic toggle="yes">Appl. Soft Comput.</italic><bold>98</bold>, 106859 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wagh</surname><given-names>P</given-names></name><name name-style="western"><surname>Sane</surname><given-names>S</given-names></name><name name-style="western"><surname>Kekre</surname><given-names>HB</given-names></name></person-group><article-title>Detection of citrus greening disease using hybrid CNN-LSTM architecture with visual explanations</article-title><source>Expert Syst. Appl.</source><year>2021</year><volume>167</volume><fpage>114346</fpage></element-citation><mixed-citation id="mc-CR8" publication-type="journal">Wagh, P., Sane, S. &amp; Kekre, H. B. Detection of citrus greening disease using hybrid CNN-LSTM architecture with visual explanations. <italic toggle="yes">Expert Syst. Appl.</italic><bold>167</bold>, 114346 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Upadhyay</surname><given-names>N</given-names></name><name name-style="western"><surname>Gupta</surname><given-names>N</given-names></name></person-group><article-title>Detecting fungi-affected multi-crop disease on heterogeneous region dataset using modified ResNeXt approach</article-title><source>Environ Monit Assess</source><year>2024</year><volume>196</volume><fpage>610</fpage><pub-id pub-id-type="doi">10.1007/s10661-024-12790-0</pub-id><pub-id pub-id-type="pmid">38862723</pub-id></element-citation><mixed-citation id="mc-CR9" publication-type="journal">Upadhyay, N. &amp; Gupta, N. Detecting fungi-affected multi-crop disease on heterogeneous region dataset using modified ResNeXt approach. <italic toggle="yes">Environ Monit Assess</italic><bold>196</bold>, 610. 10.1007/s10661-024-12790-0 (2024).<pub-id pub-id-type="pmid">38862723</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s10661-024-12790-0</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sharma</surname><given-names>P</given-names></name><name name-style="western"><surname>Hans</surname><given-names>R</given-names></name><name name-style="western"><surname>Grover</surname><given-names>J</given-names></name></person-group><article-title>Vision transformer-based deep learning model for citrus disease detection</article-title><source>Neural Comput. Appl.</source><year>2022</year><volume>34</volume><fpage>16933</fpage><lpage>16948</lpage></element-citation><mixed-citation id="mc-CR10" publication-type="journal">Sharma, P., Hans, R. &amp; Grover, J. Vision transformer-based deep learning model for citrus disease detection. <italic toggle="yes">Neural Comput. Appl.</italic><bold>34</bold>, 16933&#8211;16948 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other"><ext-link ext-link-type="uri" xlink:href="https://www.fao.org/">https://www.fao.org/</ext-link>.</mixed-citation></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other"><ext-link ext-link-type="uri" xlink:href="https://www.ibge.gov.br/en">https://www.ibge.gov.br/en</ext-link>.</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other"><ext-link ext-link-type="uri" xlink:href="https://www.iari.res.in/en/index.php">https://www.iari.res.in/en/index.php</ext-link>.</mixed-citation></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Csillik</surname><given-names>O</given-names></name><name name-style="western"><surname>Cherbini</surname><given-names>J</given-names></name><name name-style="western"><surname>Johnson</surname><given-names>R</given-names></name><name name-style="western"><surname>Lyons</surname><given-names>A</given-names></name><name name-style="western"><surname>Kelly</surname><given-names>M</given-names></name></person-group><article-title>Identification of citrus trees from unmanned aerial vehicle imagery using convolutional neural networks</article-title><source>Drones</source><year>2018</year><volume>2</volume><issue>4</issue><fpage>39</fpage></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Csillik, O., Cherbini, J., Johnson, R., Lyons, A. &amp; Kelly, M. Identification of citrus trees from unmanned aerial vehicle imagery using convolutional neural networks. <italic toggle="yes">Drones</italic><bold>2</bold>(4), 39 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chane</surname><given-names>CS</given-names></name><name name-style="western"><surname>Mansouri</surname><given-names>A</given-names></name><name name-style="western"><surname>Marzani</surname><given-names>FS</given-names></name><name name-style="western"><surname>Boochs</surname><given-names>F</given-names></name></person-group><article-title>Integration of 3D and multispectral data for cultural heritage applications: Survey and perspectives</article-title><source>Image Vis. Comput.</source><year>2013</year><volume>31</volume><issue>1</issue><fpage>91</fpage><lpage>102</lpage></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Chane, C. S., Mansouri, A., Marzani, F. S. &amp; Boochs, F. Integration of 3D and multispectral data for cultural heritage applications: Survey and perspectives. <italic toggle="yes">Image Vis. Comput.</italic><bold>31</bold>(1), 91&#8211;102 (2013).</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mohanty</surname><given-names>SP</given-names></name><name name-style="western"><surname>Hughes</surname><given-names>DP</given-names></name><name name-style="western"><surname>Salath&#233;</surname><given-names>M</given-names></name></person-group><article-title>Using deep learning for image-based plant disease detection</article-title><source>Front. Plant Sci.</source><year>2016</year><volume>7</volume><fpage>215232</fpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.3389/fpls.2016.01419</pub-id><pub-id pub-id-type="pmcid">PMC5032846</pub-id><pub-id pub-id-type="pmid">27713752</pub-id></element-citation><mixed-citation id="mc-CR16" publication-type="journal">Mohanty, S. P., Hughes, D. P. &amp; Salath&#233;, M. Using deep learning for image-based plant disease detection. <italic toggle="yes">Front. Plant Sci.</italic><bold>7</bold>, 215232 (2016).<pub-id pub-id-type="doi" assigning-authority="pmc">10.3389/fpls.2016.01419</pub-id><pub-id pub-id-type="pmcid">PMC5032846</pub-id><pub-id pub-id-type="pmid">27713752</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Wagle, S. A. A deep learning-based approach in classification and validation of tomato leaf disease. <italic toggle="yes">Traitement du signal</italic><bold>38</bold>(3), 699-709. (2021).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Saha, R. &amp; Neware, S. Orange fruit disease classification using deep learning approach. <italic toggle="yes">International Journal</italic><bold>9</bold>(2), 2297-2301 (2020).</mixed-citation></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sladojevic</surname><given-names>S</given-names></name><name name-style="western"><surname>Arsenovic</surname><given-names>M</given-names></name><name name-style="western"><surname>Anderla</surname><given-names>A</given-names></name><name name-style="western"><surname>Culibrk</surname><given-names>D</given-names></name><name name-style="western"><surname>Stefanovic</surname><given-names>D</given-names></name></person-group><article-title>Deep neural networks based recognition of plant diseases by leaf image classification</article-title><source>Computational intelligence and neuroscience</source><year>2016</year><volume>2016</volume><issue>1</issue><fpage>3289801</fpage><pub-id pub-id-type="pmid">27418923</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1155/2016/3289801</pub-id><pub-id pub-id-type="pmcid">PMC4934169</pub-id></element-citation><mixed-citation id="mc-CR19" publication-type="journal">Sladojevic, S., Arsenovic, M., Anderla, A., Culibrk, D. &amp; Stefanovic, D. Deep neural networks based recognition of plant diseases by leaf image classification. <italic toggle="yes">Computational intelligence and neuroscience</italic><bold>2016</bold>(1), 3289801 (2016).<pub-id pub-id-type="pmid">27418923</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1155/2016/3289801</pub-id><pub-id pub-id-type="pmcid">PMC4934169</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Agarwal</surname><given-names>M</given-names></name><name name-style="western"><surname>Gupta</surname><given-names>SK</given-names></name><name name-style="western"><surname>Biswas</surname><given-names>KK</given-names></name></person-group><article-title>Development of Efficient CNN model for Tomato crop disease identification</article-title><source>Sustain. Comput.: Inform. Syst.</source><year>2020</year><volume>28</volume><fpage>100407</fpage></element-citation><mixed-citation id="mc-CR20" publication-type="journal">Agarwal, M., Gupta, S. K. &amp; Biswas, K. K. Development of Efficient CNN model for Tomato crop disease identification. <italic toggle="yes">Sustain. Comput.: Inform. Syst.</italic><bold>28</bold>, 100407 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rangarajan</surname><given-names>AK</given-names></name><name name-style="western"><surname>Purushothaman</surname><given-names>R</given-names></name><name name-style="western"><surname>Ramesh</surname><given-names>A</given-names></name></person-group><article-title>Tomato crop disease classification using pre-trained deep learning algorithm</article-title><source>Procedia Comput. Sci.</source><year>2018</year><volume>133</volume><fpage>1040</fpage><lpage>1047</lpage></element-citation><mixed-citation id="mc-CR21" publication-type="journal">Rangarajan, A. K., Purushothaman, R. &amp; Ramesh, A. Tomato crop disease classification using pre-trained deep learning algorithm. <italic toggle="yes">Procedia Comput. Sci.</italic><bold>133</bold>, 1040&#8211;1047 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xing</surname><given-names>S.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>M.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>K.-k.</given-names></name></person-group><article-title>Citrus pests and diseases recognition model using weakly dense connected convolution network</article-title><source>Sensors</source><year>2019</year><volume>19</volume><issue>14</issue><fpage>3195</fpage><pub-id pub-id-type="pmid">31331122</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/s19143195</pub-id><pub-id pub-id-type="pmcid">PMC6679302</pub-id></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Xing, S., Lee, M. &amp; Lee, K.-k. Citrus pests and diseases recognition model using weakly dense connected convolution network. <italic toggle="yes">Sensors</italic><bold>19</bold>(14), 3195 (2019).<pub-id pub-id-type="pmid">31331122</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/s19143195</pub-id><pub-id pub-id-type="pmcid">PMC6679302</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Z</given-names></name><name name-style="western"><surname>Yuan</surname><given-names>G</given-names></name><name name-style="western"><surname>Huili</surname><given-names>P</given-names></name><name name-style="western"><surname>Shan</surname><given-names>S</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Z</given-names></name><name name-style="western"><surname>Song</surname><given-names>H</given-names></name><name name-style="western"><surname>Xiangbin</surname><given-names>X</given-names></name></person-group><article-title>1-Methylcyclopropene suppressed the growth of Penicillium digitatum and inhibited the green mould in citrus fruit</article-title><source>J. Phytopathol.</source><year>2021</year><volume>169</volume><issue>2</issue><fpage>83</fpage><lpage>90</lpage></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Wang, Z. et al. 1-Methylcyclopropene suppressed the growth of Penicillium digitatum and inhibited the green mould in citrus fruit. <italic toggle="yes">J. Phytopathol.</italic><bold>169</bold>(2), 83&#8211;90 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Gangwar, A. et al. Time and space efficient multi-model convolution vision transformer for tomato disease detection from leaf images with varied backgrounds. <italic toggle="yes">Comput. Mater. Contin.</italic><bold>79</bold>(1), (2024).</mixed-citation></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Barman</surname><given-names>U</given-names></name><name name-style="western"><surname>Sarma</surname><given-names>P</given-names></name><name name-style="western"><surname>Rahman</surname><given-names>M</given-names></name><name name-style="western"><surname>Deka</surname><given-names>V</given-names></name><name name-style="western"><surname>Lahkar</surname><given-names>S</given-names></name><name name-style="western"><surname>Sharma</surname><given-names>V</given-names></name><name name-style="western"><surname>Saikia</surname><given-names>MJ</given-names></name></person-group><article-title>Vit-SmartAgri: vision transformer and smartphone-based plant disease detection for smart agriculture</article-title><source>Agronomy</source><year>2024</year><volume>14</volume><issue>2</issue><fpage>327</fpage></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Barman, U. et al. Vit-SmartAgri: vision transformer and smartphone-based plant disease detection for smart agriculture. <italic toggle="yes">Agronomy</italic><bold>14</bold>(2), 327 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Janarthan</surname><given-names>S</given-names></name><name name-style="western"><surname>Thuseethan</surname><given-names>S</given-names></name><name name-style="western"><surname>Rajasegarar</surname><given-names>S</given-names></name><name name-style="western"><surname>Lyu</surname><given-names>Q</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>Y</given-names></name><name name-style="western"><surname>Yearwood</surname><given-names>J</given-names></name></person-group><article-title>Deep metric learning based citrus disease classification with sparse data</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>162588</fpage><lpage>162600</lpage></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Janarthan, S. et al. Deep metric learning based citrus disease classification with sparse data. <italic toggle="yes">IEEE Access</italic><bold>8</bold>, 162588&#8211;162600 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dhaka</surname><given-names>VS</given-names></name><name name-style="western"><surname>Kundu</surname><given-names>N</given-names></name><name name-style="western"><surname>Rani</surname><given-names>G</given-names></name><name name-style="western"><surname>Zumpano</surname><given-names>E</given-names></name><name name-style="western"><surname>Vocaturo</surname><given-names>E</given-names></name></person-group><article-title>Role of internet of things and deep learning techniques in plant disease detection and classification: A focused review</article-title><source>Sensors</source><year>2023</year><volume>23</volume><issue>18</issue><fpage>7877</fpage><pub-id pub-id-type="pmid">37765934</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/s23187877</pub-id><pub-id pub-id-type="pmcid">PMC10537018</pub-id></element-citation><mixed-citation id="mc-CR27" publication-type="journal">Dhaka, V. S., Kundu, N., Rani, G., Zumpano, E. &amp; Vocaturo, E. Role of internet of things and deep learning techniques in plant disease detection and classification: A focused review. <italic toggle="yes">Sensors</italic><bold>23</bold>(18), 7877 (2023).<pub-id pub-id-type="pmid">37765934</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/s23187877</pub-id><pub-id pub-id-type="pmcid">PMC10537018</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>X</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>C</given-names></name><name name-style="western"><surname>Sun</surname><given-names>J</given-names></name><name name-style="western"><surname>Cao</surname><given-names>Y</given-names></name><name name-style="western"><surname>Yao</surname><given-names>K</given-names></name><name name-style="western"><surname>Min</surname><given-names>X</given-names></name></person-group><article-title>A deep learning method for predicting lead content in oilseed rape leaves using fluorescence hyperspectral imaging</article-title><source>Food Chem.</source><year>2023</year><volume>409</volume><fpage>135251</fpage><pub-id pub-id-type="pmid">36586261</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.foodchem.2022.135251</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Zhou, X. et al. A deep learning method for predicting lead content in oilseed rape leaves using fluorescence hyperspectral imaging. <italic toggle="yes">Food Chem.</italic><bold>409</bold>, 135251 (2023).<pub-id pub-id-type="pmid">36586261</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.foodchem.2022.135251</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Syed-Ab-Rahman</surname><given-names>S F</given-names></name><name name-style="western"><surname>Hesamian</surname><given-names>M H</given-names></name><name name-style="western"><surname>Prasad</surname><given-names>M</given-names></name></person-group><article-title>Citrus disease detection and classification using end-to-end anchor-based deep learning model</article-title><source>Appl. Intell.</source><year>2022</year><volume>52</volume><issue>1</issue><fpage>927</fpage><lpage>938</lpage></element-citation><mixed-citation id="mc-CR29" publication-type="journal">Syed-Ab-Rahman, S. F., Hesamian, M. H. &amp; Prasad, M.. Citrus disease detection and classification using end-to-end anchor-based deep learning model. <italic toggle="yes">Appl. Intell.</italic><bold>52</bold>(1), 927&#8211;938 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dong</surname><given-names>R</given-names></name><name name-style="western"><surname>Shiraiwa</surname><given-names>A</given-names></name><name name-style="western"><surname>Hayashi</surname><given-names>T</given-names></name></person-group><article-title>A simple diagnostic method for citrus greening disease with deep learning</article-title><source>Electron. Commun. Jpn.</source><year>2025</year><volume>108</volume><issue>1</issue><fpage>e12472</fpage></element-citation><mixed-citation id="mc-CR30" publication-type="journal">Dong, R., Shiraiwa, A. &amp; Hayashi, T. A simple diagnostic method for citrus greening disease with deep learning. <italic toggle="yes">Electron. Commun. Jpn.</italic><bold>108</bold>(1), e12472 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Siam</surname><given-names>AKMF</given-names></name><name name-style="western"><surname>Kobir</surname><given-names>PB</given-names></name><name name-style="western"><surname>Nirob</surname><given-names>MAS</given-names></name><name name-style="western"><surname>Mamun</surname><given-names>SB</given-names></name><name name-style="western"><surname>Assaduzzaman</surname><given-names>M</given-names></name><name name-style="western"><surname>Noori</surname><given-names>SRH</given-names></name></person-group><article-title>A comprehensive image dataset for the identification of lemon leaf diseases and computer vision applications</article-title><source>Data in Brief</source><year>2025</year><volume>58</volume><fpage>111244</fpage><pub-id pub-id-type="pmid">39811522</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.dib.2024.111244</pub-id><pub-id pub-id-type="pmcid">PMC11732584</pub-id></element-citation><mixed-citation id="mc-CR31" publication-type="journal">Siam, A. K. M. F. et al. A comprehensive image dataset for the identification of lemon leaf diseases and computer vision applications. <italic toggle="yes">Data in Brief</italic><bold>58</bold>, 111244 (2025).<pub-id pub-id-type="pmid">39811522</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.dib.2024.111244</pub-id><pub-id pub-id-type="pmcid">PMC11732584</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Perona</surname><given-names>P</given-names></name><name name-style="western"><surname>Malik</surname><given-names>J</given-names></name></person-group><article-title>Scale-space and edge detection using anisotropic diffusion</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>1990</year><volume>12</volume><issue>7</issue><fpage>629</fpage><lpage>639</lpage></element-citation><mixed-citation id="mc-CR32" publication-type="journal">Perona, P. &amp; Malik, J. Scale-space and edge detection using anisotropic diffusion. <italic toggle="yes">IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>12</bold>(7), 629&#8211;639 (1990).</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Whitaker, R. &amp; Gerig, G. Vector-valued diffusion. In G<italic toggle="yes">eometry-driven diffusion in computer vision</italic>, pp. 93-134. Dordrecht: Springer Netherlands, (1994).</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Peter, P. &amp; Weickert, J. Colour image compression with anisotropic diffusion. In <italic toggle="yes">2014 IEEE International Conference on Image Processing (ICIP)</italic>, pp. 4822-4826. IEEE, (2014).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Roussos, A. &amp; Maragos, P. Vector-valued image interpolation by an anisotropic diffusion-projection PDE. In <italic toggle="yes">International Conference on Scale Space and Variational Methods in Computer Vision</italic>, pp. 104-115. Berlin, Heidelberg: Springer Berlin Heidelberg, (2007).</mixed-citation></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Fern&#225;ndez</surname><given-names>J-J</given-names></name><name name-style="western"><surname>Li</surname><given-names>S</given-names></name></person-group><article-title>An improved algorithm for anisotropic nonlinear diffusion for denoising cryo-tomograms</article-title><source>J. Struct. Biol.</source><year>2003</year><volume>144</volume><issue>1&#8211;2</issue><fpage>152</fpage><lpage>161</lpage><pub-id pub-id-type="pmid">14643218</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.jsb.2003.09.010</pub-id></element-citation><mixed-citation id="mc-CR36" publication-type="journal">Fern&#225;ndez, J.-J. &amp; Li, S. An improved algorithm for anisotropic nonlinear diffusion for denoising cryo-tomograms. <italic toggle="yes">J. Struct. Biol.</italic><bold>144</bold>(1&#8211;2), 152&#8211;161 (2003).<pub-id pub-id-type="pmid">14643218</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.jsb.2003.09.010</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bala</surname><given-names>AA</given-names></name><name name-style="western"><surname>Priya</surname><given-names>PA</given-names></name><name name-style="western"><surname>Maik</surname><given-names>V</given-names></name></person-group><article-title>Hybrid technique for fundus image enhancement using modified morphological filter and denoising net</article-title><source>J. Supercomput.</source><year>2024</year><volume>80</volume><issue>9</issue><fpage>13317</fpage><lpage>13340</lpage></element-citation><mixed-citation id="mc-CR37" publication-type="journal">Bala, A. A., Priya, P. A. &amp; Maik, V. Hybrid technique for fundus image enhancement using modified morphological filter and denoising net. <italic toggle="yes">J. Supercomput.</italic><bold>80</bold>(9), 13317&#8211;13340 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Le Van, T. &amp; Van Dang, L. Enhancement of mammographic images based on wavelet denoise and morphological contrast enhancement. <italic toggle="yes">Int. J. Image Graph. Signal Process.</italic><bold>15</bold>(6), 28-40 2023</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Maidana, M. B. F., Noguera, J. L. V. Pinto-Roa, D. P. &amp; Mello-Rom&#225;n, J. C. Noise removal and contrast enhancement in fundus images via morphological operations. In <italic toggle="yes">2022 17th Iberian Conference on Information Systems and Technologies (CISTI)</italic>, pp. 1-7. IEEE, (2022).</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Samajdar, T. &amp; Quraishi, M. I. Analysis and evaluation of image quality metrics. In <italic toggle="yes">Information Systems Design and Intelligent Applications: Proceedings of Second International Conference INDIA 2015</italic>, Volume 2, pp. 369-378. Springer India, (2015).</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Hore, A. &amp; Ziou D. Image quality metrics: PSNR vs. SSIM. In <italic toggle="yes">2010 20th international conference on pattern recognition,</italic> pp. 2366-2369. IEEE, (2010).</mixed-citation></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mittal</surname><given-names>A</given-names></name><name name-style="western"><surname>Soundararajan</surname><given-names>R</given-names></name><name name-style="western"><surname>Bovik</surname><given-names>AC</given-names></name></person-group><article-title>Making a &#8220;completely blind&#8221; image quality analyzer</article-title><source>IEEE Signal Process. Lett.</source><year>2012</year><volume>20</volume><issue>3</issue><fpage>209</fpage><lpage>212</lpage></element-citation><mixed-citation id="mc-CR42" publication-type="journal">Mittal, A., Soundararajan, R. &amp; Bovik, A. C. Making a &#8220;completely blind&#8217;&#8217; image quality analyzer. <italic toggle="yes">IEEE Signal Process. Lett.</italic><bold>20</bold>(3), 209&#8211;212 (2012).</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Udayaraju</surname><given-names>Pa</given-names></name><name name-style="western"><surname>Jeyanthi</surname><given-names>P</given-names></name><name name-style="western"><surname>Sekhar</surname><given-names>BVDS</given-names></name></person-group><article-title>A hybrid multilayered classification model with VGG-19 net for retinal diseases using optical coherence tomography images</article-title><source>Soft Comput.</source><year>2023</year><volume>27</volume><issue>17</issue><fpage>12559</fpage><lpage>12570</lpage></element-citation><mixed-citation id="mc-CR43" publication-type="journal">Udayaraju, P., Jeyanthi, P. &amp; Sekhar, B. V. D. S. A hybrid multilayered classification model with VGG-19 net for retinal diseases using optical coherence tomography images. <italic toggle="yes">Soft Comput.</italic><bold>27</bold>(17), 12559&#8211;12570 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><citation-alternatives><element-citation id="ec-CR44" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Butt</surname><given-names>N</given-names></name><name name-style="western"><surname>Iqbal</surname><given-names>MM</given-names></name><name name-style="western"><surname>Ramzan</surname><given-names>S</given-names></name><name name-style="western"><surname>Raza</surname><given-names>A</given-names></name><name name-style="western"><surname>Abualigah</surname><given-names>L</given-names></name><name name-style="western"><surname>Fitriyani</surname><given-names>NL</given-names></name><name name-style="western"><surname>Yeonghyeon</surname><given-names>G</given-names></name><name name-style="western"><surname>Syafrudin</surname><given-names>M</given-names></name></person-group><article-title>Citrus diseases detection using innovative deep learning approach and Hybrid Meta-Heuristic</article-title><source>PloS one</source><year>2025</year><volume>20</volume><issue>1</issue><fpage>e0316081</fpage><pub-id pub-id-type="pmid">39841644</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1371/journal.pone.0316081</pub-id><pub-id pub-id-type="pmcid">PMC11753642</pub-id></element-citation><mixed-citation id="mc-CR44" publication-type="journal">Butt, N. et al. Citrus diseases detection using innovative deep learning approach and Hybrid Meta-Heuristic. <italic toggle="yes">PloS one</italic><bold>20</bold>(1), e0316081 (2025).<pub-id pub-id-type="pmid">39841644</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1371/journal.pone.0316081</pub-id><pub-id pub-id-type="pmcid">PMC11753642</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Deb</surname><given-names>SD</given-names></name><name name-style="western"><surname>Jha</surname><given-names>RK</given-names></name></person-group><article-title>Breast ultraSound image classification using fuzzy-rank-based ensemble network</article-title><source>Biomed. Signal Process. Control</source><year>2023</year><volume>85</volume><fpage>104871</fpage></element-citation><mixed-citation id="mc-CR45" publication-type="journal">Deb, S. D. &amp; Jha, R. K. Breast UltraSound Image classification using fuzzy-rank-based ensemble network. <italic toggle="yes">Biomed. Signal Process. Control</italic><bold>85</bold>, 104871 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mehmood</surname><given-names>A</given-names></name><name name-style="western"><surname>Ahmad</surname><given-names>M</given-names></name><name name-style="western"><surname>Ilyas</surname><given-names>QM</given-names></name></person-group><article-title>On precision agriculture: enhanced automated fruit disease identification and classification using a new ensemble classification method</article-title><source>Agriculture</source><year>2023</year><volume>13</volume><issue>2</issue><fpage>500</fpage></element-citation><mixed-citation id="mc-CR46" publication-type="journal">Mehmood, A., Ahmad, M. &amp; Ilyas, Q. M. On precision agriculture: enhanced automated fruit disease identification and classification using a new ensemble classification method. <italic toggle="yes">Agriculture</italic><bold>13</bold>(2), 500 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Koparde</surname><given-names>S</given-names></name><name name-style="western"><surname>Kotwal</surname><given-names>J</given-names></name><name name-style="western"><surname>Deshmukh</surname><given-names>S</given-names></name><name name-style="western"><surname>Adsure</surname><given-names>S</given-names></name><name name-style="western"><surname>Chaudhari</surname><given-names>P</given-names></name><name name-style="western"><surname>Kimbahune</surname><given-names>V</given-names></name></person-group><article-title>A conditional generative adversarial networks and Yolov5 darknet-based skin lesion localization and classification using independent component analysis model</article-title><source>Inform. Med. Unlocked</source><year>2024</year><volume>47</volume><fpage>101515</fpage></element-citation><mixed-citation id="mc-CR47" publication-type="journal">Koparde, S. et al. A conditional generative adversarial networks and Yolov5 darknet-based skin lesion localization and classification using independent component analysis model. <italic toggle="yes">Inform. Med. Unlocked</italic><bold>47</bold>, 101515 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><citation-alternatives><element-citation id="ec-CR48" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kotwal</surname><given-names>J</given-names></name><name name-style="western"><surname>Futane</surname><given-names>P</given-names></name><name name-style="western"><surname>Chavan</surname><given-names>G</given-names></name><name name-style="western"><surname>Chaudhari</surname><given-names>A</given-names></name><name name-style="western"><surname>Jose</surname><given-names>J</given-names></name><name name-style="western"><surname>Khan</surname><given-names>V</given-names></name></person-group><article-title>Sensor infused quantum CNN for diabetes disease prediction and diet recommendation</article-title><source>Int. J. Comput. Intell. Syst.</source><year>2025</year><volume>18</volume><issue>1</issue><fpage>113</fpage></element-citation><mixed-citation id="mc-CR48" publication-type="journal">Kotwal, J. et al. Sensor infused quantum CNN for diabetes disease prediction and diet recommendation. <italic toggle="yes">Int. J. Comput. Intell. Syst.</italic><bold>18</bold>(1), 113 (2025).</mixed-citation></citation-alternatives></ref></ref-list></back></article>
        
    </metadata>
</record>
    </GetRecord>

</OAI-PMH>