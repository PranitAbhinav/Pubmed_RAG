


<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
    <responseDate>2025-09-09T14:05:43Z</responseDate>
    <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:12405521" metadataPrefix="pmc">https://pmc.ncbi.nlm.nih.gov/api/oai/v1/mh/</request>
    
    <GetRecord>
        <record>
    <header>
    <identifier>oai:pubmedcentral.nih.gov:12405521</identifier>
    <datestamp>2025-09-04</datestamp>
    
        
        <setSpec>npjdigitmed</setSpec>
        
    
        
        <setSpec>pmc-open</setSpec>
        
    
</header>
    <metadata>
        
        <article xmlns="https://jats.nlm.nih.gov/ns/archiving/1.4/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.4/ https://jats.nlm.nih.gov/archiving/1.4/xsd/JATS-archivearticle1-4.xsd" article-type="research-article" xml:lang="en" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">NPJ Digit Med</journal-id><journal-id journal-id-type="iso-abbrev">NPJ Digit Med</journal-id><journal-id journal-id-type="pmc-domain-id">3605</journal-id><journal-id journal-id-type="pmc-domain">npjdigitmed</journal-id><journal-title-group><journal-title>NPJ Digital Medicine</journal-title></journal-title-group><issn pub-type="epub">2398-6352</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12405521</article-id><article-id pub-id-type="pmcid-ver">PMC12405521.1</article-id><article-id pub-id-type="pmcaid">12405521</article-id><article-id pub-id-type="pmcaiid">12405521</article-id><article-id pub-id-type="pmid">40897901</article-id><article-id pub-id-type="doi">10.1038/s41746-025-01964-w</article-id><article-id pub-id-type="publisher-id">1964</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Large-vocabulary segmentation for medical images with text prompts</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Zhao</surname><given-names initials="Z">Ziheng</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="Y">Yao</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wu</surname><given-names initials="C">Chaoyi</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="X">Xiaoman</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhou</surname><given-names initials="X">Xiao</given-names></name><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Zhang</surname><given-names initials="Y">Ya</given-names></name><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name name-style="western"><surname>Wang</surname><given-names initials="Y">Yanfeng</given-names></name><address><email>wangyanfeng622@sjtu.edu.cn</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><contrib contrib-type="author" corresp="yes"><name name-style="western"><surname>Xie</surname><given-names initials="W">Weidi</given-names></name><address><email>weidi@sjtu.edu.c</email></address><xref ref-type="aff" rid="Aff1">1</xref><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0220qvk04</institution-id><institution-id institution-id-type="GRID">grid.16821.3c</institution-id><institution-id institution-id-type="ISNI">0000 0004 0368 8293</institution-id><institution>Shanghai Jiao Tong University, </institution></institution-wrap>Shanghai, China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03wkvpx79</institution-id><institution-id institution-id-type="ISNI">0000 0005 0475 7227</institution-id><institution>Shanghai Artificial Intelligence Laboratory, </institution></institution-wrap>Shanghai, China </aff></contrib-group><pub-date pub-type="epub"><day>2</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>8</volume><issue-id pub-id-type="pmc-issue-id">478273</issue-id><elocation-id>566</elocation-id><history><date date-type="received"><day>9</day><month>4</month><year>2025</year></date><date date-type="accepted"><day>17</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-04 00:25:59.930"><day>04</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri content-type="pmc-pdf" xlink:href="41746_2025_Article_1964.pdf"/><abstract id="Abs1"><p id="Par1">This paper aims to build a model that can <bold>S</bold>egment <bold>A</bold>nything in 3D medical images, driven by medical terminologies as <bold>T</bold>ext prompts, termed as <bold>SAT</bold>. Our main contributions are three-fold: (i) We construct the first multimodal knowledge tree on human anatomy, including <bold>6502 anatomical terminologies</bold>; Then, we build the largest and most comprehensive segmentation dataset for training, collecting over <bold>22K 3D scans</bold> from <bold>72 datasets</bold>, across <bold>497 classes</bold>, with careful standardization on both image and label space; (ii) We propose to inject medical knowledge into a text encoder via contrastive learning and formulate a large-vocabulary segmentation model that can be prompted by medical terminologies in text form. (iii) We train <bold>SAT-Nano</bold> (110M parameters) and <bold>SAT-Pro</bold> (447M parameters). <bold>SAT-Pro</bold> achieves comparable performance to 72 nnU-Nets&#8212;the strongest specialist models trained on each dataset (over 2.2B parameters combined)&#8212;over 497 categories. Compared with the interactive approach MedSAM, SAT-Pro consistently outperforms across all 7 human body regions with +7.1% average Dice Similarity Coefficient (DSC) improvement, while showing enhanced scalability and robustness. On 2 external (cross-center) datasets, SAT-Pro achieves higher performance than all baselines (+3.7% average DSC), demonstrating superior generalization ability.</p></abstract><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Machine learning</kwd><kwd>Medical imaging</kwd></kwd-group><funding-group><award-group><funding-source><institution>National Key R&amp;D Program of China</institution></funding-source><award-id>No. 2022ZD0160702</award-id></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1" sec-type="introduction"><title>Introduction</title><p id="Par2">Medical image segmentation aims to identify and delineate regions of interest (ROIs) like organs, lesions, and tissues in diverse medical images, which plays a crucial role in numerous clinical applications, such as disease diagnosis, treatment planning, and disease progression tracking<sup><xref ref-type="bibr" rid="CR1">1</xref>&#8211;<xref ref-type="bibr" rid="CR5">5</xref></sup>, as well as in medical research<sup><xref ref-type="bibr" rid="CR6">6</xref>,<xref ref-type="bibr" rid="CR7">7</xref></sup>. Traditionally, radiologists perform manual segmentation to measure volume, shape, and location in a slice-wise manner, which is both time-consuming and challenging to scale with the growing volume of medical data. Consequently, there is a pressing need for automated and robust medical image segmentation methods in clinical settings, to enhance efficiency and scalability.</p><p id="Par3">Recent advancements in medical image analysis have been marked by a surge in deep learning. These developments have yielded a spectrum of segmentation models, each trained for specific tasks<sup><xref ref-type="bibr" rid="CR3">3</xref>,<xref ref-type="bibr" rid="CR8">8</xref>&#8211;<xref ref-type="bibr" rid="CR13">13</xref></sup>, often referred to as &#8216;specialist&#8217; models. While these models demonstrate impressive segmentation capabilities, their major drawback lies in their narrow specialization. Designed and optimized for distinct ROIs and imaging modalities, these models<sup><xref ref-type="bibr" rid="CR14">14</xref>&#8211;<xref ref-type="bibr" rid="CR19">19</xref></sup> require distinct preprocessing methods for each dataset. As a result, they often fall short in diverse and dynamic clinical environments, where adaptability to new conditions and imaging techniques is essential.</p><p id="Par4">There is a growing interest in developing foundation models for medical image segmentation<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup>, by adapting the pre-trained segment anything model (SAM)<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> models from the computer vision community. However, while transferring to medical scenarios, these models trained on natural images suffer from fundamental limitations: (i) models typically perform 2D slice segmentation, which is later fused into 3D volumes through post-processing. This approach overlooks the crucial contextual information in 3D radiological imaging; (ii) models often require point or box inputs as prompts, thus are interactive segmentation models, requiring considerable manual effort for use in practice; (iii) models suffer from significant domain gaps, from image statistics to domain-specific medical knowledge.</p><p id="Par5">In this paper, we present the <italic toggle="yes">first knowledge-enhanced</italic> foundation model for 3D medical volume segmentation, <italic toggle="yes">with medical terminology as text prompt, termed as</italic>
<bold>SAT</bold>. In practice, our model can effectively take 3D volumes as visual inputs along with text prompts to seamlessly tackle various medical image segmentation tasks, across modalities, anatomies, and body regions. As illustrated in Fig. <xref rid="Fig1" ref-type="fig">1</xref>, our proposed method distinguishes itself from previous medical segmentation paradigms, that can be seamlessly applied to clinical practice or integrated with any large language model. Specifically, we make the following contributions in Fig. <xref rid="Fig2" ref-type="fig">2</xref>:<fig id="Fig1" position="float" orientation="portrait"><label>Fig. 1</label><caption><title>Segment Anything in 3D medical images with Text.</title><p>In contrast to conventional specialist models (<bold>a</bold>) that develop specialized solutions for each task, or recently proposed interactive segmentation foundation models (<bold>b</bold>) relying on real-time human interventions, segment anything by text (SAT) directly takes 3D volumes as inputs, and uses text as prompts to perform a wide array of medical image segmentation tasks across different modalities, anatomies, and body regions (<bold>c</bold>). It can be easily applied to clinics or seamlessly integrated with any agent-based large language model.</p></caption><graphic id="d33e307" position="float" orientation="portrait" xlink:href="41746_2025_1964_Fig1_HTML.jpg"/></fig><fig id="Fig2" position="float" orientation="portrait"><label>Fig. 2</label><caption><title>Overview of SAT-DS.</title><p>comprising diverse segmentation tasks spanning multiple imaging modalities and anatomical regions, including the brain, head and neck, thorax, spine, abdomen, upper limbs, lower limbs, and pelvis. This comprehensive dataset enables the training of a large-vocabulary segmentation foundation model.</p></caption><graphic id="d33e315" position="float" orientation="portrait" xlink:href="41746_2025_1964_Fig2_HTML.jpg"/></fig></p><p id="Par6"><italic toggle="yes">On dataset construction</italic>, we construct a knowledge tree on anatomy concepts and definitions throughout the human body. On the visual side, we curate over 22K 3D medical image scans with 302K anatomical segmentation annotations, covering 497 categories from 72 publicly available medical segmentation datasets, termed as <bold>SAT-DS</bold> (Fig. <xref rid="Fig2" ref-type="fig">2</xref>). To the best of our knowledge, SAT-DS represents the largest and most comprehensive collection of public 3D medical segmentation datasets. To achieve this goal, we have invested significant effort in standardizing datasets and unifying annotation labels, paving the way for training a large-vocabulary segmentation foundation model.</p><p id="Par7"><italic toggle="yes">On architecture design and training strategy</italic>, we build a large-vocabulary segmentation foundation model that enables flexible segmentation across a spectrum of medical imaging modalities and anatomies, with text prompts. Specifically, we adopt knowledge-enhanced representation learning, leveraging textual anatomical knowledge and atlas segmentation of specific anatomical structures to train the visual-language encoders. Through this training process, the visual features of these anatomical structures are aligned with their corresponding text descriptions in the latent space, which is validated to boost the segmentation performance, especially in a long-tail distribution. Subsequently, the text embeddings of anatomy/abnormality are treated as queries in a Transformer-based architecture, iteratively attending to the visual features to update queries for precise segmentation of the queried target. To meet requirements from different computational resources, we train two models of varying sizes, namely, <bold>SAT-Nano</bold> and <bold>SAT-Pro</bold>, and validate the effectiveness of scaling model sizes.</p><p id="Par8"><italic toggle="yes">On experiment evaluation</italic>, we devise comprehensive metrics for large-vocabulary medical segmentation across various aspects, including region-wise average, organ-wise average, and dataset-wise average. Through extensive internal and external experiments, we demonstrate that:<list list-type="bullet"><list-item><p id="Par9">Building on the unprecedented dataset collection, SAT is able to handle a wide range of downstream segmentation tasks with medical terminologies as text prompts, simplifying the training and deployment procedure for conventional specialist models. On internal evaluation, <bold>SAT-Pro</bold> shows comparable overall performance to 72 nnU-Net models&#8212;the strongest specialist models that are specialized and trained individually on each dataset&#8212;over 497 categories, while using only 20% of their combined model parameters (447M vs. 2.2B+).</p></list-item><list-item><p id="Par10">Driven by text prompts, SAT outlines a novel paradigm for segmentation foundation model, as opposed to previous interactive approaches that rely on spatial prompts. This could save tremendous manual efforts from prompting in clinical applications. On performance, SAT-Pro consistently outperforms the state-of-the-art interactive model MedSAM across 7 human body regions, while being robust to targets with ambiguous spatial relationships.</p></list-item><list-item><p id="Par11">Compared to BiomedParse<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, a concurrent model on text-prompted biomedical image segmentation, SAT-Pro not only exhibits superior performance on 29 out of 30 categories, but also showcases a significantly broader capability on radiology images.</p></list-item><list-item><p id="Par12">On external evaluation, SAT-Pro delivers the best results across both external validation datasets, and surpasses all baselines including specialist and generalist models, highlighting its strong generalization capabilities as a foundation model.</p></list-item><list-item><p id="Par13">The text-prompted feature and large vocabulary of SAT make it a powerful out-of-the-box agent for language model. We show SAT can be seamlessly integrated with any large language models, such as GPT-4<sup><xref ref-type="bibr" rid="CR24">24</xref></sup>, automatically providing grounding ability in diverse clinical scenarios. This potentially extends the application diagram of medical segmentation models, and advance generalist medical artificial intelligence.</p></list-item></list></p></sec><sec id="Sec2" sec-type="results"><title>Results</title><p id="Par14">We propose <bold>S</bold>egment <bold>A</bold>nything with <bold>T</bold>ext (<bold>SAT</bold>), a large-vocabulary segmentation foundation model for 3D medical images. The objective is to handle a wide range of heterogeneous tasks using text prompts. It includes 497 anatomical targets across 8 regions and various lesions of the human body, assembled from 72 distinct datasets. To balance the computational cost and performance, we train and evaluate two variants <bold>SAT-Pro</bold> and <bold>SAT-Nano</bold>.</p><p id="Par15">In this section, we detail the experiment results, where <bold>SAT</bold> is comprehensively evaluated against three categories of methods: (i) <italic toggle="yes">specialist models</italic>, which are optimized and trained individually for each dataset, following the conventional mainstream practice in medical image segmentation. We choose nnU-Nets<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, SwinUNETR<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>, and U-Mamba<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> for comparison, as they are widely adopted representatives for CNN-based, Transformer-based, and Mamba-based architecture, respectively; (ii) <italic toggle="yes">interactive segmentation models</italic>, which have been recently investigated to provide semi-automatic segmentation with spatial prompts. We choose MedSAM<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> as a typical and state-of-the-art baseline; (iii) <italic toggle="yes">text-prompted segmentation models</italic>, which represent a paradigm shift from the previous two, capable of performing automatic segmentation across a wide range of tasks with text prompts. BiomedParse<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> is a concurrent work to ours and compared in this study.</p><p id="Par16">The evaluations are conducted on both <italic toggle="yes">internal</italic> and <italic toggle="yes">external</italic> datasets. Specifically, we split each dataset in SAT-DS into train and test splits in 8:2 ratio, a combination of these test splits is used for internal evaluation, i.e., in-domain data. When comparing to off-the-shelf models, we tailor the scope of datasets to accommodate their varying capabilities, to avoid overlapping the train and test data. The external evaluation is conducted on two very recently published datasets, namely, AbdomenAtlas 1.1<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> and LiQA<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>, as they are excluded from SAT-DS and not used in training any of these methods. This simulates the scenario where the models are tested on multi-center images. <italic toggle="yes">Note that</italic> this does not involve new classes, as the segmentation targets in the human body are relatively limited and fixed.</p><p id="Par17">We present evaluation results from various aspects, including <italic toggle="yes">region-wise</italic>, <italic toggle="yes">class-wise</italic>, and <italic toggle="yes">dataset-wise</italic>, to give a deep understanding of the models&#8217; performance on large-scale segmentation. Note that class-wise and region-wise evaluations are computed by averaging the results from different datasets. For instance, the performance metrics for the &#8220;brainstem&#8221; in CT images represent the macro average from models trained on datasets, like &#8220;HAN Seg&#8221;, &#8220;PDDCA&#8221;, and &#8220;SegRap2023 Task 1&#8221;, that all include annotations for this anatomical class. Detailed experiment settings can be found in Section &#8220;Evaluation protocols&#8221;.</p><p id="Par18">The following sections start with experiments on internal datasets in Sections &#8221;Comparison with specialist models on automatic segmentation&#8221;, &#8220;Comparison with interactive segmentation foundation model&#8221;, and &#8220;Compare with text-prompted segmentation foundation model&#8221;, with more detailed results available in the &#8220;Detailed internal evaluation results&#8221; Section in the Supplementary. Then, we present the results of different methods on external datasets in Section &#8220;Evaluation on external datasets&#8221;, with more detailed results available in the &#8220;Detailed external evaluation results&#8221; Section in the Supplementary. Finally, we demonstrate the impact of knowledge injection in Section &#8220;Ablation study on text encoder&#8221;, and SAT&#8217;s potential application scenarios in Section &#8220;Qualitative results&#8212;SAT as an interface between language and segmentation&#8221;. Additional ablation experiments are provided in the &#8220;Extended ablation studies&#8221; Section in the Supplementary; Model calibration analysis is presented in the &#8220;Calibration analysis&#8221; Section in the Supplementary.</p><sec id="Sec3"><title>Comparison with specialist models on automatic segmentation</title><p id="Par19">In this experiment setting, we compare with specialist models (nnU-Nets, U-Mamba, SwinUNETR) on all 72 datasets in <bold>SAT-DS</bold> as internal evaluation. All specialist models are trained with optimized configuration on each dataset with official codes. While both <bold>SAT-Pro</bold> and <bold>SAT-Nano</bold> are trained and evaluated on all datasets as one model. <italic toggle="yes">Note that</italic>, unless otherwise stated, SAT-Pro and SAT-Nano are trained on all 72 datasets of SAT-DS throughout the following text.</p><p id="Par20">Figure <xref rid="Fig3" ref-type="fig">3</xref>a and Supplementary Table <xref rid="MOESM1" ref-type="media">3</xref> shows the <italic toggle="yes">region-wise results</italic> on 8 regions of human body, including &#8220;Brain&#8221;, &#8220;Head and Neck&#8221;, &#8220;Thorax&#8221;, &#8220;Abdomen&#8221;, &#8220;Pelvis&#8221;, &#8220;Spine&#8221;, &#8220;Upper Limb&#8221;, and &#8220;Lower Limb&#8221;, as well as &#8220;Lesion&#8221;, in terms of dice similarity coefficient (DSC) and normalized surface distance (NSD), respectively. Classes existing in multiple regions are specifically grouped as &#8220;whole body&#8221;.<fig id="Fig3" position="float" orientation="portrait"><label>Fig. 3</label><caption><title>Internal evaluation between SAT-Pro, SAT-Nano, and three specialist models on 72 datasets from SAT-DS.</title><p>Results are merged by different human body regions and lesions. <bold>a</bold> Box plots on DSC and NSD results. The center line within each box indicates the median value; the bottom and top bounds indicate the 25th percentiles and 75th percentiles, respectively. The mean value is marked with a plus sign. The whiskers extend to 1.5 times the interquartile range. Outlier classes are plotted as individual dots. <bold>b</bold> Comparison between SAT-Pro and the most competitive specialist models nnU-Nets on performance. <bold>c</bold> Comparison between SAT and specialist models on model size and capability range. SAT has a much smaller model size compared to the ensemble of specialist models, while capable of segmenting 497 targets in one model. By comparison, each specialist model can only segment 12 targets on average.</p></caption><graphic id="d33e501" position="float" orientation="portrait" xlink:href="41746_2025_1964_Fig3_HTML.jpg"/></fig></p><p id="Par21">Despite having been proposed for a few years, nnU-Nets remains the best-performing specialist model overall. As a generalist model, <bold>SAT-Pro</bold> consistently outperforms the most competitive baseline nnU-Nets in four regions: head and neck, thorax, upper limb, and lower limb. On average, DSC of all 497 categories, SAT-Pro shows comparable performance to nnU-Nets (paired <italic toggle="yes">t</italic>-test <italic toggle="yes">p</italic>&#8201;&gt;&#8201;0.09) and U-Mamba (<italic toggle="yes">p</italic>&#8201;&gt;&#8201;0.13), while surpassing SwinUNETR significantly (<italic toggle="yes">p</italic>&#8201;&lt;&#8201;2&#8201;&#215;&#8201;10<sup>&#8722;5</sup>).</p><p id="Par22">Figure <xref rid="Fig3" ref-type="fig">3</xref>b, c provide another view on the above results, where it can be seen that SAT-Pro shows comparable segmentation performance to the 72 nnU-Nets, while being significantly smaller in size and more capable; for example, SAT-Pro is approximately <bold>1/5</bold> of the ensemble of nnU-Nets, and is able to handle 497 classes, in contrast to each specialist model handling an average of only 12 classes.</p><p id="Par23">We further finetune <bold>SAT-Pro</bold> on each dataset, and report the <italic toggle="yes">region-wise</italic> results in Supplementary Table <xref rid="MOESM1" ref-type="media">3</xref>, denoted as <bold>SAT-Ft</bold>. SAT-Ft shows notable improvement over SAT-Pro on all the regions and lesions. On average performance over all categories, it outperforms U-Mamba on both DSC (<italic toggle="yes">p</italic>&#8201;&lt;&#8201;2&#8201;&#215;&#8201;10<sup>&#8722;9</sup>) and NSD (<italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.01), and nnU-Nets on NSD (<italic toggle="yes">p</italic>&#8201;&lt;&#8201;6 &#215;&#8201;10<sup>&#8722;9</sup>). This indicates that SAT can serve as a strong pre-trained model for further adaptation.</p><p id="Par24">We present dataset-wise results in Supplementary Tables <xref rid="MOESM1" ref-type="media">5</xref>, <xref rid="MOESM1" ref-type="media">6</xref>, <xref rid="MOESM1" ref-type="media">7</xref>, and <xref rid="MOESM1" ref-type="media">8</xref>, and more detailed class-wise results in Supplementary Tables <xref rid="MOESM1" ref-type="media">9</xref>, <xref rid="MOESM1" ref-type="media">10</xref>, <xref rid="MOESM1" ref-type="media">11</xref>, and <xref rid="MOESM1" ref-type="media">12</xref>;</p></sec><sec id="Sec4"><title>Comparison with interactive segmentation foundation model</title><p id="Par25">In this section, we compare with MedSAM, an out-of-the-box interactive segmentation method trained on large-scale data. Due to inconsistent training data, we focus the internal evaluation on all the 32 datasets (out of 72) that were involved in training MedSAM for fair comparison. <italic toggle="yes">Note that</italic> even though these datasets are included in MedSAM&#8217;s training, we are unable to align the train-test splits. This means our test set might have been leaked in MedSAM&#8217;s training. We report <italic toggle="yes">three results</italic>: (i) simulate box prompts based on ground truth segmentation, using the minimum rectangle covering the ground truth (denoted as Tight), i.e., the most accurate prompts; (ii) randomly shift each box corner by up to 8% of the image resolution (denoted as Loose), i.e., allowing errors to some extent; (iii) directly use the tight box prompts as prediction (denoted as Oracle Box), i.e., the input baseline for MedSAM.</p><p id="Par26">Figure <xref rid="Fig4" ref-type="fig">4</xref>a and Supplementary Table <xref rid="MOESM1" ref-type="media">4</xref> show the <italic toggle="yes">region-wise results</italic> for all methods. Notably, <bold>SAT-Pro</bold> consistently outperforms MedSAM across all human body regions, even when MedSAM is prompted with the most accurate box (Tight), and achieves significantly superior average performance over all categories (paired <italic toggle="yes">t</italic>-test <italic toggle="yes">p</italic>&#8201;&lt;&#8201;2&#8201;&#215;&#8201;10<sup>&#8722;9</sup>). For lesion segmentation, <bold>SAT-Pro</bold> underperforms MedSAM (Tight) due to the small lesion size, where the box prompts provide very strong priors to MedSAM, as evidenced by the oracle box even outperforming MedSAM&#8217;s output on DSC score. When perturbing the box prompts, MedSAM (Loose) shows significant performance drops across all regions and metrics.<fig id="Fig4" position="float" orientation="portrait"><label>Fig. 4</label><caption><title>Internal evaluation between SAT-Pro, SAT-Nano, and MedSAM on 32 datasets from SAT-DS.</title><p>Results are merged by different human body regions and lesions. <bold>a</bold> Histograms on DSC and NSD results. <bold>b</bold> Scatter plots comparing the performance improvement of SAT-Pro over MedSAM on different segmentation targets (DSC score), with two irregularity metrics: convex ratio and oracle box DSC. Each point represents an anatomical structure or lesion, with a fitted line illustrating the trend. <bold>c</bold> Average prompt numbers required by MedSAM to segment a target in 3D radiology scan, averaged over different human body regions. <bold>d</bold> Quantitative results of MedSAM and SAT-Pro on myocardium (upper row) and colon cancer (lower row). The ground truth and segmentation masks are painted in red, while the box prompts of MedSAM are plotted in black. The DSC score is calculated in a slice-wise manner. H&amp; N head and neck.</p></caption><graphic id="d33e641" position="float" orientation="portrait" xlink:href="41746_2025_1964_Fig4_HTML.jpg"/></fig></p><p id="Par27">On <italic toggle="yes">class-wise results</italic>, in Fig. <xref rid="Fig4" ref-type="fig">4</xref>b, we present the performance difference between SAT-Pro and MedSAM on each category, with respect to the spatial irregularity of regions. Inspired by BiomedParse<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, we define spatial irregularity with two factors: the ratio of ground truth to the tightest convex, denoted as &#8220;Convex Ratio&#8221;; the DSC score between oracle box prompt and ground truth, denoted as &#8220;Oracle Box DSC&#8221;. We observe that SAT-Pro achieves greater improvement on targets with more irregular shapes, while MedSAM outperforms on some relatively regular-shaped targets.</p><p id="Par28">We further present <italic toggle="yes">qualitative results</italic> from two representative examples in Fig. <xref rid="Fig4" ref-type="fig">4</xref>d. The upper row shows segmentation of myocardium with a relatively irregular shape. MedSAM incorrectly includes the left heart ventricle surrounded by the myocardium. By comparison, SAT-Pro generates accurate predictions when simply prompted with the word &#8220;myocardium&#8221;. The lower row demonstrates colon cancer segmentation on a CT image. The tight box prompt to MedSAM can be viewed as an acceptable segmentation, despite its limitation as a rectangle, while MedSAM&#8217;s prediction is worse. In addition, in both cases, we observe noticeable performance drops when the box prompt contains certain deviations, i.e., MedSAM (Loose).</p><p id="Par29">In Fig. <xref rid="Fig4" ref-type="fig">4</xref>c, we show the average number of prompts required by MedSAM to segment a target in a 3D image scan. As it only allows slice-wise segmentation and the morphology of segmentation targets varies across different body regions, the number ranges from 10+ to 60+. By contrast, as a fully automatic segmentation model for 3D radiology images, SAT requires only a single text prompt to segment the entire 3D scan. This simplicity and scalability advantage become more pronounced for multiple target segmentation.</p><p id="Par30">We present <italic toggle="yes">dataset-wise results</italic> in Supplementary Tables <xref rid="MOESM1" ref-type="media">5</xref>, <xref rid="MOESM1" ref-type="media">6</xref>, <xref rid="MOESM1" ref-type="media">7</xref>, and <xref rid="MOESM1" ref-type="media">8</xref>, and more detailed <italic toggle="yes">class-wise results</italic> in Supplementary Table <xref rid="MOESM1" ref-type="media">13</xref>.</p></sec><sec id="Sec5"><title>Compare with text-prompted segmentation foundation model</title><p id="Par31">In this section, we compare with BiomedParse<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, a concurrent work that proposed a segmentation tool for general 2D biomedical images prompted by text. Due to inconsistent training data, we focus the internal evaluation on all the 11 datasets (out of 72) that were involved in training BiomedParse for fair comparison. We report two results for BiomedParse: (i) Based on the ground truth, we only prompt targets present in the current slice, which follows its official evaluation setting. Similar to MedSAM, this approach avoids potential false positives on unannotated slices and thus represents performance under ideal conditions. We denote these results as BiomedParse (Oracle); (ii) Consistent with SAT, we prompt all targets available in the dataset and filter out potential false positive predictions by p-values, as suggested by the official implementation.</p><p id="Par32">Figure <xref rid="Fig5" ref-type="fig">5</xref>a and Supplementary Table <xref rid="MOESM1" ref-type="media">14</xref> present the <italic toggle="yes">class-wise</italic> performance of SAT and BiomedParse. Across all categories, BiomedParse (Oracle) consistently achieves higher DSC and NSD scores compared to BiomedParse. This highlights that BiomedParse is prone to generating false positive predictions when prompted with non-existing targets, likely because BiomedParse is a 2D slice segmentation model that overlooks critical information from adjacent slices. <bold>SAT-Pro</bold> consistently outperforms BiomedParse in all 30 categories except myocardium. Even compared to BiomedParse (Oracle), SAT-Pro demonstrates superior performance on 23 out of 30 categories and notably excels in overall performance. On average across all categories, both <bold>SAT-Pro</bold> and <bold>SAT-Nano</bold> significantly outperforms BiomedParse (Oracle) (paired <italic toggle="yes">t</italic>-test <italic toggle="yes">p</italic>&#8201;&lt;&#8201;7&#8201;&#215;&#8201;10<sup>&#8722;3</sup> for DSC and <italic toggle="yes">p</italic>&#8201;&lt;&#8201;2&#8201;&#215;&#8201;10<sup>&#8722;6</sup> for NSD).<fig id="Fig5" position="float" orientation="portrait"><label>Fig. 5</label><caption><title>Internal evaluation between SAT-Pro and BiomedParse on 11 datasets from SAT-DS.</title><p><bold>a</bold> DSC and NSD scores. Results are merged and presented in a class-wise manner. <bold>b</bold> The number of anatomical structures and lesions SAT and BiomedParse can segment on different human body regions in radiology images, and on different imaging modalities. &#8220;Others&#8221; denotes non-radiology modalities. AG adrenal gland, HV heart ventricle, IVC inferior vena cava, LHA left heart atrium, UB urinary bladder.</p></caption><graphic id="d33e747" position="float" orientation="portrait" xlink:href="41746_2025_1964_Fig5_HTML.jpg"/></fig></p><p id="Par33">Furthermore, as illustrated in Fig. <xref rid="Fig5" ref-type="fig">5</xref>b, BiomedParse is primarily designed as a segmentation tool for 2D biomedical images. In contrast, SAT, developed as a large-vocabulary segmentation model specifically for 3D radiology images, demonstrates significantly broader applicability and superior performance on 3D radiology images.</p></sec><sec id="Sec6"><title>Evaluation on external datasets</title><p id="Par34">Here, we aim to evaluate the generalization performance of segmentation models on images from different medical centers. As generalist models, SAT, MedSAM, and BiomedParse are directly evaluated on two unseen datasets. For specialist models, considering their customized configurations on each dataset, we systematically evaluate 21 out of 72 specialist models on target datasets for shared categories. For example, to evaluate the generalization performance on &#8216;lung&#8217; in AbdomenAtlas, we use specialist models trained on CT-ORG and LUNA16, as they all involve this class, and then average the results. The details of the overlapped label spaces are shown in Supplementary Fig. <xref rid="MOESM1" ref-type="media">5</xref>. To maintain performance for specialist models, the pre-processing of target datasets is kept the same as the source dataset in evaluation.</p><p id="Par35">We report DSC and NSD results in Fig. <xref rid="Fig6" ref-type="fig">6</xref> and Supplementary Table <xref rid="MOESM1" ref-type="media">15</xref>, with the following observations: (i) For specialist models, U-Mamba achieves more competitive results than nnU-Nets on both DSC and NSD scores, while SwinUNETR remains the worst; (ii) For generalist models for 2D images, MedSAM (Tight) consistently outperforms BiomedParse (Oracle) on all categories, implying that accurate box prompts provide strong priors when extending to out-of-domain images; (iii) SAT-Pro achieves the best performance on average over all categories, exceeding the second-best candidate MedSAM by 2.9 on DSC (paired <italic toggle="yes">t</italic>-test <italic toggle="yes">p</italic>&#8201;&lt;&#8201;7&#8201;&#215;&#8201;10<sup>&#8722;4</sup>) and 5.52 on NSD (<italic toggle="yes">p</italic>&#8201;&lt;&#8201;9&#8201;&#215;&#8201;10<sup>&#8722;6</sup>). Meanwhile, SAT-Pro consistently outperforms the specialist models on all categories in terms of NSD score and on 17 out of 19 categories in terms of DSC score.<fig id="Fig6" position="float" orientation="portrait"><label>Fig. 6</label><caption><title>External experiments between SAT, specialist models, MedSAM, and BiomedParse on AbdomenAtlas and LiQA.</title><p>Both DSC and NSD results are presented for each class in each dataset. We enlarge the size of SAT-Pro in each sub-figure for distinction, and annotate the ranking of SAT on each class. AG adrenal gland, PV &amp; SV, portal vein and splenic vein, UB urinary bladder.</p></caption><graphic id="d33e789" position="float" orientation="portrait" xlink:href="41746_2025_1964_Fig6_HTML.jpg"/></fig></p></sec><sec id="Sec7"><title>Ablation study on text encoder</title><p id="Par36">As will be illustrated in Section &#8220;Multimodal knowledge injection&#8221;, to build a large-vocabulary segmentation model driven by text prompts, we inject domain knowledge into the text encoder to provide precise prompts for the target of interest, i.e., the encoding of terminology. In this section, we conduct experiments and discuss the effect of domain knowledge. To save computational cost, the experiment have been conducted on <bold>SAT-DS-Nano</bold> dataset.</p><p id="Par37">Specifically, we train four <bold>SAT-Nano</bold> variants with different text encoders: (i) BERT-Base, a prevalent text encoder in natural language processing, but not specifically fine-tuned on medical corpora; (ii) the text encoder of CLIP, a state-of-the-art model pretrained on 400M image-text pairs and widely used in vision-language tasks; (iii) the state-of-the-art text encoder for medical retrieval tasks, e.g., MedCPT; (iv) the text encoder pre-trained on our multimodal medical knowledge graph, as illustrated in Section &#8220;Multimodal knowledge injection&#8221;. For all variants, we use U-Net as the visual backbone and denote them as <bold>U-Net-BB,</bold>
<bold>U-Net-CLIP,</bold>
<bold>U-Net-CPT</bold>, and <bold>U-Net-Ours</bold>.</p><p id="Par38">As shown in Fig. <xref rid="Fig7" ref-type="fig">7</xref> and Supplementary Table <xref rid="MOESM1" ref-type="media">17</xref>, the performance of U-Net-BB, U-Net-CLIP, and U-Net-CPT is close. Overall, U-Net-BB performs the worst, while U-Net-CPT slightly exceeds others on DSC (+0.1) and U-Net-CLIP slightly exceeds others on NSD (+0.29) scores averaged over all classes. By contrast, U-Net-Ours surpasses all other variants consistently across all regions, with notable margins on both DSC (+1.54) and NSD (+2.36) scores on average over all classes. This demonstrates the effectiveness of our proposed multimodal knowledge injection.<fig id="Fig7" position="float" orientation="portrait"><label>Fig. 7</label><caption><title>Evaluations on SAT-Nano variants with different text encoders.</title><p>&#8220;All&#8221; denotes the average scores over all the classes (<italic toggle="yes">n</italic>&#8201;=&#8201;429), including lesion classes. <bold>a</bold> DSC comparison; <bold>b</bold> NSD comparison.</p></caption><graphic id="d33e839" position="float" orientation="portrait" xlink:href="41746_2025_1964_Fig7_HTML.jpg"/></fig></p><p id="Par39">We further investigate the effect on different classes. As illustrated in Fig. <xref rid="Fig8" ref-type="fig">8</xref>a, b, the 429 classes in <bold>SAT-DS-Nano</bold> typically follow a long-tail distribution. The 10 &#8220;head&#8221; classes account for 12.75% of the annotations in SAT-DS-Nano. In contrast, the 150 classes with minimum annotations account for only 3.25%, even though they comprise 34.97% of the 429 classes. We compare U-Net-Ours, U-Net-CPT, U-Net-CLIP, and U-Net-BB on the &#8220;head&#8221; classes, &#8220;tail&#8221; classes, and the rest (denoted as &#8220;middle&#8221; classes). In Fig. <xref rid="Fig8" ref-type="fig">8</xref>c, the performance of the model variants drops from head to tail classes, showing that the long-tailed distribution poses a significant challenge for medical segmentation. Using our proposed knowledge-enhanced text encoder, U-Net-Ours achieves the best performance across all three scenarios. On &#8220;head&#8221; classes, it outperforms the second-best variant by 0.71 on DSC and 2.44 on NSD. On &#8220;tail&#8221; classes, the improvement is even more pronounced. For more detailed results on each class and dataset, we refer the reader to Supplementary Tables <xref rid="MOESM1" ref-type="media">22</xref>, <xref rid="MOESM1" ref-type="media">23</xref>, <xref rid="MOESM1" ref-type="media">24</xref>, and <xref rid="MOESM1" ref-type="media">25</xref>.<fig id="Fig8" position="float" orientation="portrait"><label>Fig. 8</label><caption><title>The impact of domain knowledge on a long-tail distribution.</title><p><bold>a</bold> The annotation number of all the 429 classes in SAT-DS-Nano, sorted from highest to lowest. <bold>b</bold> The proportion of &#8220;head&#8221;, &#8220;middle&#8221; and &#8220;tail&#8221; in class number and annotation number. The DSC and NSD comparison on &#8220;head&#8221;, &#8220;middle&#8221; and &#8220;tail&#8221; classes.</p></caption><graphic id="d33e877" position="float" orientation="portrait" xlink:href="41746_2025_1964_Fig8_HTML.jpg"/></fig></p><p id="Par40">In addition to segmentation performance, we evaluate the text encoders on &#8220;concept-to-definition&#8221; retrieval using human anatomy knowledge. In total, we collect 6502 anatomy concept-definition pairs. We find that the Recall@1 (R1) for BERT-Base is only 0.08%, suggesting it can hardly understand these anatomy concepts and possesses almost no domain knowledge. The R1 is 4.13% for CLIP and 11.19% for MedCPT. Though this is a significant improvement over BERT-Base, they still struggle to distinguish these concepts. By contrast, our proposed text encoder achieves 99.18% R1, indicating that the knowledge is successfully injected into the text embedding for each anatomy concept.</p></sec><sec id="Sec8"><title>Qualitative results&#8212;SAT as an interface between language and segmentation</title><p id="Par41">Thanks to the text-driven features of SAT, it can be seamlessly applied as an interface between natural language and segmentation, i.e., acting as a high-performance and efficient agent for language models. Here, we demonstrate three potential applications in Fig. <xref rid="Fig9" ref-type="fig">9</xref>: (i) We demonstrate a scenario where GPT-4<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> analyzes and extracts the anatomical targets of interest from a real clinical report and prompts SAT to segment them on the clinical image. As can be seen in the upper row, the targets in reports can be well detected by the language model (GPT-4) and commendably segmented by SAT-Pro, which provides visual cues for the clinical report and enhances its interpretability; (ii) We show that SAT can help LLMs handle segmentation requests in free-form conversations with any users. The LLM can easily recognize these requests and leverage SAT to deliver precise segmentation results, which greatly extends the conversational interface. (iii) We explore more complicated situations, where SAT can ground the lesions based on comprehensive analysis of radiology images as well as contextual EHR data such as patient complaints, establishing a complete automated pipeline from diagnosis to segmentation.<fig id="Fig9" position="float" orientation="portrait"><label>Fig. 9</label><caption><title>SAT as agent for large language models.</title><p>Combining SAT-Pro and GPT4, we demonstrate three potential applications: providing visual clues for clinic reports, handling segmentation requests in free-form conversation, and an automated pipeline from diagnosis to segmentation. For each application, the specific prompt template in use is shown. The [TERMINOLOGY LIST] contains anatomical structures that SAT can segment, which can be customized based on different clinicians' requirements (e.g., in demo 3, we only provide lesion categories). While the other bolded components in the templates (e.g., [MODALITY]) are variable placeholders that need to be filled with case-specific information. We show one example case for each application in the leftmost column. Target names are extracted from GPT's text output by string parsing, and serve as the exact text prompts for SAT. We extract representative slices from the image volume for demonstration.</p></caption><graphic id="d33e899" position="float" orientation="portrait" xlink:href="41746_2025_1964_Fig9_HTML.jpg"/></fig></p></sec></sec><sec id="Sec9" sec-type="discussion"><title>Discussion</title><p id="Par42">Developing specialist models on individual tasks has been the dominant solution for medical image segmentation for years<sup><xref ref-type="bibr" rid="CR15">15</xref>,<xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR27">27</xref>&#8211;<xref ref-type="bibr" rid="CR37">37</xref></sup>. In this paper, we aim to build a large-vocabulary, effective, and flexible medical segmentation foundation model by training on an unprecedented dataset collection and driven by knowledge-enhanced text prompts. The significance of the proposed SAT is demonstrated through multiple dimensions.</p><p id="Par43">First, our work represents an important step towards a universal segmentation model in medical scenarios. Despite the diverse images and segmentation targets from different clinical scenarios, SAT can flexibly handle them within a single generalist model, effectively replacing the need for dozens of specialist models. Through comprehensive internal evaluation, SAT-Pro has demonstrated competitive results against the ensemble of 72 specialist models, achieving comparable performance to nnU-Net and U-Mamba, and superior performance to SwinUNETR. Remarkably, SAT-Pro achieves this with a model size reduced to 20% or less of the ensemble, greatly improving efficiency. When evaluating on external multi-center datasets, SAT-Pro exhibits advanced generalization ability compared to all specialist models, highlighting its excellent cross-center transferability. With dataset-specific fine-tuning, SAT-Ft can further improve the performance, thus balancing the clinical requirements between generalist solutions and specialist models.</p><p id="Par44">Second, as an automatic method prompted by text, SAT offers an alternative approach to recent works, such as interactive segmentation foundation models<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR38">38</xref></sup>. Through both qualitative and quantitative comparisons, SAT demonstrates enhanced segmentation accuracy and robustness, particularly on targets with irregular shapes. Unlike interactive methods that rely on spatial prompts and may suffer from inaccurate prompts, leading to performance fluctuations, SAT can effectively automate segmentation on 3D images with text prompts, significantly reducing user inference time and associated costs. In addition, compared to our concurrent work on text-prompted 2D segmentation foundation model, namely BiomedParse<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, SAT demonstrates significantly broader applicability in 3D radiology images and consistently outperforms it in both in-domain and out-of-domain scenarios.</p><p id="Par45">Third, our work implies that scaling laws&#8212;observed in large language models&#8212;also apply to large-vocabulary medical segmentation. In this work, we build SAT-Nano (110M) and SAT-Pro (447M). In both region-wise and class-wise evaluations, SAT-Pro shows a clear performance boost over SAT-Nano, outperforming the latter on most regions and classes. These findings indicate a promising way to continuously improve the performance of segmentation foundation models.</p><p id="Par46">Fourth, we construct the first multimodal knowledge graph on human anatomy and demonstrate that knowledge injection can significantly improve segmentation performance, particularly for &#8216;tail&#8217; classes. As the scope of medical segmentation expands to include an increasing number of targets, the long-tail problem will become more pronounced, underscoring the critical importance of knowledge enhancement in addressing this challenge.</p><p id="Par47">Lastly, SAT can be used as an agent to bridge language and segmentation. In Section &#8220;Qualitative results&#8212;SAT as an interface between language and segmentation&#8221;, we show that SAT can be applied to segment targets based on the output from language models and support visual grounding across various clinical scenarios. This highlights the potential of SAT as a high-performance, efficient, and out-of-the-box tool agent, seamlessly collaborating with ever-evolving large language models. In addition, SAT has recently been applied to provide comprehensive grounding annotations for medical visual-language datasets in a scalable manner<sup><xref ref-type="bibr" rid="CR39">39</xref>,<xref ref-type="bibr" rid="CR40">40</xref></sup>.</p><p id="Par48">As one of the first exploratory works in this field, several limitations remain to be addressed in our work, and we propose the following future works: (i) The performance of SAT-Pro still lags behind some specialist models, e.g., nnU-Nets, in some region. Further scaling up the model can be a promising direction; (ii) Although SAT is capable of segmenting 497 types of targets on medical images, its generalization ability to unseen categories (including unseen lesions/pathologies) remains limited. Inspired by recent advances in language-grounded segmentation for natural images and videos<sup><xref ref-type="bibr" rid="CR41">41</xref>&#8211;<xref ref-type="bibr" rid="CR44">44</xref></sup>, exploring open vocabulary segmentation in medical imaging represents a promising direction for future work; (iii) For practical deployment, while our current inference speed is suitable for clinical use (as shown in Supplementary Tables <xref rid="MOESM1" ref-type="media">1</xref> and <xref rid="MOESM1" ref-type="media">2</xref>), further optimization for standard clinical hardware remains important; We will explore approaches for more efficient deployment, such as our subsequent work on knowledge distillation<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>; (iv) Although SAT-DS includes datasets from multiple countries/regions (United States, Europe, China, Africa, etc.), distribution biases still persist. Many regions remain uncovered, and the dataset is heavily skewed toward adult populations with limited pediatric/fetal data (e.g., FETA2022). These demographic imbalances may affect model generalization across different populations and age groups, necessitating bias mitigation strategies in future work.</p></sec><sec id="Sec10"><title>Methods</title><p id="Par49">In this section, we first describe the two types of data collected to build SAT: multimodal domain knowledge (Section &#8220;Domain knowledge&#8221;) and medical segmentation data (Section &#8220;Segmentation dataset&#8221;). Based on them, we detail the development of SAT, starting with the task formulation (Section &#8220;Large-vocabulary segmentation prompted by text&#8221;), then the multimodal knowledge injection (Section &#8220;Multimodal knowledge injection&#8221;) and segmentation training (Section &#8220;Segmentation training&#8221;). Finally, we present the evaluation settings, including the datasets (Section &#8220;Evaluation datasets&#8221;), baselines (Section &#8220;Baselines&#8221;), protocols (Section &#8220;Evaluation protocols&#8221;), and metrics (Section &#8220;Evaluation metrics&#8221;).</p><sec id="Sec11"><title>Domain knowledge</title><p id="Par50">To acquire textual knowledge, we mainly exploit two sources of domain knowledge: the Unified Medical Language System (UMLS)<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>, a comprehensive medical knowledge graph consisting of concept definitions and their interrelations; search engines, which are prompted to organize knowledge into a graph of the same format, specifically refined for the human anatomy corpus. Regarding visual knowledge, we have compiled 72 medical segmentation datasets, creating an atlas that covers over 497 anatomical structures of the human body. Examples from these sources are illustrated in Fig. <xref rid="Fig10" ref-type="fig">10</xref>a, b. In the following, we detail each knowledge source in sequence.<fig id="Fig10" position="float" orientation="portrait"><label>Fig. 10</label><caption><title>The medical knowledge used in the visual-language pretraining.</title><p><bold>a</bold> Segmentation datasets provide an atlas for extensive anatomy concepts. In this example, atlas segmentation is marked with different colors. <bold>b</bold> Knowledge generated from UMLS and search engines encompasses a broad array of concept-definition pairs and extensive relationships. <bold>c</bold> By integrating all collected knowledge sources, a medical knowledge tree is constructed. All definitions are partially displayed for conciseness. Definition and relation denoted with * are derived from the search engine, otherwise from UMLS.</p></caption><graphic id="d33e998" position="float" orientation="portrait" xlink:href="41746_2025_1964_Fig10_HTML.jpg"/></fig></p><p id="Par51">Unified Medical Language System (UMLS)<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> is a knowledge source of biomedical vocabulary developed by the US National Library of Medicine<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. It integrates a wide range of concepts from more than 60 families of biomedical vocabularies, each equipped with a concept unique identifier and definition. It also contains the relations among these concepts. Following<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>, we extract 229,435 biomedical terminologies and definitions, as well as 1,048,575 relationship triplets, composing a knowledge graph of these terminologies.</p><p id="Par52">Although UMLS is widely acknowledged and adopted as a general medical knowledge corpus<sup><xref ref-type="bibr" rid="CR48">48</xref>&#8211;<xref ref-type="bibr" rid="CR51">51</xref></sup>, it lacks a fine-grained description of anatomy concepts critical for segmentation tasks. For example, for &#8220;liver&#8221;, the definition is &#8220;A large lobed glandular organ in the abdomen of vertebrates that is responsible for detoxification, metabolism, synthesis, and storage of various substances&#8221;, which erases the morphological features and focuses on functionality. Meanwhile, more comprehensive knowledge on human anatomy may be scattered across various authoritative websites online, e.g., Wikipedia, ScienceDirect, etc. To harvest such knowledge, we select 6502 anatomy concepts and prompt a search engine to retrieve and summarize definitions for them. We use the following prompt template:</p><p id="Par53">
<disp-quote><p id="Par54">Definition of xxx. Include the location, shape, appearance, structure, and spatial relations to other anatomical structures. No need to include functionality. End with &#8216;END&#8217;.</p></disp-quote>
</p><p id="Par55">For illustration, the search engine referred to authority websites including Columbia Surgery, Hopkins Medicine, and summarized the definition for &#8220;liver&#8221; as: &#8220;A large organ found in the upper right quadrant of the abdomen, it stands as the largest gland within the human body, with a weight of about 1.5&#8201;kilograms. This structure exhibits a reddish-brown hue and is cone or wedge-shaped &#8230;&#8201;&#8230;&#8221;. While constructing the knowledge graph, we also adopt GPT4<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> to extract 38,344 relations between anatomical structures in the generated information-dense definitions with the following prompt:</p><p id="Par56">
<disp-quote><p id="Par57">This is the description of xxx. Please help me find its relations with other anatomical structures in radiological images. Summarize them with the template: Relation: xxx (relational preposition), Anatomical structure: xxx (name of another anatomical structure). For example, &#8220;Relation: situated below, Anatomical structure: xxx&#8221;, &#8220;Relation: connected to (via xxx), Anatomical structure: xxx&#8221; &#8230;&#8201;&#8230;</p></disp-quote>
</p><p id="Par58">Segmentation datasets naturally provide visual features for anatomy concepts corresponding to or complementary to the textual description, such as the texture, spatial location, shape, and so on. Details on our collected segmentation datasets are described in Section &#8220;Segmentation dataset&#8221;. Here, we use them as a large-scale and diverse visual atlas library, and link the visual regions to corresponding concepts in the textual knowledge graph, bridging the knowledge between visual and language modality.</p><p id="Par59">In summary, by mixing these data, we construct a multimodal medical knowledge tree. As demonstrated in Fig. <xref rid="Fig10" ref-type="fig">10</xref>c, the concepts (including both anatomical structures and lesions) are linked via the relations and further extended with their definitions, containing their characteristics. Additionally, some are further mapped to the visual atlas, demonstrating their visual features that may hardly be described purely by text. More examples on the curated knowledge dataset are shown in Supplementary Tables <xref rid="MOESM1" ref-type="media">34</xref>, <xref rid="MOESM1" ref-type="media">35</xref>, and <xref rid="MOESM1" ref-type="media">36</xref>.</p></sec><sec id="Sec12"><title>Segmentation dataset</title><p id="Par60">To train our segmentation model with the ability to handle segmentation tasks of different targets, across various modalities and anatomical regions, we collect and integrate 72 diverse publicly available medical segmentation datasets, totaling 22,186 scans including CT, MRI, and PET, and 302,033 segmentation annotations spanning 8 different regions of the human body: brain, head and neck, upper limb, thorax, abdomen, pelvis, and lower limb. The dataset is termed as <bold>SAT-DS</bold>. More details are present in Supplementary Tables <xref rid="MOESM1" ref-type="media">26</xref> and <xref rid="MOESM1" ref-type="media">27</xref>. <italic toggle="yes">Note that</italic>, some public datasets are not mutually exclusive, e.g., KiTS23 and KiTS21<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>, we thus only collect the latest version, to avoid redundancy and potential data leakage in the train-test split.</p><p id="Par61">Before mixing these datasets for training, two challenges remain: (i) the anatomical targets from each dataset must be integrated into a unified annotation system. The clinic demands beneath each dataset collection might be different, resulting in different annotation standards and granularity. Meanwhile, since most datasets are annotated for training specialist models like nnU-Net<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, precise and consistent terminology or expression for anatomical targets is often ignored. Therefore, a unified label system is demanded to avoid potential contradictions when training on mixed datasets. (ii) Some critical image statistics, such as intensity distribution and voxel spacing vary from dataset to dataset, hindering the model from learning consistent image representations across datasets. In the following, we present details for dataset integration and pre-processing, and how we address the abovementioned challenges.</p><p id="Par62">To ensure a unified annotation system, we take three procedures while integrating different datasets: (i) we manually check each anatomical target in each dataset and assign a medical term to it, which is guaranteed to be precise and unambiguous across datasets. For instance, the targets that require distinction between orientations, such as the left lung and right lung, are always identified according to the left and right of the human body. And the same anatomical targets from different datasets are named consistently. For example, the <italic toggle="yes">i</italic>-th lumbar vertebrae in both TotalSegmentator<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> and MRSpineSeg<sup><xref ref-type="bibr" rid="CR11">11</xref></sup> are named with the format &#8220;lumbar vertebrae <italic toggle="yes">i</italic> (Li)&#8221;; (ii) we adjust the annotations to minimize contradictions between overlapped classes. For example, considering that many organ segmentation datasets do not exclude lesions within organs, e.g., AbdomenCT-1K and CT-ORG, we merged the lesion annotations with the corresponding infected organ annotations in other datasets to maintain consistency. (iii) the same anatomy may have been annotated with different hierarchies in different datasets. In such cases, we manually merge the fine-grained classes to generate additional classes as a complement to close the gap between datasets. For example, sub-regions of the liver in Couinaud Liver<sup><xref ref-type="bibr" rid="CR53">53</xref></sup> are merged and added as a new class &#8220;liver&#8221;. As we will keep collecting datasets to scale up SAT-DS, such a label system will be maintained and updated continuously.</p><p id="Par63">As properties of each dataset may greatly impact the training of the segmentation network<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, such as intensity distribution and voxel spacing, we deliberately apply some normalization procedures to all the datasets to ensure uniformity and compatibility between them. Firstly, all the images are reoriented to specific axcodes, respaced to a voxel size of 1&#8201;&#215;&#8201;1&#8201;&#215; 3&#8201;mm<sup>2</sup> and cropped to the non-zero region. Secondly, we apply different intensity normalization strategies to CT, MRI, and PET images. Specifically, for CT images, intensity values are truncated to [&#8722;500, 1000] and applied z-score normalization. For MRI and PET images, intensity values are clipped by 0.5% and 99.5% of the image, and then z-score normalized. During training, we randomly crop the image patch with a fixed size of 288&#8201;&#215;&#8201;288&#8201;&#215;&#8201;96. Random zoom-in, zoom-out, and intensity scaling are applied for data augmentation.</p><p id="Par64">After integrating datasets, we derive a segmentation data collection that covers 497 segmentation classes, far outpacing each single dataset in both diversity and scale. Specifically, the data collection is more than <italic toggle="yes">fourth times</italic> the size of the largest dataset (BraTS2023-GLI) in terms of volume number, and nearly <italic toggle="yes">triple</italic> the most comprehensive dataset (DAP Atlas) in terms of the class number. We divide the human body into eight regions and classify each class into them manually. Figure <xref rid="Fig11" ref-type="fig">11</xref>b, c shows the distribution of classes and annotations across different human body regions. We further show the distribution of some example classes in each region in Fig. <xref rid="Fig11" ref-type="fig">11</xref>a. The extensive range of categories and regions lays the foundation for the SAT&#8217;s wide application scenarios.<fig id="Fig11" position="float" orientation="portrait"><label>Fig. 11</label><caption><title>Statistics of SAT-DS across different anatomical regions.</title><p><bold>a</bold> Annotation num of some representative classes in each anatomical region; <bold>b</bold> Number of classes in each anatomical region; <bold>c</bold> Number of annotations in each anatomical region. LC/HC laryngeal/hypopharyngeal cancer, WHM white matter hyperintensities, PV &amp; SV portal vein and splenic vein, IVC inferior vena cava, Thoracic V thoracic vertebrae, Cervical V cervical vertebrae, Lumbar V lumbar vertebrae.</p></caption><graphic id="d33e1140" position="float" orientation="portrait" xlink:href="41746_2025_1964_Fig11_HTML.jpg"/></fig></p><p id="Par65">In the process of building SAT-DS, we merge a wide range of segmentation tasks and establish a unified label system by using natural language/text. Generally speaking, there are three advantages to doing this: (i) natural language is powerful and discriminative, which enables better differentiation of the medical terminologies in the language embedding space; (ii) as shown in previous work<sup><xref ref-type="bibr" rid="CR48">48</xref>&#8211;<xref ref-type="bibr" rid="CR51">51</xref></sup>, knowledge-enhanced representation learning for the text encoder demonstrates promising performance, allowing to learn the implicit or explicit relationships between these segmentation targets. For example, segmenting a specific lobe of the liver requires the exact segmentation of the liver as an organ in the abdominal cavity, and shall be facilitated by referring to other parts of the liver. Therefore, establishing such connections via systematic medical knowledge shall be beneficial. (iii) Text prompts can be given automatically without any human intervention, for instance, from large language models. This would pave the way for building a segmentation model that can be flexibly integrated into foundation models for generalist medical artificial intelligence, as a powerful grounding tool.</p></sec><sec id="Sec13"><title>Large-vocabulary segmentation prompted by text</title><p id="Par66">Assuming we have a segmentation dataset collection, i.e., <inline-formula id="IEq1"><alternatives><tex-math id="d33e1154">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{D}}=\{({x}_{1},{y}_{1};{T}_{1}),\ldots ,({x}_{K},{y}_{K};{T}_{K})\}$$\end{document}</tex-math><mml:math id="d33e1159"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">D</mml:mi><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41746_2025_1964_Article_IEq1.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq2"><alternatives><tex-math id="d33e1215">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x}_{i}\in {{\mathbb{R}}}^{H\times W\times D\times C}$$\end{document}</tex-math><mml:math id="d33e1220"><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41746_2025_1964_Article_IEq2.gif"/></alternatives></inline-formula> denotes the image scan, <inline-formula id="IEq3"><alternatives><tex-math id="d33e1242">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y}_{i}\in {{\mathbb{R}}}^{H\times W\times D\times M}$$\end{document}</tex-math><mml:math id="d33e1247"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41746_2025_1964_Article_IEq3.gif"/></alternatives></inline-formula> is the binary segmentation annotations of the anatomical targets in the image and <italic toggle="yes">T</italic><sub><italic toggle="yes">i</italic></sub>&#8201;=&#8201;{<italic toggle="yes">t</italic><sub>1</sub>, <italic toggle="yes">t</italic><sub>2</sub>, &#8230;, <italic toggle="yes">t</italic><sub><italic toggle="yes">M</italic></sub>} denotes the corresponding medical terminology set, the segmentation task can be formulated as:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e1290">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{y}}_{i}={\Phi }_{{\rm{SAT}}}({\Phi }_{{\rm{visual}}}({x}_{i}),{\Phi }_{{\rm{text}}}({T}_{i})),$$\end{document}</tex-math><mml:math id="d33e1296"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#934;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">SAT</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#934;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">visual</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#934;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">text</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>T</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic position="anchor" orientation="portrait" xlink:href="41746_2025_1964_Article_Equ1.gif"/></alternatives></disp-formula>where <italic toggle="yes">&#934;</italic><sub>visual</sub> is a visual encoder, <italic toggle="yes">&#934;</italic><sub>text</sub> is a text encoder, <italic toggle="yes">&#934;</italic><sub>SAT</sub> is a large-vocabulary segmentation foundation model. Ideally, <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub> can be an image scan from any modality and anatomical region, and <italic toggle="yes">T</italic><sub><italic toggle="yes">i</italic></sub> can contain an arbitrary number of text-based medical terminologies of interest.</p><p id="Par67">To build such a model, we consider two main stages, namely, multimodal knowledge injection and segmentation training. In the following, we firstly show how to structure multimodal medical knowledge and inject it into a text encoder (Section &#8220;Multimodal knowledge injection&#8221;). Then, we employ the text encoder to guide our segmentation model training on SAT-DS dataset (Section &#8220;Segmentation training&#8221;). In addition, we provide more details about the model architecture and training strategies in the &#8220;Technical details&#8221; Section in the Supplementary.</p></sec><sec id="Sec14"><title>Multimodal knowledge injection</title><p id="Par68">Here, we aim to inject rich multimodal medical knowledge into the visual and text encoders. The section starts from the procedure for structuring the multimodal medical knowledge data and further presents details to use them for visual-language pre-training.</p><p id="Par69">As shown in Fig. <xref rid="Fig12" ref-type="fig">12</xref>a, the data from UMLS, search engine, and segmentation datasets can be aggregated into two formats:<list list-type="bullet"><list-item><p id="Par70"><bold>Textual medical concept pair</bold>. For text-only knowledge, each concept <italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub> is associated with a definition <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>, constructing pairs of text (<italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub>; <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>). We also derive a knowledge graph that connects the medical concepts through abundant triplet relationships (<italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">r</italic><sub><italic toggle="yes">i</italic><italic toggle="yes">j</italic></sub>, <italic toggle="yes">t</italic><sub><italic toggle="yes">j</italic></sub>). This graph can be alternatively seen as a specialized text pair, (<italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub>&#8201;+&#8201;<italic toggle="yes">r</italic><sub><italic toggle="yes">i</italic><italic toggle="yes">j</italic></sub>; <italic toggle="yes">t</italic><sub><italic toggle="yes">j</italic></sub>) or (<italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub>; <italic toggle="yes">r</italic><sub><italic toggle="yes">i</italic><italic toggle="yes">j</italic></sub> +&#8201;<italic toggle="yes">t</italic><sub><italic toggle="yes">j</italic></sub>), where &#8220;+&#8221; refers to string concatenation. In this way, we can thus unify the two kinds of textual knowledge.</p></list-item><list-item><p id="Par71"><bold>Visual medical concept pair</bold>. To align with the segmentation task, we gather pairs consisting of a concept (can be either an anatomical structure or lesion) and its image atlas. Note that, multiple pairs could be extracted from a single image. These pairs share a similar format to the segmentation data, denoted as (<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>; <italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub>), where <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub> and <italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub> are consistent with their definition in Section 4.3 and <inline-formula id="IEq4"><alternatives><tex-math id="d33e1521">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y}_{i}\in {{\mathbb{R}}}^{H\times W\times D\times 1}$$\end{document}</tex-math><mml:math id="d33e1526"><mml:mrow><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41746_2025_1964_Article_IEq4.gif"/></alternatives></inline-formula> is a binary segmentation mask for <italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub>.</p></list-item></list><fig id="Fig12" position="float" orientation="portrait"><label>Fig. 12</label><caption><title>Overview of SAT.</title><p><bold>a</bold> We inject multimodal medical knowledge into knowledge encoders via contrastive learning. The knowledge is in different formats: atlas segmentation, concepts(terminologies), definitions, and relationships between concepts. We devise visual and text encoders to embed them; <bold>b</bold> We train a segmentation network based on the text prompts from the pre-trained text encoder. It is capable of segmenting a wide range of targets for image scans from different modalities and anatomical regions.</p></caption><graphic id="d33e1567" position="float" orientation="portrait" xlink:href="41746_2025_1964_Fig12_HTML.jpg"/></fig></p><p id="Par72">In summary, all the knowledge can either be represented as pure text description, e.g., <italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub> + <italic toggle="yes">r</italic><sub><italic toggle="yes">i</italic><italic toggle="yes">j</italic></sub>, <italic toggle="yes">r</italic><sub><italic toggle="yes">i</italic><italic toggle="yes">j</italic></sub> + <italic toggle="yes">t</italic><sub><italic toggle="yes">j</italic></sub>, or atlas segmentation (<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>), and paired further.</p><p id="Par73">As shown in Fig. <xref rid="Fig12" ref-type="fig">12</xref>a, for pure text description, we encode them with a BERT<sup><xref ref-type="bibr" rid="CR54">54</xref></sup> pre-trained on PubMed abstracts<sup><xref ref-type="bibr" rid="CR55">55</xref></sup>:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="d33e1637">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z={\Phi }_{{\rm{text}}}({\bf{t}}),{\bf{t}}\in [{t}_{i},{p}_{i},{t}_{i}+{r}_{ij},{r}_{ij}+{t}_{j}],z\in {{\mathbb{R}}}^{d},$$\end{document}</tex-math><mml:math id="d33e1643"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#934;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">text</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi mathvariant="bold">t</mml:mi><mml:mo>&#8712;</mml:mo><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic position="anchor" orientation="portrait" xlink:href="41746_2025_1964_Article_Equ2.gif"/></alternatives></disp-formula>where <italic toggle="yes">d</italic> refers to the feature dimension. For visual concepts, we adopt the visual encoder <italic toggle="yes">&#934;</italic><sub>visual</sub>. Given the excellent robustness and performance of U-Net<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR18">18</xref></sup>, we apply a standard 3D U-Net encoder to extract multi-scale image embeddings:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="d33e1733">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${V}_{i}=\{{v}_{i1},{v}_{i2},\ldots ,{v}_{iS}\}={\Phi }_{{\rm{visual}}}({x}_{i}),{v}_{is}\in {{\mathbb{R}}}^{{H}_{s}\times {W}_{s}\times {D}_{s}\times {d}_{s}},$$\end{document}</tex-math><mml:math id="d33e1739"><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>S</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#934;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">visual</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:msub><mml:mrow><mml:mi>H</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>W</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>D</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mo>&#215;</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic position="anchor" orientation="portrait" xlink:href="41746_2025_1964_Article_Equ3.gif"/></alternatives></disp-formula>where <italic toggle="yes">V</italic><sub><italic toggle="yes">i</italic></sub> is the multi-scale feature maps from U-Net encoder layers, and <italic toggle="yes">H</italic><sub><italic toggle="yes">s</italic></sub>, <italic toggle="yes">W</italic><sub><italic toggle="yes">s</italic></sub>, <italic toggle="yes">D</italic><sub><italic toggle="yes">s</italic></sub>, <italic toggle="yes">d</italic><sub><italic toggle="yes">s</italic></sub> are the spatial resolutions and channel width at different layers. We further average ROI pooling on these feature maps, respectively, based on the down-sampled segmentation mask fitting resolutions at different layers. The concatenated pooled features can thus be treated as a representation of the anatomical target on this image, containing multi-scale visual clues for it.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="d33e1861">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$z={{\mathcal{F}}}_{{\rm{pooling}}}({\Phi }_{{\rm{visual}}}({x}_{i});{y}_{i}),z\in {{\mathbb{R}}}^{d}.$$\end{document}</tex-math><mml:math id="d33e1867"><mml:mrow><mml:mi>z</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">F</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">pooling</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#934;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">visual</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>;</mml:mo><mml:msub><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mi>z</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic position="anchor" orientation="portrait" xlink:href="41746_2025_1964_Article_Equ4.gif"/></alternatives></disp-formula></p><p id="Par74">We train the visual encoder and text encoder by maximizing the similarities between all positive knowledge pairs, linked by text prompts (medical terminology names), as shown in Fig. <xref rid="Fig12" ref-type="fig">12</xref>a. Specifically, given (<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>; <italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub>), (<italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub>; <italic toggle="yes">p</italic><sub><italic toggle="yes">i</italic></sub>), (<italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub>&#8201;+&#8201;<italic toggle="yes">r</italic><sub><italic toggle="yes">i</italic><italic toggle="yes">j</italic></sub>; <italic toggle="yes">t</italic><sub><italic toggle="yes">j</italic></sub>), (<italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub>; <italic toggle="yes">r</italic><sub><italic toggle="yes">i</italic><italic toggle="yes">j</italic></sub>&#8201;+&#8201;<italic toggle="yes">t</italic><sub><italic toggle="yes">j</italic></sub>), for simplicity, we denote all the encoded features as <italic toggle="yes">z</italic>, regardless of their modality format. For a batch of <italic toggle="yes">N</italic> pairs <inline-formula id="IEq5"><alternatives><tex-math id="d33e1999">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\{({z}_{1},z^{\prime} ),\ldots ({z}_{N},z^{\prime} )\}$$\end{document}</tex-math><mml:math id="d33e2004"><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mo>&#8230;</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:math><inline-graphic xlink:href="41746_2025_1964_Article_IEq5.gif"/></alternatives></inline-formula>, we have:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="d33e2043">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${{\mathcal{L}}}_{{\rm{knowledge}}}=-\frac{1}{N}\mathop{\sum }\limits_{i=1}^{N}(\log \frac{\exp ({z}_{i}\cdot z^{\prime} /\tau )}{\mathop{\sum }\nolimits_{k = 1}^{N}{{\mathbb{1}}}_{i\ne k}\exp ({z}_{i}\cdot z^{\prime} /\tau )}+\log \frac{\exp ({z}_{i}\cdot z^{\prime} /\tau )}{\mathop{\sum }\nolimits_{k = 1}^{N}{{\mathbb{1}}}_{i\ne k}\exp ({z}_{k}\cdot z^{\prime} /\tau )}),$$\end{document}</tex-math><mml:math id="d33e2049"><mml:mrow><mml:msub><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">knowledge</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mo>&#8722;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:munderover><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>log</mml:mi><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">1</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8800;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>log</mml:mi><mml:mfrac><mml:mrow><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>k</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>N</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi mathvariant="double-struck">1</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8800;</mml:mo><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mi>exp</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:msup><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup><mml:mo>/</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:mfrac></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic position="anchor" orientation="portrait" xlink:href="41746_2025_1964_Article_Equ5.gif"/></alternatives></disp-formula>with <italic toggle="yes">&#964;</italic>&#8201;=&#8201;0.07 as temperature.</p><p id="Par75">On formulation, this procedure resembles a typical contrastive learning pipeline<sup><xref ref-type="bibr" rid="CR56">56</xref>&#8211;<xref ref-type="bibr" rid="CR58">58</xref></sup>. However, different from previous work that directly contrasts the paired visual-language data, we aim for knowledge-enhanced representation learning. By maximizing the similarities between the constructed positive textual and visual feature pairs, we force the text encoders to construct neural representations for medical concepts based on domain knowledge from two aspects: (i) through the well-established knowledge graph in text form, the text encoder enables encoding relationships between concepts in the latent space; (ii) the model captures the characteristics of anatomical structures and lesions via both visual atlas segmentations and detailed definitions. Therefore, in contrast to the one-hot labeling that treats each anatomical target as being independent, such continuous neural representation shall provide more helpful guidance for the segmentation task.</p></sec><sec id="Sec15"><title>Segmentation Training</title><p id="Par76">With the pre-trained visual and text encoder, we now continue the procedure for building the segmentation model with text as prompts. Figure <xref rid="Fig12" ref-type="fig">12</xref>b demonstrates the overall framework. Specifically, apart from the pre-trained visual and text encoders, the segmentation model consists of three more components: a visual decoder <italic toggle="yes">&#934;</italic><sub>dec</sub>, a query decoder <italic toggle="yes">&#934;</italic><sub>query</sub>, and a mask generator. Although a sample in the segmentation dataset collection (<italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>, <italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>; <italic toggle="yes">T</italic><sub><italic toggle="yes">i</italic></sub>) may contain multiple annotations, <italic toggle="yes">i.e</italic>., <italic toggle="yes">T</italic><sub><italic toggle="yes">i</italic></sub>&#8201;=&#8201;{<italic toggle="yes">t</italic><sub>1</sub>, <italic toggle="yes">t</italic><sub>2</sub>, &#8230;, <italic toggle="yes">t</italic><sub><italic toggle="yes">M</italic></sub>}, for simplicity, we first describe the segmentation procedure for one target <italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub> in the following.</p><p id="Par77">Given an anatomical terminology <italic toggle="yes">t</italic><sub><italic toggle="yes">i</italic></sub>, we employ the pre-trained text encoder to generate its neural embedding, which serves as the text prompt for segmentation:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="d33e2280">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${z}_{i}={\Phi }_{{\rm{text}}}({t}_{i}),{z}_{i}\in {{\mathbb{R}}}^{d}.$$\end{document}</tex-math><mml:math id="d33e2286"><mml:mrow><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#934;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">text</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic position="anchor" orientation="portrait" xlink:href="41746_2025_1964_Article_Equ6.gif"/></alternatives></disp-formula>Note that, after pre-training the text encoder with domain knowledge injection, <italic toggle="yes">z</italic><sub><italic toggle="yes">i</italic></sub> should contain both the textual background information and visual information from atlas samples.</p><p id="Par78">For image scan <italic toggle="yes">y</italic><sub><italic toggle="yes">i</italic></sub>, we first adopt the pre-trained visual encoder to derive the multi-scale image embeddings <italic toggle="yes">V</italic><sub><italic toggle="yes">i</italic></sub>, as explained in Equa. (<xref rid="Equ3" ref-type="disp-formula">3</xref>), and continue training it. Then, in the visual decoder, the feature maps from the encoder are gradually upsampled with skip connections, effectively following the U-Net architecture<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR18">18</xref></sup>, ending up with per-pixel dense features:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="d33e2357">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${u}_{i}={\Phi }_{{\rm{dec}}}({V}_{i}),{u}_{i}\in {{\mathbb{R}}}^{H\times W\times D\times d^{\prime} },$$\end{document}</tex-math><mml:math id="d33e2363"><mml:mrow><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#934;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">dec</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic position="anchor" orientation="portrait" xlink:href="41746_2025_1964_Article_Equ7.gif"/></alternatives></disp-formula>where <inline-formula id="IEq6"><alternatives><tex-math id="d33e2413">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$d^{\prime}$$\end{document}</tex-math><mml:math id="d33e2418"><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mo>&#8242;</mml:mo></mml:mrow></mml:msup></mml:math><inline-graphic xlink:href="41746_2025_1964_Article_IEq6.gif"/></alternatives></inline-formula> is the dimension for the per-pixel dense feature after recovering to the original resolution.</p><p id="Par79">Although a general representation of the anatomical target is derived from the pre-trained text encoder with a text prompt, visual variations may still exist from patient to patient, we thus insert a transformer-based query decoder to further enhance the text prompts with visual clues. In practice, it consists of 6 standard transformer decoders<sup><xref ref-type="bibr" rid="CR59">59</xref></sup> that treat text embedding as query, and the pooled multi-scale visual features from the U-Net encoder as key, values, formulated as:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="d33e2432">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${q}_{i}={\Phi }_{{\rm{query}}}({V}_{i},{z}_{i}),{q}_{i}\in {{\mathbb{R}}}^{d}.$$\end{document}</tex-math><mml:math id="d33e2438"><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi mathvariant="normal">&#934;</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">query</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>V</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>z</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>d</mml:mi></mml:mrow></mml:msup><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic position="anchor" orientation="portrait" xlink:href="41746_2025_1964_Article_Equ8.gif"/></alternatives></disp-formula>Where <italic toggle="yes">z</italic><sub><italic toggle="yes">i</italic></sub> is consistent with <italic toggle="yes">z</italic> in Eq. (<xref rid="Equ4" ref-type="disp-formula">4</xref>). Therefore <italic toggle="yes">q</italic><sub><italic toggle="yes">i</italic></sub> can be seen as an adapted representation of the anatomical target in a specific image scan <italic toggle="yes">x</italic><sub><italic toggle="yes">i</italic></sub>.</p><p id="Par80">Finally, by conducting a pixel-wise dot product between the representation of the anatomical target and the fine-grained per-pixel embedding, we can acquire a per-pixel prediction:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="d33e2511">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{y}}_{i}=\sigma (g({q}_{i})\cdot {u}_{i}),{\hat{y}}_{i}\in {{\mathbb{R}}}^{H\times W\times D},$$\end{document}</tex-math><mml:math id="d33e2517"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>g</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>q</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>&#8901;</mml:mo><mml:msub><mml:mrow><mml:mi>u</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi></mml:mrow></mml:msup><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic position="anchor" orientation="portrait" xlink:href="41746_2025_1964_Article_Equ9.gif"/></alternatives></disp-formula>where <italic toggle="yes">g</italic>( &#8901; ) is a feed-forward layer projecting <italic toggle="yes">q</italic><sub><italic toggle="yes">i</italic></sub> to a consistent dimension with the dense feature map <italic toggle="yes">u</italic><sub><italic toggle="yes">i</italic></sub>, and <italic toggle="yes">&#963;</italic>( &#8901; ) denotes the sigmoid function. Note that the whole forward procedure does not involve any operation between different text prompts. Therefore, for input with multiple text prompts or segmentation targets, <italic toggle="yes">i.e</italic>., <italic toggle="yes">T</italic><sub><italic toggle="yes">i</italic></sub>&#8201;=&#8201;{<italic toggle="yes">t</italic><sub>1</sub>, <italic toggle="yes">t</italic><sub>2</sub>, &#8230;, <italic toggle="yes">t</italic><sub><italic toggle="yes">M</italic></sub>}, the processes described in Eqs. (<xref rid="Equ6" ref-type="disp-formula">6</xref>), (<xref rid="Equ8" ref-type="disp-formula">8</xref>), and (<xref rid="Equ9" ref-type="disp-formula">9</xref>) will be executed for each target in parallel, and we could derive <inline-formula id="IEq7"><alternatives><tex-math id="d33e2627">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\hat{y}}_{i}\in {{\mathbb{R}}}^{H\times W\times D\times M}$$\end{document}</tex-math><mml:math id="d33e2632"><mml:mrow><mml:msub><mml:mrow><mml:mover accent="true"><mml:mrow><mml:mi>y</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="double-struck">R</mml:mi></mml:mrow><mml:mrow><mml:mi>H</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>W</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>D</mml:mi><mml:mo>&#215;</mml:mo><mml:mi>M</mml:mi></mml:mrow></mml:msup></mml:mrow></mml:math><inline-graphic xlink:href="41746_2025_1964_Article_IEq7.gif"/></alternatives></inline-formula>.</p><p id="Par81">Following<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, we adopt a loss function as the sum of binary cross-entropy loss and dice loss. For a sample with <italic toggle="yes">M</italic> classes and <italic toggle="yes">C</italic> voxels, we denote <italic toggle="yes">p</italic><sub>c,m</sub> and <italic toggle="yes">s</italic><sub>c,m</sub> as the prediction and ground truth for <italic toggle="yes">c</italic>-th pixel, respectively, on class <italic toggle="yes">m</italic>, the loss is:<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="d33e2685">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\mathcal{L}}=\underbrace{-\frac{1}{M}\mathop{\sum }\limits_{{\rm{m = 1}}}^{M}\frac{1}{C}\mathop{\sum }\limits_{{\rm{c = 1}}}^{C}{p}_{{\rm{c,m}}}\cdot \log {s}_{{\rm{c,m}}}}_{{\rm{Binary}}\,{\rm{Cross}}\,{\rm{Entropy}}\,{\rm{Loss}}}+(\underbrace{1-\frac{2\mathop{\sum }\nolimits_{{\rm{i = 1}}}^{M}\mathop{\sum }\nolimits_{{\rm{c = 1}}}^{C}{p}_{{\rm{c,m}}}\cdot {s}_{{\rm{c,m}}}}{\mathop{\sum }\nolimits_{{\rm{m = 1}}}^{M}\mathop{\sum }\nolimits_{{\rm{c = 1}}}^{C}{p}_{{\rm{c,m}}}^{2}+\mathop{\sum }\nolimits_{{\rm{m = 1}}}^{M}\mathop{\sum }\nolimits_{{\rm{c = 1}}}^{C}{s}_{{\rm{c,m}}}^{2}}}_{{\rm{Dice}}\,{\rm{Loss}}})$$\end{document}</tex-math><mml:math id="d33e2691"><mml:mrow><mml:mi class="MJX-tex-caligraphic" mathvariant="script">L</mml:mi><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:munder accentunder="true"><mml:mrow><mml:mo>&#8722;</mml:mo><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m=1</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:munderover><mml:mfrac><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:mfrac><mml:munderover accent="false" accentunder="false"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c=1</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:munderover><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c,m</mml:mi></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:mi>log</mml:mi><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c,m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mo stretchy="true">&#9183;</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Binary</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">Cross</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">Entropy</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">Loss</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:munder accentunder="true"><mml:mrow><mml:mn>1</mml:mn><mml:mo>&#8722;</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:msubsup><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">i=1</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c=1</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:msub><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c,m</mml:mi></mml:mrow></mml:msub><mml:mo>&#8901;</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c,m</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:msubsup><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m=1</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c=1</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>p</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c,m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup><mml:mo>+</mml:mo><mml:msubsup><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">m=1</mml:mi></mml:mrow><mml:mrow><mml:mi>M</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c=1</mml:mi></mml:mrow><mml:mrow><mml:mi>C</mml:mi></mml:mrow></mml:msubsup><mml:msubsup><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi mathvariant="normal">c,m</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msubsup></mml:mrow></mml:mfrac></mml:mrow><mml:mrow><mml:mo stretchy="true">&#9183;</mml:mo></mml:mrow></mml:munder></mml:mrow><mml:mrow><mml:mi mathvariant="normal">Dice</mml:mi><mml:mspace width="0.25em"/><mml:mi mathvariant="normal">Loss</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>)</mml:mo></mml:mrow></mml:mrow></mml:math><graphic position="anchor" orientation="portrait" xlink:href="41746_2025_1964_Article_Equ10.gif"/></alternatives></disp-formula></p></sec><sec id="Sec16"><title>Evaluation datasets</title><p id="Par82">To strike a balance between extensive experiments and computational costs, we utilize two collections of datasets in evaluation:<list list-type="bullet"><list-item><p id="Par83"><bold>SAT-DS</bold>. As described in Section &#8220;Segmentation dataset&#8221;, this contains all the 72 datasets, 497 classes from all human body regions, 22,186 image scans, and 302,033 segmentation annotations.</p></list-item><list-item><p id="Par84"><bold>SAT-DS-Nano</bold>. A subset of SAT-DS, including only 49 datasets, 13,303 images, and 151,461 annotations. Note that SAT-DS-Nano also covers 429 classes from all human body regions, adequate to evaluate the large-vocabulary segmentation task.</p></list-item></list>The detailed composition of SAT-DS and SAT-DS-Nano can be found in Supplementary Tables <xref rid="MOESM1" ref-type="media">32</xref> and <xref rid="MOESM1" ref-type="media">33</xref>. As there is no existing benchmark for evaluating the large-vocabulary segmentation foundation model, we randomly split each dataset into 80% for training and 20% for testing: (i) datasets may share the same images but with different classes. For example, Couinaud Liver provides fine-grained liver segmentation on a subset of MSD Hepatic Vessel. We carefully split the Couinaud Liver to make sure the test set will not be leaked in the train set of MSD Hepatic Vessel; (ii) scans of the same patient but different modalities are treated as different samples during training and evaluation. For example, MSD Prostate contains T2 and ADC scans of each patient. However, they share the same structure on the image. To avoid potential data leaking, we carefully split such datasets by patient ID. Note that when involving segmentation datasets in the visual-language pretraining, we only use the training data to avoid potential data leaking. For datasets involved in SAT-DS-Nano, we keep their splits the same as in SAT-DS. The download link for each dataset can be found in Section 6, and we have released our dataset processing code and train-test splits to the research community for reproduction and benchmarking.</p></sec><sec id="Sec17"><title>Baselines</title><p id="Par85">We take nnU-Net<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>, U-Mamba<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>, and SwinUNETR<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> as representative types of <italic toggle="yes">specialist model</italic> and strong baselines for comparison. For a comprehensive evaluation, we train one specialist model on each of the datasets. Note that, following<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>, we split Totalsegmentator into 6 subsets and treat them as different datasets. Similarly, datasets such as CHAOS with both MRI and CT images are treated as two different datasets. When training specialist models on each dataset, we adopt a multi-class segmentation setting and deliver the masks of all categories in this dataset at once. We derive the optimal network architecture and pre-processing pipeline with the default setting of each specialist model. We present the detailed network design of nnU-Nets in Supplementary Table <xref rid="MOESM1" ref-type="media">26</xref> and Table <xref rid="MOESM1" ref-type="media">27</xref> for a straightforward comparison. In summary, we train an ensemble of 72 models for each type of specialist model, that are customized on each dataset. We adopt the latest official implementation of nnU-Net v2 and U-Mamba in practice. The SwinUNETR is adopted to the same auto-configuration framework as U-Mamba.</p><p id="Par86">We take MedSAM<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> as a representative <italic toggle="yes">interactive segmentation model</italic> and competitive baseline. MedSAM finetunes SAM<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> on 86 segmentation datasets, and supports 2D medical image segmentation with box prompts. We follow the official implementation to process and infer image slice by slice, and calculate the metrics on the finally stacked 3D prediction. For each single target on a slice, to simulate box prompts towards it, we both take the minimum rectangle containing the ground truth segmentation (Tight), and follow the official data augmentation procedure, randomly shift each corner up to 8% of the whole image resolution (Loose). In addition, we consider directly using the tight box prompts as predictions (Oracle Box).</p><p id="Par87">We take BiomedParse<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>, a concurrent <italic toggle="yes">text-prompted segmentation model</italic> for 2D biomedical images, as a baseline. We follow the official implementation for data processing, inference, and post-filtering. Similar to MedSAM, we process and infer image slice by slice, and calculate the metrics on the finally stacked 3D prediction. As BiomedParse may fail to detect the target on the slice, we evaluate it under two settings: only prompt target present in the current slice (Oracle) and prompt all the targets available in the dataset and post-filter out potential false positive predictions by p-values.</p></sec><sec id="Sec18"><title>Evaluation protocols</title><p id="Par88">Given our goal is to develop a large-vocabulary medical segmentation foundation model, this provides opportunities to evaluate novel perspectives in addition to the traditional evaluation per dataset. Specifically, we conduct the internal evaluations from three dimensions:<list list-type="bullet"><list-item><p id="Par89"><bold>Class-wise evaluation</bold>. As SAT is capable of segmenting a wide range of anatomical targets across the human body, we merge the results from the same classes across datasets to indicate the performance on each anatomical target. Specifically, we follow macro-average method: for a class annotated in multiple datasets, we first calculate its average scores within each dataset, and then average them over all datasets. Note that the same anatomical structures or lesions from different modalities are treated as different classes in this work, e.g., liver in both CT and MRI images.</p></list-item><list-item><p id="Par90"><bold>Region-wise evaluation</bold>. In general, anatomical structures from the same human body region are closely connected and more likely to be involved in diagnosis within the same hospital department. Here, we consider the region-wise evaluation: based on class-wise evaluation, we merge results from all classes in the same body region, as to indicate the general performance in this region. For classes existing in multiple regions, we classify them into &#8220;Whole Body&#8221; category. In addition, we report results for lesions classes independently as a category &#8220;lesion&#8221;, instead of merging them into specific regions.</p></list-item><list-item><p id="Par91"><bold>Dataset-wise evaluation</bold>. Results of the classes within the same dataset are averaged to indicate the performance on this dataset. This is the same as the conventional evaluation protocol of specialist segmentation models trained on a single dataset.</p></list-item></list></p></sec><sec id="Sec19"><title>Evaluation metrics</title><p id="Par92">We quantitatively evaluate the segmentation performance from the perspective of region and boundary metrics<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>, e.g., DSC and NSD, respectively.</p><p id="Par93">DSC is a standard region-based metric for medical image segmentation evaluation. It measures the overlap between the model&#8217;s prediction <italic toggle="yes">P</italic> and ground truth <italic toggle="yes">G</italic>, formally defined as:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="d33e2969">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$DSC(P,G)=\frac{2| P\bigcap G| }{| P| +| G| }.$$\end{document}</tex-math><mml:math id="d33e2975"><mml:mrow><mml:mi>D</mml:mi><mml:mi>S</mml:mi><mml:mi>C</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn><mml:mo>&#8739;</mml:mo><mml:mi>P</mml:mi><mml:mo mathsize="big"> &#8898;</mml:mo><mml:mi>G</mml:mi><mml:mo>&#8739;</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8739;</mml:mo><mml:mi>P</mml:mi><mml:mo>&#8739;</mml:mo><mml:mo>+</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi>G</mml:mi><mml:mo>&#8739;</mml:mo></mml:mrow></mml:mfrac><mml:mo>.</mml:mo></mml:mrow></mml:math><graphic position="anchor" orientation="portrait" xlink:href="41746_2025_1964_Article_Equ11.gif"/></alternatives></disp-formula></p><p id="Par94">NSD<sup><xref ref-type="bibr" rid="CR61">61</xref></sup> is a boundary-based metric that measures the consistency at the boundary area of the model&#8217;s prediction <italic toggle="yes">P</italic> and ground truth <italic toggle="yes">G</italic>, which is defined as:<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="d33e3019">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$NSD(P,G)=\frac{| \partial P\bigcap {B}_{\partial G}| +| \partial G\bigcap {B}_{\partial P}| }{| \partial P| +| \partial G| },$$\end{document}</tex-math><mml:math id="d33e3025"><mml:mrow><mml:mi>N</mml:mi><mml:mi>S</mml:mi><mml:mi>D</mml:mi><mml:mrow><mml:mo>(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mi>G</mml:mi></mml:mrow><mml:mo>)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mo>&#8739;</mml:mo><mml:mi>&#8706;</mml:mi><mml:mi>P</mml:mi><mml:mo mathsize="big"> &#8898;</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>&#8706;</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>&#8739;</mml:mo><mml:mo>+</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi>&#8706;</mml:mi><mml:mi>G</mml:mi><mml:mo mathsize="big"> &#8898;</mml:mo><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>&#8706;</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>&#8739;</mml:mo></mml:mrow><mml:mrow><mml:mo>&#8739;</mml:mo><mml:mi>&#8706;</mml:mi><mml:mi>P</mml:mi><mml:mo>&#8739;</mml:mo><mml:mo>+</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi>&#8706;</mml:mi><mml:mi>G</mml:mi><mml:mo>&#8739;</mml:mo></mml:mrow></mml:mfrac><mml:mo>,</mml:mo></mml:mrow></mml:math><graphic position="anchor" orientation="portrait" xlink:href="41746_2025_1964_Article_Equ12.gif"/></alternatives></disp-formula>where <inline-formula id="IEq8"><alternatives><tex-math id="d33e3079">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${B}_{\partial P}=\{x\in {{\bf{R}}}^{3}| \exists \hat{x}\in \partial P,| | x-\hat{x}| | \le \tau \}$$\end{document}</tex-math><mml:math id="d33e3084"><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>&#8706;</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>&#8739;</mml:mo><mml:mo>&#8707;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mo>&#8712;</mml:mo><mml:mi>&#8706;</mml:mi><mml:mi>P</mml:mi><mml:mo>,</mml:mo><mml:mo>&#8739;</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mo>&#8739;</mml:mo><mml:mo>&#8739;</mml:mo><mml:mo>&#8804;</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41746_2025_1964_Article_IEq8.gif"/></alternatives></inline-formula> and <inline-formula id="IEq9"><alternatives><tex-math id="d33e3133">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${B}_{\partial G}=\{x\in {{\bf{R}}}^{3}| \exists \hat{x}\in \partial G,| | x-\hat{x}| | \le \tau \}$$\end{document}</tex-math><mml:math id="d33e3138"><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>&#8706;</mml:mi><mml:mi>G</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>x</mml:mi><mml:mo>&#8712;</mml:mo><mml:msup><mml:mrow><mml:mi mathvariant="bold">R</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>&#8739;</mml:mo><mml:mo>&#8707;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mo>&#8712;</mml:mo><mml:mi>&#8706;</mml:mi><mml:mi>G</mml:mi><mml:mo>,</mml:mo><mml:mo>&#8739;</mml:mo><mml:mo>&#8739;</mml:mo><mml:mi>x</mml:mi><mml:mo>&#8722;</mml:mo><mml:mover accent="true"><mml:mrow><mml:mi>x</mml:mi></mml:mrow><mml:mrow><mml:mo>^</mml:mo></mml:mrow></mml:mover><mml:mo>&#8739;</mml:mo><mml:mo>&#8739;</mml:mo><mml:mo>&#8804;</mml:mo><mml:mi>&#964;</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow></mml:math><inline-graphic xlink:href="41746_2025_1964_Article_IEq9.gif"/></alternatives></inline-formula> are the boundary areas of the model&#8217;s prediction and ground truth at a tolerance <italic toggle="yes">&#964;</italic>, respectively. We set <italic toggle="yes">&#964;</italic> as 1 in the experiments.</p></sec></sec><sec id="Sec20" sec-type="supplementary-material"><title>Supplementary information</title><p>
<supplementary-material content-type="local-data" id="MOESM1" position="float" orientation="portrait"><media xlink:href="41746_2025_1964_MOESM1_ESM.pdf" position="float" orientation="portrait"><caption><p>Supplementary information</p></caption></media></supplementary-material>
</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold> Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><sec><title>Supplementary information</title><p>The online version contains supplementary material available at 10.1038/s41746-025-01964-w.</p></sec><ack><title>Acknowledgements</title><p>This work is supported by the Science and Technology Commission of Shanghai Municipality (Nos. 22511106101, 18DZ2270700, and 21DZ1100100), 111 plan (No. BP0719010), State Key Laboratory of UHD Video and Audio Production and Presentation, and National Key R&amp;D Program of China (No. 2022ZD0160702).</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>All authors make contributions to the conception or design of the work. Specifically, Z.Z. contributed to the technical implementation. Z.Z. and Y.Z. (Yao) contributed to data collection and processing. Z.Z., Y.Z. (Yao) and X.Z. (Xiao) contributed to the baseline implementation. All authors contributed to the drafting and revising of the manuscript.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The access to each dataset can be found in Table <xref rid="Tab1" ref-type="table">1</xref>. The data process code to buildSAT-DS and our train-test splits for reproducibility and benchmarking are available at <ext-link ext-link-type="uri" xlink:href="https://github.com/zhaoziheng/SAT-DS">https://github.com/zhaoziheng/SAT-DS</ext-link>.<table-wrap id="Tab1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Download links of the 72 datasets in SAT-DS</p></caption><table frame="hsides" rules="groups"><thead><tr><th colspan="1" rowspan="1">Dataset</th><th colspan="1" rowspan="1">Download link</th></tr></thead><tbody><tr><td colspan="1" rowspan="1">AbdomenCT1K<sup><xref ref-type="bibr" rid="CR8">8</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://github.com/JunMa11/AbdomenCT-1K">https://github.com/JunMa11/AbdomenCT-1K</ext-link></td></tr><tr><td colspan="1" rowspan="1">ACDC<sup><xref ref-type="bibr" rid="CR62">62</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://humanheart-project.creatis.insa-lyon.fr/database/">https://humanheart-project.creatis.insa-lyon.fr/database/</ext-link></td></tr><tr><td colspan="1" rowspan="1">AMOS CT<sup><xref ref-type="bibr" rid="CR63">63</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/7262581">https://zenodo.org/records/7262581</ext-link></td></tr><tr><td colspan="1" rowspan="1">AMOS MRI<sup><xref ref-type="bibr" rid="CR63">63</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/records/7262581">https://zenodo.org/records/7262581</ext-link></td></tr><tr><td colspan="1" rowspan="1">ATLASR2<sup><xref ref-type="bibr" rid="CR64">64</xref></sup></td><td colspan="1" rowspan="1"/></tr><tr><td colspan="1" rowspan="1">ATLAS<sup><xref ref-type="bibr" rid="CR65">65</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://atlas-challenge.u-bourgogne.fr">https://atlas-challenge.u-bourgogne.fr</ext-link></td></tr><tr><td colspan="1" rowspan="1">autoPET<sup><xref ref-type="bibr" rid="CR66">66</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=93258287">https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=93258287</ext-link></td></tr><tr><td colspan="1" rowspan="1">Brain Atlas<sup><xref ref-type="bibr" rid="CR67">67</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="http://brain-development.org/">http://brain-development.org/</ext-link></td></tr><tr><td colspan="1" rowspan="1">BrainPTM<sup><xref ref-type="bibr" rid="CR68">68</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://brainptm-2021.grand-challenge.org/">https://brainptm-2021.grand-challenge.org/</ext-link></td></tr><tr><td colspan="1" rowspan="1">BraTS2023 GLI<sup><xref ref-type="bibr" rid="CR69">69</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn51514105">https://www.synapse.org/#!Synapse:syn51514105</ext-link></td></tr><tr><td colspan="1" rowspan="1">BraTS2023 MEN<sup><xref ref-type="bibr" rid="CR70">70</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn51514106">https://www.synapse.org/#!Synapse:syn51514106</ext-link></td></tr><tr><td colspan="1" rowspan="1">BraTS2023 MET<sup><xref ref-type="bibr" rid="CR71">71</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn51514107">https://www.synapse.org/#!Synapse:syn51514107</ext-link></td></tr><tr><td colspan="1" rowspan="1">BraTS2023 PED<sup><xref ref-type="bibr" rid="CR72">72</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn51514108">https://www.synapse.org/#!Synapse:syn51514108</ext-link></td></tr><tr><td colspan="1" rowspan="1">BraTS2023 SSA<sup><xref ref-type="bibr" rid="CR73">73</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn51514109">https://www.synapse.org/#!Synapse:syn51514109</ext-link></td></tr><tr><td colspan="1" rowspan="1">BTCV Abdomen<sup><xref ref-type="bibr" rid="CR74">74</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn3193805/wiki/217789">https://www.synapse.org/#!Synapse:syn3193805/wiki/217789</ext-link></td></tr><tr><td colspan="1" rowspan="1">BTCV Cervix<sup><xref ref-type="bibr" rid="CR74">74</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn3193805/wiki/217790">https://www.synapse.org/#!Synapse:syn3193805/wiki/217790</ext-link></td></tr><tr><td colspan="1" rowspan="1">CHAOS CT<sup><xref ref-type="bibr" rid="CR75">75</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://chaos.grand-challenge.org/">https://chaos.grand-challenge.org/</ext-link></td></tr><tr><td colspan="1" rowspan="1">CHAOS MRI<sup><xref ref-type="bibr" rid="CR75">75</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://chaos.grand-challenge.org/">https://chaos.grand-challenge.org/</ext-link></td></tr><tr><td colspan="1" rowspan="1">CMRxMotion<sup><xref ref-type="bibr" rid="CR76">76</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://www.synapse.org/#!Synapse:syn28503327/files/">https://www.synapse.org/#!Synapse:syn28503327/files/</ext-link></td></tr><tr><td colspan="1" rowspan="1">Couinaud<sup><xref ref-type="bibr" rid="CR53">53</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://github.com/GLCUnet/dataset">https://github.com/GLCUnet/dataset</ext-link></td></tr><tr><td colspan="1" rowspan="1">COVID-19 CT Seg<sup><xref ref-type="bibr" rid="CR13">13</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://github.com/JunMa11/COVID-19-CT-Seg-Benchmark">https://github.com/JunMa11/COVID-19-CT-Seg-Benchmark</ext-link></td></tr><tr><td colspan="1" rowspan="1">CrossMoDA2021<sup><xref ref-type="bibr" rid="CR77">77</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://crossmoda.grand-challenge.org/Data/">https://crossmoda.grand-challenge.org/Data/</ext-link></td></tr><tr><td colspan="1" rowspan="1">CT-ORG<sup><xref ref-type="bibr" rid="CR78">78</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=61080890">https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=61080890</ext-link></td></tr><tr><td colspan="1" rowspan="1">CTPelvic1K<sup><xref ref-type="bibr" rid="CR79">79</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/4588403#.YEyLq_0zaCo">https://zenodo.org/record/4588403#Y&#729;EyLq_0zaCo</ext-link></td></tr><tr><td colspan="1" rowspan="1">DAP Atlas<sup><xref ref-type="bibr" rid="CR80">80</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://github.com/alexanderjaus/AtlasDataset">https://github.com/alexanderjaus/AtlasDataset</ext-link></td></tr><tr><td colspan="1" rowspan="1">FeTA2022<sup><xref ref-type="bibr" rid="CR81">81</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://feta.grand-challenge.org/data-download/">https://feta.grand-challenge.org/data-download/</ext-link></td></tr><tr><td colspan="1" rowspan="1">FLARE22<sup><xref ref-type="bibr" rid="CR9">9</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://flare22.grand-challenge.org/">https://flare22.grand-challenge.org/</ext-link></td></tr><tr><td colspan="1" rowspan="1">FUMPE<sup><xref ref-type="bibr" rid="CR82">82</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/datasets/andrewmvd/pulmonary-embolism-in-ct-images">https://www.kaggle.com/datasets/andrewmvd/pulmonary-embolism-in-ct-images</ext-link></td></tr><tr><td colspan="1" rowspan="1">HAN Seg<sup><xref ref-type="bibr" rid="CR83">83</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/">https://zenodo.org/record/</ext-link></td></tr><tr><td colspan="1" rowspan="1">HECKTOR2022<sup><xref ref-type="bibr" rid="CR84">84</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://hecktor.grand-challenge.org/Data/">https://hecktor.grand-challenge.org/Data/</ext-link></td></tr><tr><td colspan="1" rowspan="1">INSTANCE<sup><xref ref-type="bibr" rid="CR85">85</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://instance.grand-challenge.org/Dataset/">https://instance.grand-challenge.org/Dataset/</ext-link></td></tr><tr><td colspan="1" rowspan="1">ISLES2022<sup><xref ref-type="bibr" rid="CR86">86</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="http://www.isles-challenge.org/">http://www.isles-challenge.org/</ext-link></td></tr><tr><td colspan="1" rowspan="1">KiPA22<sup><xref ref-type="bibr" rid="CR87">87</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://kipa22.grand-challenge.org/dataset/">https://kipa22.grand-challenge.org/dataset/</ext-link></td></tr><tr><td colspan="1" rowspan="1">KiTS23<sup><xref ref-type="bibr" rid="CR12">12</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://github.com/neheller/kits23">https://github.com/neheller/kits23</ext-link></td></tr><tr><td colspan="1" rowspan="1">LAScarQS2022 Task 1<sup><xref ref-type="bibr" rid="CR88">88</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://zmiclab.github.io/projects/lascarqs22/data.html">https://zmiclab.github.io/projects/lascarqs22/data.html</ext-link></td></tr><tr><td colspan="1" rowspan="1">LAScarQS2022 Task 2<sup><xref ref-type="bibr" rid="CR88">88</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://zmiclab.github.io/projects/lascarqs22/data.html">https://zmiclab.github.io/projects/lascarqs22/data.html</ext-link></td></tr><tr><td colspan="1" rowspan="1">LNDb<sup><xref ref-type="bibr" rid="CR89">89</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/7153205#.Yz_oVHbMJPZ">https://zenodo.org/record/7153205#Y&#729;z_oVHbMJPZ</ext-link></td></tr><tr><td colspan="1" rowspan="1">LUNA16<sup><xref ref-type="bibr" rid="CR90">90</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://luna16.grand-challenge.org/">https://luna16.grand-challenge.org/</ext-link></td></tr><tr><td colspan="1" rowspan="1">MM-WHS CT<sup><xref ref-type="bibr" rid="CR91">91</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://mega.nz/folder/UNMF2YYI#1cqJVzo4p_wESv9P_pc8uA">https://mega.nz/folder/UNMF2YYI#1cqJVzo4p_wESv9P_pc8uA</ext-link></td></tr><tr><td colspan="1" rowspan="1">MM-WHS MR<sup><xref ref-type="bibr" rid="CR91">91</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://mega.nz/folder/UNMF2YYI#1cqJVzo4p_wESv9P_pc8uA">https://mega.nz/folder/UNMF2YYI#1cqJVzo4p_wESv9P_pc8uA</ext-link></td></tr><tr><td colspan="1" rowspan="1">MRSpineSeg<sup><xref ref-type="bibr" rid="CR11">11</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://www.cg.informatik.uni-siegen.de/en/spine-segmentation-and-analysis">https://www.cg.informatik.uni-siegen.de/en/spine-segmentation-and-analysis</ext-link></td></tr><tr><td colspan="1" rowspan="1">MSD Cardiac<sup><xref ref-type="bibr" rid="CR10">10</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="http://medicaldecathlon.com/">http://medicaldecathlon.com/</ext-link></td></tr><tr><td colspan="1" rowspan="1">MSD Colon<sup><xref ref-type="bibr" rid="CR10">10</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="http://medicaldecathlon.com/">http://medicaldecathlon.com/</ext-link></td></tr><tr><td colspan="1" rowspan="1">MSD HepaticVessel<sup><xref ref-type="bibr" rid="CR10">10</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="http://medicaldecathlon.com/">http://medicaldecathlon.com/</ext-link></td></tr><tr><td colspan="1" rowspan="1">MSD Hippocampus<sup><xref ref-type="bibr" rid="CR10">10</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="http://medicaldecathlon.com/">http://medicaldecathlon.com/</ext-link></td></tr><tr><td colspan="1" rowspan="1">MSD Liver<sup><xref ref-type="bibr" rid="CR10">10</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="http://medicaldecathlon.com/">http://medicaldecathlon.com/</ext-link></td></tr><tr><td colspan="1" rowspan="1">MSD Lung<sup><xref ref-type="bibr" rid="CR10">10</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="http://medicaldecathlon.com/">http://medicaldecathlon.com/</ext-link></td></tr><tr><td colspan="1" rowspan="1">MSD Pancreas<sup><xref ref-type="bibr" rid="CR10">10</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="http://medicaldecathlon.com/">http://medicaldecathlon.com/</ext-link></td></tr><tr><td colspan="1" rowspan="1">MSD Prostate<sup><xref ref-type="bibr" rid="CR10">10</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="http://medicaldecathlon.com/">http://medicaldecathlon.com/</ext-link></td></tr><tr><td colspan="1" rowspan="1">MSD Spleen<sup><xref ref-type="bibr" rid="CR10">10</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="http://medicaldecathlon.com/">http://medicaldecathlon.com/</ext-link></td></tr><tr><td colspan="1" rowspan="1">MyoPS2020<sup><xref ref-type="bibr" rid="CR92">92</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://mega.nz/folder/BRdnDISQ#FnCg9ykPlTWYe5hrRZxi-w">https://mega.nz/folder/BRdnDISQ#FnCg9ykPlTWYe5hrRZxi-w</ext-link></td></tr><tr><td colspan="1" rowspan="1">NSCLC<sup><xref ref-type="bibr" rid="CR93">93</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=68551327">https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=68551327</ext-link></td></tr><tr><td colspan="1" rowspan="1">Pancreas CT<sup><xref ref-type="bibr" rid="CR94">94</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://wiki.cancerimagingarchive.net/display/public/pancreas-ct">https://wiki.cancerimagingarchive.net/display/public/pancreas-ct</ext-link></td></tr><tr><td colspan="1" rowspan="1">Parse2022<sup><xref ref-type="bibr" rid="CR95">95</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://parse2022.grand-challenge.org/Dataset/">https://parse2022.grand-challenge.org/Dataset/</ext-link></td></tr><tr><td colspan="1" rowspan="1">PDDCA<sup><xref ref-type="bibr" rid="CR96">96</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://www.imagenglab.com/newsite/pddca/">https://www.imagenglab.com/newsite/pddca/</ext-link></td></tr><tr><td colspan="1" rowspan="1">PROMISE12<sup><xref ref-type="bibr" rid="CR97">97</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://promise12.grand-challenge.org/Details/">https://promise12.grand-challenge.org/Details/</ext-link></td></tr><tr><td colspan="1" rowspan="1">SEGA<sup><xref ref-type="bibr" rid="CR98">98</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://multicenteraorta.grand-challenge.org/data/">https://multicenteraorta.grand-challenge.org/data/</ext-link></td></tr><tr><td colspan="1" rowspan="1">SegRap2023 Task1<sup><xref ref-type="bibr" rid="CR99">99</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://segrap2023.grand-challenge.org/">https://segrap2023.grand-challenge.org/</ext-link></td></tr><tr><td colspan="1" rowspan="1">SegRap2023 Task2<sup><xref ref-type="bibr" rid="CR99">99</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://segrap2023.grand-challenge.org/">https://segrap2023.grand-challenge.org/</ext-link></td></tr><tr><td colspan="1" rowspan="1">SegTHOR<sup><xref ref-type="bibr" rid="CR100">100</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://competitions.codalab.org/competitions/21145#learn_the_details">https://competitions.codalab.org/competitions/21145#learn_the_details</ext-link></td></tr><tr><td colspan="1" rowspan="1">SKI10<sup><xref ref-type="bibr" rid="CR101">101</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://ambellan.de/sharing/QjrntLwah">https://ambellan.de/sharing/QjrntLwah</ext-link></td></tr><tr><td colspan="1" rowspan="1">SLIVER07<sup><xref ref-type="bibr" rid="CR102">102</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://sliver07.grand-challenge.org/">https://sliver07.grand-challenge.org/</ext-link></td></tr><tr><td colspan="1" rowspan="1">ToothFairy<sup><xref ref-type="bibr" rid="CR103">103</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://ditto.ing.unimore.it/toothfairy/">https://ditto.ing.unimore.it/toothfairy/</ext-link></td></tr><tr><td colspan="1" rowspan="1">TotalSegmentator Cardiac<sup><xref ref-type="bibr" rid="CR52">52</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/6802614">https://zenodo.org/record/6802614</ext-link></td></tr><tr><td colspan="1" rowspan="1">TotalSegmentator Muscles<sup><xref ref-type="bibr" rid="CR52">52</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/6802614">https://zenodo.org/record/6802614</ext-link></td></tr><tr><td colspan="1" rowspan="1">TotalSegmentator Organs<sup><xref ref-type="bibr" rid="CR52">52</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/6802614">https://zenodo.org/record/6802614</ext-link></td></tr><tr><td colspan="1" rowspan="1">TotalSegmentator Ribs<sup><xref ref-type="bibr" rid="CR52">52</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/6802614">https://zenodo.org/record/6802614</ext-link></td></tr><tr><td colspan="1" rowspan="1">TotalSegmentator Vertebrae<sup><xref ref-type="bibr" rid="CR52">52</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/6802614">https://zenodo.org/record/6802614</ext-link></td></tr><tr><td colspan="1" rowspan="1">TotalSegmentator V2<sup><xref ref-type="bibr" rid="CR52">52</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://zenodo.org/record/6802614">https://zenodo.org/record/6802614</ext-link></td></tr><tr><td colspan="1" rowspan="1">VerSe<sup><xref ref-type="bibr" rid="CR104">104</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://github.com/anjany/verse">https://github.com/anjany/verse</ext-link></td></tr><tr><td colspan="1" rowspan="1">WMH<sup><xref ref-type="bibr" rid="CR105">105</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://wmh.isi.uu.nl/">https://wmh.isi.uu.nl/</ext-link></td></tr><tr><td colspan="1" rowspan="1">WORD<sup><xref ref-type="bibr" rid="CR106">106</xref></sup></td><td colspan="1" rowspan="1"><ext-link ext-link-type="uri" xlink:href="https://github.com/HiLab-git/WORD">https://github.com/HiLab-git/WORD</ext-link></td></tr></tbody></table></table-wrap></p></notes><notes notes-type="data-availability"><title>Code availability</title><p>The code is available at <ext-link ext-link-type="uri" xlink:href="https://github.com/zhaoziheng/SAT">https://github.com/zhaoziheng/SAT</ext-link>.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par95">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Zhao</surname><given-names>L</given-names></name><name name-style="western"><surname>Wang</surname><given-names>M</given-names></name><name name-style="western"><surname>Song</surname><given-names>Z</given-names></name></person-group><article-title>Organ at risk segmentation in head and neck CT images using a two-stage segmentation framework based on 3d U-Net</article-title><source>IEEE Access</source><year>2019</year><volume>7</volume><fpage>144591</fpage><lpage>144602</lpage></element-citation><mixed-citation id="mc-CR1" publication-type="journal">Wang, Y., Zhao, L., Wang, M. &amp; Song, Z. Organ at risk segmentation in head and neck CT images using a two-stage segmentation framework based on 3d U-Net. <italic toggle="yes">IEEE Access</italic><bold>7</bold>, 144591&#8211;144602 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yan</surname><given-names>K</given-names></name><name name-style="western"><surname>Wang</surname><given-names>X</given-names></name><name name-style="western"><surname>Lu</surname><given-names>L</given-names></name><name name-style="western"><surname>Summers</surname><given-names>RM</given-names></name></person-group><article-title>Deeplesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning</article-title><source>J. Med. Imaging</source><year>2018</year><volume>5</volume><fpage>036501</fpage><lpage>036501</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1117/1.JMI.5.3.036501</pub-id><pub-id pub-id-type="pmcid">PMC6052252</pub-id><pub-id pub-id-type="pmid">30035154</pub-id></element-citation><mixed-citation id="mc-CR2" publication-type="journal">Yan, K., Wang, X., Lu, L. &amp; Summers, R. M. Deeplesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning. <italic toggle="yes">J. Med. Imaging</italic><bold>5</bold>, 036501&#8211;036501 (2018).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1117/1.JMI.5.3.036501</pub-id><pub-id pub-id-type="pmcid">PMC6052252</pub-id><pub-id pub-id-type="pmid">30035154</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Baid, U. et al. The RSNA-ASNR-MICCAI Brats 2021 benchmark on brain tumor segmentation and radiogenomic classification. Preprint at 10.48550/arXiv.2107.02314 (2021).</mixed-citation></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nouranian</surname><given-names>S</given-names></name><etal/></person-group><article-title>Learning-based multi-label segmentation of transrectal ultrasound images for prostate brachytherapy</article-title><source>IEEE Trans. Med. Imaging</source><year>2015</year><volume>35</volume><fpage>921</fpage><lpage>932</lpage><pub-id pub-id-type="pmid">26599701</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2015.2502540</pub-id></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Nouranian, S. et al. Learning-based multi-label segmentation of transrectal ultrasound images for prostate brachytherapy. <italic toggle="yes">IEEE Trans. Med. Imaging</italic><bold>35</bold>, 921&#8211;932 (2015).<pub-id pub-id-type="pmid">26599701</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2015.2502540</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jaffray</surname><given-names>DA</given-names></name><name name-style="western"><surname>Knaul</surname><given-names>F</given-names></name><name name-style="western"><surname>Baumann</surname><given-names>M</given-names></name><name name-style="western"><surname>Gospodarowicz</surname><given-names>M</given-names></name></person-group><article-title>Harnessing progress in radiotherapy for global cancer control</article-title><source>Nat. Cancer</source><year>2023</year><volume>4</volume><fpage>1228</fpage><lpage>1238</lpage><pub-id pub-id-type="pmid">37749355</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s43018-023-00619-7</pub-id></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Jaffray, D. A., Knaul, F., Baumann, M. &amp; Gospodarowicz, M. Harnessing progress in radiotherapy for global cancer control. <italic toggle="yes">Nat. Cancer</italic><bold>4</bold>, 1228&#8211;1238 (2023).<pub-id pub-id-type="pmid">37749355</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s43018-023-00619-7</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><mixed-citation publication-type="other">Nauffal, V. et al. Noninvasive assessment of organ-specific and shared pathways in multi-organ fibrosis using T1 mapping. <italic toggle="yes">Nat. Med.</italic><bold>30</bold>, 1749&#8211;1760 (2024).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41591-024-03010-w</pub-id><pub-id pub-id-type="pmid">38806679</pub-id></mixed-citation></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bai</surname><given-names>W</given-names></name><etal/></person-group><article-title>A population-based phenome-wide association study of cardiac and aortic structure and function</article-title><source>Nat. Med.</source><year>2020</year><volume>26</volume><fpage>1654</fpage><lpage>1662</lpage><pub-id pub-id-type="pmid">32839619</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41591-020-1009-y</pub-id><pub-id pub-id-type="pmcid">PMC7613250</pub-id></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Bai, W. et al. A population-based phenome-wide association study of cardiac and aortic structure and function. <italic toggle="yes">Nat. Med.</italic><bold>26</bold>, 1654&#8211;1662 (2020).<pub-id pub-id-type="pmid">32839619</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41591-020-1009-y</pub-id><pub-id pub-id-type="pmcid">PMC7613250</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>J</given-names></name><etal/></person-group><article-title>Abdomenct-1k: is abdominal organ segmentation a solved problem?</article-title><source>IEEE Trans. Pattern Anal. Machine Intell.</source><year>2021</year><volume>44</volume><fpage>6695</fpage><lpage>6714</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TPAMI.2021.3100536</pub-id><pub-id pub-id-type="pmid">34314356</pub-id></element-citation><mixed-citation id="mc-CR8" publication-type="journal">Ma, J. et al. Abdomenct-1k: is abdominal organ segmentation a solved problem? <italic toggle="yes">IEEE Trans. Pattern Anal. Machine Intell.</italic><bold>44</bold>, 6695&#8211;6714 (2021).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TPAMI.2021.3100536</pub-id><pub-id pub-id-type="pmid">34314356</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>J</given-names></name><etal/></person-group><article-title>Unleashing the strengths of unlabelled data in deep learning-assisted pan-cancer abdominal organ quantification: the flare22 challenge</article-title><source>Lancet Digit. Health</source><year>2024</year><volume>6</volume><fpage>e815</fpage><lpage>e826</lpage><pub-id pub-id-type="pmid">39455194</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/S2589-7500(24)00154-7</pub-id></element-citation><mixed-citation id="mc-CR9" publication-type="journal">Ma, J. et al. Unleashing the strengths of unlabelled data in deep learning-assisted pan-cancer abdominal organ quantification: the flare22 challenge. <italic toggle="yes">Lancet Digit. Health</italic><bold>6</bold>, e815&#8211;e826 (2024).<pub-id pub-id-type="pmid">39455194</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/S2589-7500(24)00154-7</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Antonelli</surname><given-names>M</given-names></name><etal/></person-group><article-title>The medical segmentation decathlon</article-title><source>Nat. Commun.</source><year>2022</year><volume>13</volume><fpage>4128</fpage><pub-id pub-id-type="pmid">35840566</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41467-022-30695-9</pub-id><pub-id pub-id-type="pmcid">PMC9287542</pub-id></element-citation><mixed-citation id="mc-CR10" publication-type="journal">Antonelli, M. et al. The medical segmentation decathlon. <italic toggle="yes">Nat. Commun.</italic><bold>13</bold>, 4128 (2022).<pub-id pub-id-type="pmid">35840566</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41467-022-30695-9</pub-id><pub-id pub-id-type="pmcid">PMC9287542</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pang</surname><given-names>S</given-names></name><etal/></person-group><article-title>Spineparsenet: spine parsing for volumetric MR image by a two-stage segmentation framework with semantic image representation</article-title><source>IEEE Trans. Med. Imaging</source><year>2020</year><volume>40</volume><fpage>262</fpage><lpage>273</lpage><pub-id pub-id-type="pmid">32956047</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2020.3025087</pub-id></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Pang, S. et al. Spineparsenet: spine parsing for volumetric MR image by a two-stage segmentation framework with semantic image representation. <italic toggle="yes">IEEE Trans. Med. Imaging</italic><bold>40</bold>, 262&#8211;273 (2020).<pub-id pub-id-type="pmid">32956047</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2020.3025087</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Heller, N. et al. The kits21 challenge: automatic segmentation of kidneys, renal tumors, and renal cysts in corticomedullary-phase CT. Preprint at 10.48550/arXiv.2307.01984 (2023).</mixed-citation></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>J</given-names></name><etal/></person-group><article-title>Toward data-efficient learning: a benchmark for COVID-19 CT lung and infection segmentation</article-title><source>Med. Phys.</source><year>2021</year><volume>48</volume><fpage>1197</fpage><lpage>1210</lpage><pub-id pub-id-type="pmid">33354790</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/mp.14676</pub-id></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Ma, J. et al. Toward data-efficient learning: a benchmark for COVID-19 CT lung and infection segmentation. <italic toggle="yes">Med. Phys.</italic><bold>48</bold>, 1197&#8211;1210 (2021).<pub-id pub-id-type="pmid">33354790</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/mp.14676</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><mixed-citation publication-type="other">Ma, J., Li, F. &amp; Wang, B. U-mamba: enhancing long-range dependency for biomedical image segmentation. Preprint at 10.48550/arXiv.2401.04722 (2024).</mixed-citation></ref><ref id="CR15"><label>15.</label><mixed-citation publication-type="other">Hatamizadeh, A. et al. UNETR: transformers for 3D medical image segmentation. In <italic toggle="yes">Proc. IEEE/CVF Winter Conference on Applications of Computer Vision</italic> 574&#8211;584 (IEEE, 2022).</mixed-citation></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Isensee</surname><given-names>F</given-names></name><name name-style="western"><surname>Jaeger</surname><given-names>PF</given-names></name><name name-style="western"><surname>Kohl</surname><given-names>SA</given-names></name><name name-style="western"><surname>Petersen</surname><given-names>J</given-names></name><name name-style="western"><surname>Maier-Hein</surname><given-names>KH</given-names></name></person-group><article-title>nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation</article-title><source>Nat. Methods</source><year>2021</year><volume>18</volume><fpage>203</fpage><lpage>211</lpage><pub-id pub-id-type="pmid">33288961</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41592-020-01008-z</pub-id></element-citation><mixed-citation id="mc-CR16" publication-type="journal">Isensee, F., Jaeger, P. F., Kohl, S. A., Petersen, J. &amp; Maier-Hein, K. H. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. <italic toggle="yes">Nat. Methods</italic><bold>18</bold>, 203&#8211;211 (2021).<pub-id pub-id-type="pmid">33288961</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41592-020-01008-z</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Milletari, F., Navab, N. &amp; Ahmadi, S.-A. V-net: fully convolutional neural networks for volumetric medical image segmentation. In <italic toggle="yes">International Conference on 3D Vision (3DV)</italic> 565&#8211;571 (IEEE, 2016).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Ronneberger, O., Fischer, P. &amp; Brox, T. U-net: convolutional networks for biomedical image segmentation. In <italic toggle="yes">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</italic> 234&#8211;241 (Springer, 2015).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Hatamizadeh, A. et al. Swin UNETR: swin transformers for semantic segmentation of brain tumors in MRI images. In <italic toggle="yes">International MICCAI Brainlesion Workshop</italic>, 272&#8211;284 (Springer, 2021).</mixed-citation></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>J</given-names></name><etal/></person-group><article-title>Segment anything in medical images</article-title><source>Nat. Commun.</source><year>2024</year><volume>15</volume><fpage>654</fpage><pub-id pub-id-type="pmid">38253604</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41467-024-44824-z</pub-id><pub-id pub-id-type="pmcid">PMC10803759</pub-id></element-citation><mixed-citation id="mc-CR20" publication-type="journal">Ma, J. et al. Segment anything in medical images. <italic toggle="yes">Nat. Commun.</italic><bold>15</bold>, 654 (2024).<pub-id pub-id-type="pmid">38253604</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41467-024-44824-z</pub-id><pub-id pub-id-type="pmcid">PMC10803759</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Segment anything model for medical images?</article-title><source>Med. Image Anal.</source><year>2024</year><volume>92</volume><fpage>103061</fpage><pub-id pub-id-type="pmid">38086235</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2023.103061</pub-id></element-citation><mixed-citation id="mc-CR21" publication-type="journal">Huang, Y. et al. Segment anything model for medical images? <italic toggle="yes">Med. Image Anal.</italic><bold>92</bold>, 103061 (2024).<pub-id pub-id-type="pmid">38086235</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2023.103061</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><mixed-citation publication-type="other">Kirillov, A. et al. Segment anything. In <italic toggle="yes">Proc. IEEE/CVF International Conference on Computer Vision</italic> 4015&#8211;4026 (IEEE, 2023).</mixed-citation></ref><ref id="CR23"><label>23.</label><mixed-citation publication-type="other">Zhao, T. et al. A foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities. <italic toggle="yes">Nat. Methods</italic><bold>22</bold>, 166&#8211;176 (2024).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41592-024-02499-w</pub-id><pub-id pub-id-type="pmid">39558098</pub-id></mixed-citation></ref><ref id="CR24"><label>24.</label><mixed-citation publication-type="other">Achiam, J. et al. GPT-4 Technical Report. Preprint at 10.48550/arXiv.2303.08774 (2023).</mixed-citation></ref><ref id="CR25"><label>25.</label><mixed-citation publication-type="other">Qu, C. et al. Abdomenatlas-8k: annotating 8,000 CT volumes for multi-organ segmentation in three weeks. In Proc<italic toggle="yes">. 37th Conference on Neural Information Processing Systems (NeurIPS 2023) Track on Datasets and Benchmarks</italic> (NIPS, 2023).</mixed-citation></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Liu, Y. et al. Merit: multi-view evidential learning for reliable and interpretable liver fibrosis staging. <italic toggle="yes">Med. Image Anal.</italic><bold>102</bold><italic toggle="yes">,</italic> 103507 (2024).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2025.103507</pub-id><pub-id pub-id-type="pmid">40022854</pub-id></mixed-citation></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dou</surname><given-names>Q</given-names></name><etal/></person-group><article-title>3D deeply supervised network for automated segmentation of volumetric medical images</article-title><source>Med. Image Anal.</source><year>2017</year><volume>41</volume><fpage>40</fpage><lpage>54</lpage><pub-id pub-id-type="pmid">28526212</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2017.05.001</pub-id></element-citation><mixed-citation id="mc-CR27" publication-type="journal">Dou, Q. et al. 3D deeply supervised network for automated segmentation of volumetric medical images. <italic toggle="yes">Med. Image Anal.</italic><bold>41</bold>, 40&#8211;54 (2017).<pub-id pub-id-type="pmid">28526212</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2017.05.001</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>X</given-names></name><etal/></person-group><article-title>H-denseunet: hybrid densely connected UNET for liver and tumor segmentation from CT volumes</article-title><source>IEEE Trans. Med. Imaging</source><year>2018</year><volume>37</volume><fpage>2663</fpage><lpage>2674</lpage><pub-id pub-id-type="pmid">29994201</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2018.2845918</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Li, X. et al. H-denseunet: hybrid densely connected UNET for liver and tumor segmentation from CT volumes. <italic toggle="yes">IEEE Trans. Med. Imaging</italic><bold>37</bold>, 2663&#8211;2674 (2018).<pub-id pub-id-type="pmid">29994201</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2018.2845918</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Yu, Q. et al. Recurrent saliency transformation network: incorporating multi-stage visual cues for small organ segmentation. In <italic toggle="yes">Proc. IEEE Conference on Computer Vision and Pattern Recognition</italic> 8280&#8211;8289 (IEEE, 2018).</mixed-citation></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>Z</given-names></name><name name-style="western"><surname>Siddiquee</surname><given-names>MMR</given-names></name><name name-style="western"><surname>Tajbakhsh</surname><given-names>N</given-names></name><name name-style="western"><surname>Liang</surname><given-names>J</given-names></name></person-group><article-title>UNet++: redesigning skip connections to exploit multiscale features in image segmentation</article-title><source>IEEE Trans. Med. Imaging</source><year>2019</year><volume>39</volume><fpage>1856</fpage><lpage>1867</lpage><pub-id pub-id-type="pmid">31841402</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2019.2959609</pub-id><pub-id pub-id-type="pmcid">PMC7357299</pub-id></element-citation><mixed-citation id="mc-CR30" publication-type="journal">Zhou, Z., Siddiquee, M. M. R., Tajbakhsh, N. &amp; Liang, J. UNet++: redesigning skip connections to exploit multiscale features in image segmentation. <italic toggle="yes">IEEE Trans. Med. Imaging</italic><bold>39</bold>, 1856&#8211;1867 (2019).<pub-id pub-id-type="pmid">31841402</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2019.2959609</pub-id><pub-id pub-id-type="pmcid">PMC7357299</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Schlemper</surname><given-names>J</given-names></name><etal/></person-group><article-title>Attention gated networks: learning to leverage salient regions in medical images</article-title><source>Med. Image Anal.</source><year>2019</year><volume>53</volume><fpage>197</fpage><lpage>207</lpage><pub-id pub-id-type="pmid">30802813</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2019.01.012</pub-id><pub-id pub-id-type="pmcid">PMC7610718</pub-id></element-citation><mixed-citation id="mc-CR31" publication-type="journal">Schlemper, J. et al. Attention gated networks: learning to leverage salient regions in medical images. <italic toggle="yes">Med. Image Anal.</italic><bold>53</bold>, 197&#8211;207 (2019).<pub-id pub-id-type="pmid">30802813</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2019.01.012</pub-id><pub-id pub-id-type="pmcid">PMC7610718</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Decoupled pyramid correlation network for liver tumor segmentation from CT images</article-title><source>Med. Phys.</source><year>2022</year><volume>49</volume><fpage>7207</fpage><lpage>7221</lpage><pub-id pub-id-type="pmid">35620834</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/mp.15723</pub-id></element-citation><mixed-citation id="mc-CR32" publication-type="journal">Zhang, Y. et al. Decoupled pyramid correlation network for liver tumor segmentation from CT images. <italic toggle="yes">Med. Phys.</italic><bold>49</bold>, 7207&#8211;7221 (2022).<pub-id pub-id-type="pmid">35620834</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/mp.15723</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Cao, H. et al. Swin-Unet: Unet-like pure transformer for medical image segmentation. In <italic toggle="yes">European Conference on Computer Vision</italic> 205&#8211;218 (Springer, 2022).</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Xie, Y., Zhang, J., Shen, C. &amp; Xia, Y. CoTr: efficiently bridging CNN and transformer for 3D medical image segmentation. In <italic toggle="yes">Medical Image Computing and Computer-Assisted Intervention (MICCAI)</italic> 171&#8211;180 (Springer, 2021).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Zhang, Y. et al. mmformer: Multimodal medical transformer for incomplete multimodal learning of brain tumor segmentation. In <italic toggle="yes">International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</italic> 107&#8211;117 (Springer, 2022).</mixed-citation></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>J</given-names></name><etal/></person-group><article-title>TransUNet: Rethinking the U-Net architecture design for medical image segmentation through the lens of transformers</article-title><source>Med. Image Anal.</source><year>2024</year><volume>97</volume><fpage>103280</fpage><pub-id pub-id-type="pmid">39096845</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2024.103280</pub-id></element-citation><mixed-citation id="mc-CR36" publication-type="journal">Chen, J. et al. TransUNet: Rethinking the U-Net architecture design for medical image segmentation through the lens of transformers. <italic toggle="yes">Med. Image Anal.</italic><bold>97</bold>, 103280 (2024).<pub-id pub-id-type="pmid">39096845</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2024.103280</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Zhou, H.-Y. et al. nnformer: volumetric medical image segmentation via a 3d transformer. <italic toggle="yes">IEEE Transactions on Image Processing</italic> (IEEE, 2023).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TIP.2023.3293771</pub-id><pub-id pub-id-type="pmid">37440404</pub-id></mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Wang, H. et al. Sam-med3d: Towards general-purpose segmentation models for volumetric medical images. In <italic toggle="yes">European Conference on Computer Vision</italic> 51&#8211;67 (Springer, 2024).</mixed-citation></ref><ref id="CR39"><label>39.</label><mixed-citation publication-type="other">Zhang, X. et al. Radgenome-chest CT: a grounded vision-language dataset for chest CT analysis. Preprint at 10.48550/arXiv.2404.16754 (2024).</mixed-citation></ref><ref id="CR40"><label>40.</label><mixed-citation publication-type="other">Xie, Y. et al. MedTrinity-25M: A large-scale multimodal dataset with multigranular annotations for medicine. In <italic toggle="yes">International Conference on Learning Representations</italic> (2025).</mixed-citation></ref><ref id="CR41"><label>41.</label><mixed-citation publication-type="other">Xu, J. et al. Learning open-vocabulary semantic segmentation models from natural language supervision. In <italic toggle="yes">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 2935&#8211;2944 (IEEE, 2023).</mixed-citation></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>H</given-names></name><etal/></person-group><article-title>Ov-vis: open-vocabulary video instance segmentation</article-title><source>Int. J. Computer Vis.</source><year>2024</year><volume>132</volume><fpage>5048</fpage><lpage>5065</lpage></element-citation><mixed-citation id="mc-CR42" publication-type="journal">Wang, H. et al. Ov-vis: open-vocabulary video instance segmentation. <italic toggle="yes">Int. J. Computer Vis.</italic><bold>132</bold>, 5048&#8211;5065 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Lai, X. et al. Lisa: Reasoning segmentation via large language model. In <italic toggle="yes">Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition</italic> 9579&#8211;9589 (IEEE, 2024).</mixed-citation></ref><ref id="CR44"><label>44.</label><mixed-citation publication-type="other">Yan, C. et al. Visa: Reasoning video object segmentation via large language models. In <italic toggle="yes">European Conference on Computer Vision</italic> 98&#8211;115 (Springer, 2024).</mixed-citation></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Li, H. et al. Lorkd: low-rank knowledge decomposition for medical foundation models. Preprint at 10.48550/arXiv.2404.17184 (2024).</mixed-citation></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bodenreider</surname><given-names>O</given-names></name></person-group><article-title>The unified medical language system (UMLS): integrating biomedical terminology</article-title><source>Nucleic Acids Res.</source><year>2004</year><volume>32</volume><fpage>D267</fpage><lpage>D270</lpage><pub-id pub-id-type="pmid">14681409</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/nar/gkh061</pub-id><pub-id pub-id-type="pmcid">PMC308795</pub-id></element-citation><mixed-citation id="mc-CR46" publication-type="journal">Bodenreider, O. The unified medical language system (UMLS): integrating biomedical terminology. <italic toggle="yes">Nucleic Acids Res.</italic><bold>32</bold>, D267&#8211;D270 (2004).<pub-id pub-id-type="pmid">14681409</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/nar/gkh061</pub-id><pub-id pub-id-type="pmcid">PMC308795</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">National Library of Medicine. National library of medicine&#8212;national institutes of health. <ext-link ext-link-type="uri" xlink:href="https://www.nlm.nih.gov/">https://www.nlm.nih.gov/</ext-link>.</mixed-citation></ref><ref id="CR48"><label>48.</label><citation-alternatives><element-citation id="ec-CR48" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>X</given-names></name><name name-style="western"><surname>Wu</surname><given-names>C</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Xie</surname><given-names>W</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name></person-group><article-title>Knowledge-enhanced visual-language pre-training on chest radiology images</article-title><source>Nat. Commun.</source><year>2023</year><volume>14</volume><fpage>4542</fpage><pub-id pub-id-type="pmid">37507376</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41467-023-40260-7</pub-id><pub-id pub-id-type="pmcid">PMC10382552</pub-id></element-citation><mixed-citation id="mc-CR48" publication-type="journal">Zhang, X., Wu, C., Zhang, Y., Xie, W. &amp; Wang, Y. Knowledge-enhanced visual-language pre-training on chest radiology images. <italic toggle="yes">Nat. Commun.</italic><bold>14</bold>, 4542 (2023).<pub-id pub-id-type="pmid">37507376</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41467-023-40260-7</pub-id><pub-id pub-id-type="pmcid">PMC10382552</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR49"><label>49.</label><mixed-citation publication-type="other">Wu, C., Zhang, X., Zhang, Y., Wang, Y. &amp; Xie, W. Medklip: medical knowledge enhanced language-image pre-training for x-ray diagnosis. In <italic toggle="yes">Proc. IEEE/CVF International Conference on Computer Vision</italic> 21372&#8211;21383 (IEEE, 2023).</mixed-citation></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Lei, J. et al. Unibrain: universal brain MRI diagnosis with hierarchical knowledge-enhanced pre-training. <italic toggle="yes">Computerized Med. Imaging Graph.</italic> 102516 (2025). <ext-link ext-link-type="uri" xlink:href="https://www.sciencedirect.com/science/article/pii/S0895611125000254">https://www.sciencedirect.com/science/article/pii/S0895611125000254</ext-link>.<pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.compmedimag.2025.102516</pub-id><pub-id pub-id-type="pmid">40073706</pub-id></mixed-citation></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>Q</given-names></name><etal/></person-group><article-title>Large-scale long-tailed disease diagnosis on radiology images</article-title><source>Nat. Commun.</source><year>2024</year><volume>15</volume><fpage>10147</fpage><pub-id pub-id-type="pmid">39578456</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41467-024-54424-6</pub-id><pub-id pub-id-type="pmcid">PMC11584732</pub-id></element-citation><mixed-citation id="mc-CR51" publication-type="journal">Zheng, Q. et al. Large-scale long-tailed disease diagnosis on radiology images. <italic toggle="yes">Nat. Commun.</italic><bold>15</bold>, 10147 (2024).<pub-id pub-id-type="pmid">39578456</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41467-024-54424-6</pub-id><pub-id pub-id-type="pmcid">PMC11584732</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><mixed-citation publication-type="other">Wasserthal, J. et al. Totalsegmentator: robust segmentation of 104 anatomic structures in CT images. <italic toggle="yes">Radiol. Artif. Intell.</italic><bold>5</bold>, e230024 (2023).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1148/ryai.230024</pub-id><pub-id pub-id-type="pmcid">PMC10546353</pub-id><pub-id pub-id-type="pmid">37795137</pub-id></mixed-citation></ref><ref id="CR53"><label>53.</label><mixed-citation publication-type="other">Tian, J., Liu, L., Shi, Z. &amp; Xu, F. Automatic couinaud segmentation from CT volumes on liver using GLC-UNET. In <italic toggle="yes">International Workshop on Machine Learning in Medical Imaging</italic> 274&#8211;282 (Springer, 2019).</mixed-citation></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">Kenton, J. D. M.-W. C. &amp; Toutanova, L. K. BERT: pre-training of deep bidirectional transformers for language understanding. In <italic toggle="yes">Proceedings of Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NACCL-HLT)</italic> 4171&#8211;4186 (2019).</mixed-citation></ref><ref id="CR55"><label>55.</label><citation-alternatives><element-citation id="ec-CR55" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lee</surname><given-names>J</given-names></name><etal/></person-group><article-title>Biobert: a pre-trained biomedical language representation model for biomedical text mining</article-title><source>Bioinformatics</source><year>2020</year><volume>36</volume><fpage>1234</fpage><lpage>1240</lpage><pub-id pub-id-type="pmid">31501885</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/bioinformatics/btz682</pub-id><pub-id pub-id-type="pmcid">PMC7703786</pub-id></element-citation><mixed-citation id="mc-CR55" publication-type="journal">Lee, J. et al. Biobert: a pre-trained biomedical language representation model for biomedical text mining. <italic toggle="yes">Bioinformatics</italic><bold>36</bold>, 1234&#8211;1240 (2020).<pub-id pub-id-type="pmid">31501885</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/bioinformatics/btz682</pub-id><pub-id pub-id-type="pmcid">PMC7703786</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Radford, A. et al. Learning transferable visual models from natural language supervision. In <italic toggle="yes">International conference on machine learning</italic> 8748&#8211;8763 (PMLR, 2021).</mixed-citation></ref><ref id="CR57"><label>57.</label><mixed-citation publication-type="other">Lin, W. et al. PMC-CLIP: contrastive language-image pre-training using biomedical documents. In <italic toggle="yes">International Conference on Medical Image Computing and Computer-Assisted Intervention</italic> 525&#8211;536 (Springer, 2023).</mixed-citation></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">Zhang, S. et al. A multimodal biomedical foundation model trained from fifteen million image&#8211;text pairs. <italic toggle="yes">NEJM AI</italic><bold>2</bold>, AIoa2400640 (2024).</mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">Vaswani, A. et al. Attention is all you need. In <italic toggle="yes">Advances in Neural Information Processing Systems</italic><bold>30</bold> (2017).</mixed-citation></ref><ref id="CR60"><label>60.</label><citation-alternatives><element-citation id="ec-CR60" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Maier-Hein</surname><given-names>L</given-names></name><etal/></person-group><article-title>Metrics reloaded: recommendations for image analysis validation</article-title><source>Nat. Methods</source><year>2024</year><volume>21</volume><fpage>195</fpage><lpage>212</lpage><pub-id pub-id-type="pmid">38347141</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41592-023-02151-z</pub-id><pub-id pub-id-type="pmcid">PMC11182665</pub-id></element-citation><mixed-citation id="mc-CR60" publication-type="journal">Maier-Hein, L. et al. Metrics reloaded: recommendations for image analysis validation. <italic toggle="yes">Nat. Methods</italic><bold>21</bold>, 195&#8211;212 (2024).<pub-id pub-id-type="pmid">38347141</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41592-023-02151-z</pub-id><pub-id pub-id-type="pmcid">PMC11182665</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR61"><label>61.</label><citation-alternatives><element-citation id="ec-CR61" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nikolov</surname><given-names>S</given-names></name><etal/></person-group><article-title>Clinically applicable segmentation of head and neck anatomy for radiotherapy: deep learning algorithm development and validation study</article-title><source>J. Med. Internet Res.</source><year>2021</year><volume>23</volume><fpage>e26151</fpage><pub-id pub-id-type="pmid">34255661</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.2196/26151</pub-id><pub-id pub-id-type="pmcid">PMC8314151</pub-id></element-citation><mixed-citation id="mc-CR61" publication-type="journal">Nikolov, S. et al. Clinically applicable segmentation of head and neck anatomy for radiotherapy: deep learning algorithm development and validation study. <italic toggle="yes">J. Med. Internet Res.</italic><bold>23</bold>, e26151 (2021).<pub-id pub-id-type="pmid">34255661</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.2196/26151</pub-id><pub-id pub-id-type="pmcid">PMC8314151</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR62"><label>62.</label><citation-alternatives><element-citation id="ec-CR62" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bernard</surname><given-names>O</given-names></name><etal/></person-group><article-title>Deep learning techniques for automatic MRI cardiac multi-structures segmentation and diagnosis: is the problem solved?</article-title><source>IEEE Trans. Med. Imaging</source><year>2018</year><volume>37</volume><fpage>2514</fpage><lpage>2525</lpage><pub-id pub-id-type="pmid">29994302</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2018.2837502</pub-id></element-citation><mixed-citation id="mc-CR62" publication-type="journal">Bernard, O. et al. Deep learning techniques for automatic MRI cardiac multi-structures segmentation and diagnosis: is the problem solved? <italic toggle="yes">IEEE Trans. Med. Imaging</italic><bold>37</bold>, 2514&#8211;2525 (2018).<pub-id pub-id-type="pmid">29994302</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2018.2837502</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR63"><label>63.</label><citation-alternatives><element-citation id="ec-CR63" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ji</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Amos: a large-scale abdominal multi-organ benchmark for versatile medical image segmentation</article-title><source>Adv. Neural Inf. Process.Systems</source><year>2022</year><volume>35</volume><fpage>36722</fpage><lpage>36732</lpage></element-citation><mixed-citation id="mc-CR63" publication-type="journal">Ji, Y. et al. Amos: a large-scale abdominal multi-organ benchmark for versatile medical image segmentation. <italic toggle="yes">Adv. Neural Inf. Process.Systems</italic><bold>35</bold>, 36722&#8211;36732 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR64"><label>64.</label><citation-alternatives><element-citation id="ec-CR64" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liew</surname><given-names>S-L</given-names></name><etal/></person-group><article-title>A large, curated, open-source stroke neuroimaging dataset to improve lesion segmentation algorithms</article-title><source>Sci. Data</source><year>2022</year><volume>9</volume><fpage>320</fpage><pub-id pub-id-type="pmid">35710678</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41597-022-01401-7</pub-id><pub-id pub-id-type="pmcid">PMC9203460</pub-id></element-citation><mixed-citation id="mc-CR64" publication-type="journal">Liew, S.-L. et al. A large, curated, open-source stroke neuroimaging dataset to improve lesion segmentation algorithms. <italic toggle="yes">Sci. Data</italic><bold>9</bold>, 320 (2022).<pub-id pub-id-type="pmid">35710678</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41597-022-01401-7</pub-id><pub-id pub-id-type="pmcid">PMC9203460</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR65"><label>65.</label><citation-alternatives><element-citation id="ec-CR65" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Quinton</surname><given-names>F</given-names></name><etal/></person-group><article-title>A tumour and liver automatic segmentation (atlas) dataset on contrast-enhanced magnetic resonance imaging for hepatocellular carcinoma</article-title><source>Data</source><year>2023</year><volume>8</volume><fpage>79</fpage></element-citation><mixed-citation id="mc-CR65" publication-type="journal">Quinton, F. et al. A tumour and liver automatic segmentation (atlas) dataset on contrast-enhanced magnetic resonance imaging for hepatocellular carcinoma. <italic toggle="yes">Data</italic><bold>8</bold>, 79 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR66"><label>66.</label><citation-alternatives><element-citation id="ec-CR66" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gatidis</surname><given-names>S</given-names></name><etal/></person-group><article-title>A whole-body FDG-PET/CT dataset with manually annotated tumor lesions</article-title><source>Sci. Data</source><year>2022</year><volume>9</volume><fpage>601</fpage><pub-id pub-id-type="pmid">36195599</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41597-022-01718-3</pub-id><pub-id pub-id-type="pmcid">PMC9532417</pub-id></element-citation><mixed-citation id="mc-CR66" publication-type="journal">Gatidis, S. et al. A whole-body FDG-PET/CT dataset with manually annotated tumor lesions. <italic toggle="yes">Sci. Data</italic><bold>9</bold>, 601 (2022).<pub-id pub-id-type="pmid">36195599</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41597-022-01718-3</pub-id><pub-id pub-id-type="pmcid">PMC9532417</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR67"><label>67.</label><citation-alternatives><element-citation id="ec-CR67" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Serag</surname><given-names>A</given-names></name><etal/></person-group><article-title>Construction of a consistent high-definition spatio-temporal atlas of the developing brain using adaptive kernel regression</article-title><source>Neuroimage</source><year>2012</year><volume>59</volume><fpage>2255</fpage><lpage>2265</lpage><pub-id pub-id-type="pmid">21985910</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2011.09.062</pub-id></element-citation><mixed-citation id="mc-CR67" publication-type="journal">Serag, A. et al. Construction of a consistent high-definition spatio-temporal atlas of the developing brain using adaptive kernel regression. <italic toggle="yes">Neuroimage</italic><bold>59</bold>, 2255&#8211;2265 (2012).<pub-id pub-id-type="pmid">21985910</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.neuroimage.2011.09.062</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR68"><label>68.</label><citation-alternatives><element-citation id="ec-CR68" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Avital</surname><given-names>I</given-names></name><etal/></person-group><article-title>Neural segmentation of seeding ROIs (sROIS) for pre-surgical brain tractography</article-title><source>IEEE Trans. Med. Imaging</source><year>2019</year><volume>39</volume><fpage>1655</fpage><lpage>1667</lpage><pub-id pub-id-type="pmid">31751233</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2019.2954477</pub-id></element-citation><mixed-citation id="mc-CR68" publication-type="journal">Avital, I. et al. Neural segmentation of seeding ROIs (sROIS) for pre-surgical brain tractography. <italic toggle="yes">IEEE Trans. Med. Imaging</italic><bold>39</bold>, 1655&#8211;1667 (2019).<pub-id pub-id-type="pmid">31751233</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2019.2954477</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR69"><label>69.</label><citation-alternatives><element-citation id="ec-CR69" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Menze</surname><given-names>BH</given-names></name><etal/></person-group><article-title>The multimodal brain tumor image segmentation benchmark (BraTS)</article-title><source>IEEE Trans. Med. Imaging</source><year>2014</year><volume>34</volume><fpage>1993</fpage><lpage>2024</lpage><pub-id pub-id-type="pmid">25494501</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2014.2377694</pub-id><pub-id pub-id-type="pmcid">PMC4833122</pub-id></element-citation><mixed-citation id="mc-CR69" publication-type="journal">Menze, B. H. et al. The multimodal brain tumor image segmentation benchmark (BraTS). <italic toggle="yes">IEEE Trans. Med. Imaging</italic><bold>34</bold>, 1993&#8211;2024 (2014).<pub-id pub-id-type="pmid">25494501</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2014.2377694</pub-id><pub-id pub-id-type="pmcid">PMC4833122</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR70"><label>70.</label><mixed-citation publication-type="other">LaBella, D. et al. The ASNR-MICCAI brain tumor segmentation (BraTS) challenge 2023: intracranial meningioma. Preprint at 10.48550/arXiv:2305.07642 (2023).</mixed-citation></ref><ref id="CR71"><label>71.</label><mixed-citation publication-type="other">Moawad, A. W. et al. The brain tumor segmentation (BraTS-METS) challenge 2023: brain metastasis segmentation on pre-treatment MRI. Preprint at 10.48550/arXiv.2306.00838 (2023).</mixed-citation></ref><ref id="CR72"><label>72.</label><mixed-citation publication-type="other">Kazerooni, A. F. et al. The brain tumor segmentation (BraTS) challenge 2023: focus on pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). Preprint at 10.48550/arXiv.2404.15009 (2023).</mixed-citation></ref><ref id="CR73"><label>73.</label><mixed-citation publication-type="other">Adewole, M. et al. The brain tumor segmentation (BraTS) challenge 2023: glioma segmentation in sub-Saharan Africa patient population (BraTS-Africa). Preprint at 10.48550/arXiv.2305.19369 (2023).</mixed-citation></ref><ref id="CR74"><label>74.</label><mixed-citation publication-type="other">Landman, B. et al. Miccai multi-atlas labeling beyond the cranial vault&#8211;workshop and challenge. In <italic toggle="yes">Proc. MICCAI Multi-Atlas Labeling Beyond Cranial Vault-Workshop Challenge</italic> Vol. 5, 12 (2015).</mixed-citation></ref><ref id="CR75"><label>75.</label><citation-alternatives><element-citation id="ec-CR75" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kavur</surname><given-names>AE</given-names></name><etal/></person-group><article-title>Chaos challenge-combined (CT-MR) healthy abdominal organ segmentation</article-title><source>Med. Image Anal.</source><year>2021</year><volume>69</volume><fpage>101950</fpage><pub-id pub-id-type="pmid">33421920</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2020.101950</pub-id></element-citation><mixed-citation id="mc-CR75" publication-type="journal">Kavur, A. E. et al. Chaos challenge-combined (CT-MR) healthy abdominal organ segmentation. <italic toggle="yes">Med. Image Anal.</italic><bold>69</bold>, 101950 (2021).<pub-id pub-id-type="pmid">33421920</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2020.101950</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR76"><label>76.</label><mixed-citation publication-type="other">Wang, K. et al. Extreme cardiac MRI analysis under respiratory motion: Results of the CMRxMotion Challenge. Preprint at 10.48550/arXiv.2507.19165 (2025).</mixed-citation></ref><ref id="CR77"><label>77.</label><citation-alternatives><element-citation id="ec-CR77" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dorent</surname><given-names>R</given-names></name><etal/></person-group><article-title>Crossmoda 2021 challenge: benchmark of cross-modality domain adaptation techniques for vestibular schwannoma and cochlea segmentation</article-title><source>Med. Image Anal.</source><year>2023</year><volume>83</volume><fpage>102628</fpage><pub-id pub-id-type="pmid">36283200</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2022.102628</pub-id><pub-id pub-id-type="pmcid">PMC10186181</pub-id></element-citation><mixed-citation id="mc-CR77" publication-type="journal">Dorent, R. et al. Crossmoda 2021 challenge: benchmark of cross-modality domain adaptation techniques for vestibular schwannoma and cochlea segmentation. <italic toggle="yes">Med. Image Anal.</italic><bold>83</bold>, 102628 (2023).<pub-id pub-id-type="pmid">36283200</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2022.102628</pub-id><pub-id pub-id-type="pmcid">PMC10186181</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR78"><label>78.</label><citation-alternatives><element-citation id="ec-CR78" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rister</surname><given-names>B</given-names></name><name name-style="western"><surname>Yi</surname><given-names>D</given-names></name><name name-style="western"><surname>Shivakumar</surname><given-names>K</given-names></name><name name-style="western"><surname>Nobashi</surname><given-names>T</given-names></name><name name-style="western"><surname>Rubin</surname><given-names>DL</given-names></name></person-group><article-title>CT-org, a new dataset for multiple organ segmentation in computed tomography</article-title><source>Sci. Data</source><year>2020</year><volume>7</volume><fpage>381</fpage><pub-id pub-id-type="pmid">33177518</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41597-020-00715-8</pub-id><pub-id pub-id-type="pmcid">PMC7658204</pub-id></element-citation><mixed-citation id="mc-CR78" publication-type="journal">Rister, B., Yi, D., Shivakumar, K., Nobashi, T. &amp; Rubin, D. L. CT-org, a new dataset for multiple organ segmentation in computed tomography. <italic toggle="yes">Sci. Data</italic><bold>7</bold>, 381 (2020).<pub-id pub-id-type="pmid">33177518</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41597-020-00715-8</pub-id><pub-id pub-id-type="pmcid">PMC7658204</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR79"><label>79.</label><citation-alternatives><element-citation id="ec-CR79" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>P</given-names></name><etal/></person-group><article-title>Deep learning to segment pelvic bones: large-scale CT datasets and baseline models</article-title><source>Int. J. Comput. Assist. Radiol. Surg.</source><year>2021</year><volume>16</volume><fpage>749</fpage><pub-id pub-id-type="pmid">33864189</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s11548-021-02363-8</pub-id></element-citation><mixed-citation id="mc-CR79" publication-type="journal">Liu, P. et al. Deep learning to segment pelvic bones: large-scale CT datasets and baseline models. <italic toggle="yes">Int. J. Comput. Assist. Radiol. Surg.</italic><bold>16</bold>, 749 (2021).<pub-id pub-id-type="pmid">33864189</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s11548-021-02363-8</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR80"><label>80.</label><mixed-citation publication-type="other">Jaus, A. et al. Towards unifying anatomy segmentation: Automated generation of a full-body ct dataset. In <italic toggle="yes">IEEE International Conference on Image Processing</italic> 41&#8211;47 (IEEE, 2024).</mixed-citation></ref><ref id="CR81"><label>81.</label><citation-alternatives><element-citation id="ec-CR81" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Payette</surname><given-names>K</given-names></name><etal/></person-group><article-title>An automatic multi-tissue human fetal brain segmentation benchmark using the fetal tissue annotation dataset</article-title><source>Sci. Data</source><year>2021</year><volume>8</volume><fpage>167</fpage><pub-id pub-id-type="pmid">34230489</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41597-021-00946-3</pub-id><pub-id pub-id-type="pmcid">PMC8260784</pub-id></element-citation><mixed-citation id="mc-CR81" publication-type="journal">Payette, K. et al. An automatic multi-tissue human fetal brain segmentation benchmark using the fetal tissue annotation dataset. <italic toggle="yes">Sci. Data</italic><bold>8</bold>, 167 (2021).<pub-id pub-id-type="pmid">34230489</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41597-021-00946-3</pub-id><pub-id pub-id-type="pmcid">PMC8260784</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR82"><label>82.</label><mixed-citation publication-type="other">Masoudi, M. et al. A new dataset of computed-tomography angiography images for computer-aided detection of pulmonary embolism. <italic toggle="yes">Sci. Data</italic><bold>5</bold>, 180180 (2018).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/sdata.2018.180</pub-id><pub-id pub-id-type="pmcid">PMC6122162</pub-id><pub-id pub-id-type="pmid">30179235</pub-id></mixed-citation></ref><ref id="CR83"><label>83.</label><citation-alternatives><element-citation id="ec-CR83" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Podobnik</surname><given-names>G</given-names></name><name name-style="western"><surname>Strojan</surname><given-names>P</given-names></name><name name-style="western"><surname>Peterlin</surname><given-names>P</given-names></name><name name-style="western"><surname>Ibragimov</surname><given-names>B</given-names></name><name name-style="western"><surname>Vrtovec</surname><given-names>T</given-names></name></person-group><article-title>Han-seg: the head and neck organ-at-risk CT and MR segmentation dataset</article-title><source>Med. Phys.</source><year>2023</year><volume>50</volume><fpage>1917</fpage><lpage>1927</lpage><pub-id pub-id-type="pmid">36594372</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/mp.16197</pub-id></element-citation><mixed-citation id="mc-CR83" publication-type="journal">Podobnik, G., Strojan, P., Peterlin, P., Ibragimov, B. &amp; Vrtovec, T. Han-seg: the head and neck organ-at-risk CT and MR segmentation dataset. <italic toggle="yes">Med. Phys.</italic><bold>50</bold>, 1917&#8211;1927 (2023).<pub-id pub-id-type="pmid">36594372</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/mp.16197</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR84"><label>84.</label><mixed-citation publication-type="other">Andrearczyk, V., Oreiller, V., Hatt, M. &amp; Depeursinge, A. Overview of the hecktor challenge at MICCAI 2022: Automatic head and neck tumor segmentation and outcome prediction in PET/CT. In Andrearczyk, V., Oreiller, V., Hatt, M. &amp; Depeursinge, A. (eds.) <italic toggle="yes">Head and Neck Tumor Segmentation and Outcome Prediction. HECKTOR 2022. Lecture Notes in Computer Science</italic> Vol. 13626 (Springer, 2023).</mixed-citation></ref><ref id="CR85"><label>85.</label><mixed-citation publication-type="other">Li, X. et al. The state-of-the-art 3d anisotropic intracranial hemorrhage segmentation on non-contrast head CT: the instance challenge. Preprint at 10.48550/arXiv.2301.03281 (2023).</mixed-citation></ref><ref id="CR86"><label>86.</label><citation-alternatives><element-citation id="ec-CR86" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hernandez Petzsche</surname><given-names>MR</given-names></name><etal/></person-group><article-title>Isles 2022: a multi-center magnetic resonance imaging stroke lesion segmentation dataset</article-title><source>Sci. Data</source><year>2022</year><volume>9</volume><fpage>762</fpage><pub-id pub-id-type="pmid">36496501</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41597-022-01875-5</pub-id><pub-id pub-id-type="pmcid">PMC9741583</pub-id></element-citation><mixed-citation id="mc-CR86" publication-type="journal">Hernandez Petzsche, M. R. et al. Isles 2022: a multi-center magnetic resonance imaging stroke lesion segmentation dataset. <italic toggle="yes">Sci. Data</italic><bold>9</bold>, 762 (2022).<pub-id pub-id-type="pmid">36496501</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41597-022-01875-5</pub-id><pub-id pub-id-type="pmcid">PMC9741583</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR87"><label>87.</label><citation-alternatives><element-citation id="ec-CR87" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>He</surname><given-names>Y</given-names></name><etal/></person-group><article-title>Meta grayscale adaptive network for 3D integrated renal structures segmentation</article-title><source>Med. Image Anal.</source><year>2021</year><volume>71</volume><fpage>102055</fpage><pub-id pub-id-type="pmid">33866259</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2021.102055</pub-id></element-citation><mixed-citation id="mc-CR87" publication-type="journal">He, Y. et al. Meta grayscale adaptive network for 3D integrated renal structures segmentation. <italic toggle="yes">Med. Image Anal.</italic><bold>71</bold>, 102055 (2021).<pub-id pub-id-type="pmid">33866259</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2021.102055</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR88"><label>88.</label><citation-alternatives><element-citation id="ec-CR88" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>L</given-names></name><name name-style="western"><surname>Zimmer</surname><given-names>VA</given-names></name><name name-style="western"><surname>Schnabel</surname><given-names>JA</given-names></name><name name-style="western"><surname>Zhuang</surname><given-names>X</given-names></name></person-group><article-title>Atrialjsqnet: a new framework for joint segmentation and quantification of left atrium and scars incorporating spatial and shape information</article-title><source>Med. Image Anal.</source><year>2022</year><volume>76</volume><fpage>102303</fpage><pub-id pub-id-type="pmid">34875581</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2021.102303</pub-id></element-citation><mixed-citation id="mc-CR88" publication-type="journal">Li, L., Zimmer, V. A., Schnabel, J. A. &amp; Zhuang, X. Atrialjsqnet: a new framework for joint segmentation and quantification of left atrium and scars incorporating spatial and shape information. <italic toggle="yes">Med. Image Anal.</italic><bold>76</bold>, 102303 (2022).<pub-id pub-id-type="pmid">34875581</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2021.102303</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR89"><label>89.</label><citation-alternatives><element-citation id="ec-CR89" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pedrosa</surname><given-names>J</given-names></name><etal/></person-group><article-title>Lndb challenge on automatic lung cancer patient management</article-title><source>Med. Image Anal.</source><year>2021</year><volume>70</volume><fpage>102027</fpage><pub-id pub-id-type="pmid">33740739</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2021.102027</pub-id></element-citation><mixed-citation id="mc-CR89" publication-type="journal">Pedrosa, J. et al. Lndb challenge on automatic lung cancer patient management. <italic toggle="yes">Med. Image Anal.</italic><bold>70</bold>, 102027 (2021).<pub-id pub-id-type="pmid">33740739</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2021.102027</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR90"><label>90.</label><citation-alternatives><element-citation id="ec-CR90" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Setio</surname><given-names>AAA</given-names></name><etal/></person-group><article-title>Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the luna16 challenge</article-title><source>Med. Image Anal.</source><year>2017</year><volume>42</volume><fpage>1</fpage><lpage>13</lpage><pub-id pub-id-type="pmid">28732268</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2017.06.015</pub-id></element-citation><mixed-citation id="mc-CR90" publication-type="journal">Setio, A. A. A. et al. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the luna16 challenge. <italic toggle="yes">Med. Image Anal.</italic><bold>42</bold>, 1&#8211;13 (2017).<pub-id pub-id-type="pmid">28732268</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2017.06.015</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR91"><label>91.</label><citation-alternatives><element-citation id="ec-CR91" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhuang</surname><given-names>X</given-names></name><name name-style="western"><surname>Shen</surname><given-names>J</given-names></name></person-group><article-title>Multi-scale patch and multi-modality atlases for whole heart segmentation of MRI</article-title><source>Med. Image Anal.</source><year>2016</year><volume>31</volume><fpage>77</fpage><lpage>87</lpage><pub-id pub-id-type="pmid">26999615</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2016.02.006</pub-id></element-citation><mixed-citation id="mc-CR91" publication-type="journal">Zhuang, X. &amp; Shen, J. Multi-scale patch and multi-modality atlases for whole heart segmentation of MRI. <italic toggle="yes">Med. Image Anal.</italic><bold>31</bold>, 77&#8211;87 (2016).<pub-id pub-id-type="pmid">26999615</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2016.02.006</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR92"><label>92.</label><citation-alternatives><element-citation id="ec-CR92" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Qiu</surname><given-names>J</given-names></name><etal/></person-group><article-title>Myops-net: myocardial pathology segmentation with flexible combination of multi-sequence CMR images</article-title><source>Med. Image Anal.</source><year>2023</year><volume>84</volume><fpage>102694</fpage><pub-id pub-id-type="pmid">36495601</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2022.102694</pub-id></element-citation><mixed-citation id="mc-CR92" publication-type="journal">Qiu, J. et al. Myops-net: myocardial pathology segmentation with flexible combination of multi-sequence CMR images. <italic toggle="yes">Med. Image Anal.</italic><bold>84</bold>, 102694 (2023).<pub-id pub-id-type="pmid">36495601</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2022.102694</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR93"><label>93.</label><citation-alternatives><element-citation id="ec-CR93" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bakr</surname><given-names>S</given-names></name><etal/></person-group><article-title>A radiogenomic dataset of non-small cell lung cancer</article-title><source>Sci. Data</source><year>2018</year><volume>5</volume><fpage>1</fpage><lpage>9</lpage><pub-id pub-id-type="pmid">30325352</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/sdata.2018.202</pub-id><pub-id pub-id-type="pmcid">PMC6190740</pub-id></element-citation><mixed-citation id="mc-CR93" publication-type="journal">Bakr, S. et al. A radiogenomic dataset of non-small cell lung cancer. <italic toggle="yes">Sci. Data</italic><bold>5</bold>, 1&#8211;9 (2018).<pub-id pub-id-type="pmid">30325352</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/sdata.2018.202</pub-id><pub-id pub-id-type="pmcid">PMC6190740</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR94"><label>94.</label><mixed-citation publication-type="other">Roth, H. R. et al. Deeporgan: multi-level deep convolutional networks for automated pancreas segmentation. In <italic toggle="yes">Medical Image Computing and Computer-Assisted Intervention&#8211;MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part I 18</italic>, 556&#8211;564 (Springer, 2015).</mixed-citation></ref><ref id="CR95"><label>95.</label><mixed-citation publication-type="other">Luo, G. et al. Efficient automatic segmentation for multi-level pulmonary arteries: the Parse challenge. Preprint at 10.48550/arXiv.2304.03708 (2023).</mixed-citation></ref><ref id="CR96"><label>96.</label><citation-alternatives><element-citation id="ec-CR96" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Raudaschl</surname><given-names>PF</given-names></name><etal/></person-group><article-title>Evaluation of segmentation methods on head and neck CT: auto-segmentation challenge 2015</article-title><source>Med. Phys.</source><year>2017</year><volume>44</volume><fpage>2020</fpage><lpage>2036</lpage><pub-id pub-id-type="pmid">28273355</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/mp.12197</pub-id></element-citation><mixed-citation id="mc-CR96" publication-type="journal">Raudaschl, P. F. et al. Evaluation of segmentation methods on head and neck CT: auto-segmentation challenge 2015. <italic toggle="yes">Med. Phys.</italic><bold>44</bold>, 2020&#8211;2036 (2017).<pub-id pub-id-type="pmid">28273355</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1002/mp.12197</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR97"><label>97.</label><citation-alternatives><element-citation id="ec-CR97" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Litjens</surname><given-names>G</given-names></name><etal/></person-group><article-title>Evaluation of prostate segmentation algorithms for MRI: the promise12 challenge</article-title><source>Med. Image Anal.</source><year>2014</year><volume>18</volume><fpage>359</fpage><lpage>373</lpage><pub-id pub-id-type="pmid">24418598</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2013.12.002</pub-id><pub-id pub-id-type="pmcid">PMC4137968</pub-id></element-citation><mixed-citation id="mc-CR97" publication-type="journal">Litjens, G. et al. Evaluation of prostate segmentation algorithms for MRI: the promise12 challenge. <italic toggle="yes">Med. Image Anal.</italic><bold>18</bold>, 359&#8211;373 (2014).<pub-id pub-id-type="pmid">24418598</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2013.12.002</pub-id><pub-id pub-id-type="pmcid">PMC4137968</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR98"><label>98.</label><citation-alternatives><element-citation id="ec-CR98" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Radl</surname><given-names>L</given-names></name><etal/></person-group><article-title>Avt: Multicenter aortic vessel tree cta dataset collection with ground truth segmentation masks</article-title><source>Data Brief</source><year>2022</year><volume>40</volume><fpage>107801</fpage><pub-id pub-id-type="pmid">35059483</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.dib.2022.107801</pub-id><pub-id pub-id-type="pmcid">PMC8760499</pub-id></element-citation><mixed-citation id="mc-CR98" publication-type="journal">Radl, L. et al. Avt: Multicenter aortic vessel tree cta dataset collection with ground truth segmentation masks. <italic toggle="yes">Data Brief</italic><bold>40</bold>, 107801 (2022).<pub-id pub-id-type="pmid">35059483</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.dib.2022.107801</pub-id><pub-id pub-id-type="pmcid">PMC8760499</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR99"><label>99.</label><mixed-citation publication-type="other">Luo, X. et al. Segrap2023: a benchmark of organs-at-risk and gross tumor volume segmentation for radiotherapy planning of nasopharyngeal carcinoma. <italic toggle="yes">Med. Image Anal.</italic><bold>101</bold>, 103447 (2025).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2024.103447</pub-id><pub-id pub-id-type="pmid">39756265</pub-id></mixed-citation></ref><ref id="CR100"><label>100.</label><mixed-citation publication-type="other">Lambert, Z., Petitjean, C., Dubray, B. &amp; Kuan, S. Segthor: segmentation of thoracic organs at risk in CT images. In <italic toggle="yes">10th International Conference on Image Processing Theory, Tools and Applications (IPTA)</italic> 1&#8211;6 (IEEE, 2020).</mixed-citation></ref><ref id="CR101"><label>101.</label><mixed-citation publication-type="other">Heimann, T., Morrison, B. J., Styner, M. A. et al. Segmentation of knee images: a grand challenge. In <italic toggle="yes">Proc. MICCAI Workshop on Medical Image Analysis for the Clinic.</italic><bold>1</bold> (2010).</mixed-citation></ref><ref id="CR102"><label>102.</label><citation-alternatives><element-citation id="ec-CR102" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Heimann</surname><given-names>T</given-names></name><etal/></person-group><article-title>Comparison and evaluation of methods for liver segmentation from CT datasets</article-title><source>IEEE Trans. Med. Imaging</source><year>2009</year><volume>28</volume><fpage>1251</fpage><lpage>1265</lpage><pub-id pub-id-type="pmid">19211338</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2009.2013851</pub-id></element-citation><mixed-citation id="mc-CR102" publication-type="journal">Heimann, T. et al. Comparison and evaluation of methods for liver segmentation from CT datasets. <italic toggle="yes">IEEE Trans. Med. Imaging</italic><bold>28</bold>, 1251&#8211;1265 (2009).<pub-id pub-id-type="pmid">19211338</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2009.2013851</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR103"><label>103.</label><citation-alternatives><element-citation id="ec-CR103" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cipriano</surname><given-names>M</given-names></name><etal/></person-group><article-title>Deep segmentation of the mandibular canal: a new 3d annotated dataset of CBCT volumes</article-title><source>IEEE Access</source><year>2022</year><volume>10</volume><fpage>11500</fpage><lpage>11510</lpage></element-citation><mixed-citation id="mc-CR103" publication-type="journal">Cipriano, M. et al. Deep segmentation of the mandibular canal: a new 3d annotated dataset of CBCT volumes. <italic toggle="yes">IEEE Access</italic><bold>10</bold>, 11500&#8211;11510 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR104"><label>104.</label><citation-alternatives><element-citation id="ec-CR104" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sekuboyina</surname><given-names>A</given-names></name><etal/></person-group><article-title>Verse: a vertebrae labelling and segmentation benchmark for multi-detector CT images</article-title><source>Med. Image Anal.</source><year>2021</year><volume>73</volume><fpage>102166</fpage><pub-id pub-id-type="pmid">34340104</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2021.102166</pub-id></element-citation><mixed-citation id="mc-CR104" publication-type="journal">Sekuboyina, A. et al. Verse: a vertebrae labelling and segmentation benchmark for multi-detector CT images. <italic toggle="yes">Med. Image Anal.</italic><bold>73</bold>, 102166 (2021).<pub-id pub-id-type="pmid">34340104</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2021.102166</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR105"><label>105.</label><citation-alternatives><element-citation id="ec-CR105" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kuijf</surname><given-names>HJ</given-names></name><etal/></person-group><article-title>Standardized assessment of automatic segmentation of white matter hyperintensities and results of the WMH segmentation challenge</article-title><source>IEEE Trans. Med. Imaging</source><year>2019</year><volume>38</volume><fpage>2556</fpage><lpage>2568</lpage><pub-id pub-id-type="pmid">30908194</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2019.2905770</pub-id><pub-id pub-id-type="pmcid">PMC7590957</pub-id></element-citation><mixed-citation id="mc-CR105" publication-type="journal">Kuijf, H. J. et al. Standardized assessment of automatic segmentation of white matter hyperintensities and results of the WMH segmentation challenge. <italic toggle="yes">IEEE Trans. Med. Imaging</italic><bold>38</bold>, 2556&#8211;2568 (2019).<pub-id pub-id-type="pmid">30908194</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TMI.2019.2905770</pub-id><pub-id pub-id-type="pmcid">PMC7590957</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR106"><label>106.</label><citation-alternatives><element-citation id="ec-CR106" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luo</surname><given-names>X</given-names></name><etal/></person-group><article-title>Word: a large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image</article-title><source>Med. Image Anal.</source><year>2022</year><volume>82</volume><fpage>102642</fpage><lpage>102642</lpage><pub-id pub-id-type="pmid">36223682</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2022.102642</pub-id></element-citation><mixed-citation id="mc-CR106" publication-type="journal">Luo, X. et al. Word: a large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image. <italic toggle="yes">Med. Image Anal.</italic><bold>82</bold>, 102642&#8211;102642 (2022).<pub-id pub-id-type="pmid">36223682</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.media.2022.102642</pub-id></mixed-citation></citation-alternatives></ref></ref-list></back></article>
        
    </metadata>
</record>
    </GetRecord>

</OAI-PMH>