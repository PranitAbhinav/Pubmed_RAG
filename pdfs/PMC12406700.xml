


<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
    <responseDate>2025-09-09T14:03:51Z</responseDate>
    <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:12406700" metadataPrefix="pmc">https://pmc.ncbi.nlm.nih.gov/api/oai/v1/mh/</request>
    
    <GetRecord>
        <record>
    <header>
    <identifier>oai:pubmedcentral.nih.gov:12406700</identifier>
    <datestamp>2025-09-04</datestamp>
    
        
        <setSpec>bib</setSpec>
        
    
        
        <setSpec>pmc-open</setSpec>
        
    
</header>
    <metadata>
        
        <article xmlns="https://jats.nlm.nih.gov/ns/archiving/1.4/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.4/ https://jats.nlm.nih.gov/archiving/1.4/xsd/JATS-archivearticle1-4.xsd" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Brief Bioinform</journal-id><journal-id journal-id-type="iso-abbrev">Brief Bioinform</journal-id><journal-id journal-id-type="pmc-domain-id">721</journal-id><journal-id journal-id-type="pmc-domain">bib</journal-id><journal-id journal-id-type="publisher-id">bib</journal-id><journal-title-group><journal-title>Briefings in Bioinformatics</journal-title></journal-title-group><issn pub-type="ppub">1467-5463</issn><issn pub-type="epub">1477-4054</issn><publisher><publisher-name>Oxford University Press</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12406700</article-id><article-id pub-id-type="pmcid-ver">PMC12406700.1</article-id><article-id pub-id-type="pmcaid">12406700</article-id><article-id pub-id-type="pmcaiid">12406700</article-id><article-id pub-id-type="pmid">40900115</article-id><article-id pub-id-type="doi">10.1093/bib/bbaf454</article-id><article-id pub-id-type="publisher-id">bbaf454</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Review</subject></subj-group><subj-group subj-group-type="category-taxonomy-collection"><subject>AcademicSubjects/SCI01060</subject></subj-group></article-categories><title-group><article-title>Beyond rigid docking: deep learning approaches for fully flexible protein&#8211;ligand interactions</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name name-style="western"><surname>Lee</surname><given-names initials="J">John</given-names></name><aff>
<institution>Bioinformatics Center</institution>, Institute for Chemical Research, Kyoto University, Uji 611-0011, <country country="JP">Japan</country></aff><xref rid="cor1" ref-type="corresp"/></contrib><contrib contrib-type="author"><name name-style="western"><surname>Hao Nguyen</surname><given-names initials="C">Canh</given-names></name><aff>
<institution>Bioinformatics Center</institution>, Institute for Chemical Research, Kyoto University, Uji 611-0011, <country country="JP">Japan</country></aff></contrib><contrib contrib-type="author"><name name-style="western"><surname>Mamitsuka</surname><given-names initials="H">Hiroshi</given-names></name><aff>
<institution>Bioinformatics Center</institution>, Institute for Chemical Research, Kyoto University, Uji 611-0011, <country country="JP">Japan</country></aff></contrib></contrib-group><author-notes><corresp id="cor1">Corresponding author. Bioinformatics Center, Institute for Chemical Research, Kyoto University, Uji 611-0011, Japan. E-mail: <email>john.c.lee@kuicr.kyoto-u.ac.jp</email></corresp></author-notes><pub-date pub-type="collection"><month>9</month><year>2025</year></pub-date><pub-date pub-type="epub" iso-8601-date="2025-09-03"><day>03</day><month>9</month><year>2025</year></pub-date><volume>26</volume><issue>5</issue><issue-id pub-id-type="pmc-issue-id">495954</issue-id><elocation-id>bbaf454</elocation-id><history><date date-type="received"><day>12</day><month>5</month><year>2025</year></date><date date-type="rev-recd"><day>28</day><month>7</month><year>2025</year></date><date date-type="accepted"><day>29</day><month>7</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>03</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-04 10:25:45.023"><day>04</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025. Published by Oxford University Press.</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an Open Access article distributed under the terms of the Creative Commons Attribution License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by/4.0/">https://creativecommons.org/licenses/by/4.0/</ext-link>), which permits unrestricted reuse, distribution, and reproduction in any medium, provided the original work is properly cited.</license-p></license></permissions><self-uri content-type="pmc-pdf" xlink:href="bbaf454.pdf"/><self-uri xlink:href="bbaf454.pdf"/><abstract><title>Abstract</title><p>Sparked by AlphaFold2&#8217;s groundbreaking success in protein structure prediction, recent years have seen a surge of interest in developing deep learning (DL) models for molecular docking. Molecular docking is a computational approach for predicting how proteins interact with small molecules known as ligands. It has become an essential tool in drug discovery, enabling structure-based virtual screening (VS) methods to efficiently explore vast libraries of drug-like molecules and identify potential therapeutic candidates. However, traditional docking methods primarily rely on search-and-score algorithms, which are computationally demanding. To be viable for VS applications, these methods often sacrifice accuracy for speed by simplifying their search algorithms and scoring functions. Recent advancements in DL have transformed molecular docking, offering accuracy that rivals&#8212;or even surpasses&#8212;traditional approaches while significantly reducing computational costs. Despite these advancements, DL-based molecular docking still faces major challenges. DL models often struggle to generalize beyond their training data and frequently mispredict key molecular properties, such as stereochemistry, bond lengths, and steric interactions, leading to physically unrealistic predictions. To overcome these limitations, a new generation of models is using DL to incorporate protein flexibility into docking predictions, aiming to more accurately capture the dynamic nature of biomolecular interactions&#8212;a long-standing challenge for traditional methods. This review explores how DL has reshaped molecular docking, examines its current shortcomings, and highlights emerging solutions. Finally, we discuss future opportunities to further bridge the gap between computational predictions and real-world molecular interactions.</p></abstract><kwd-group><kwd>flexible docking</kwd><kwd>molecular docking</kwd><kwd>protein&#8211;ligand interaction</kwd><kwd>diffusion models</kwd><kwd>co-folding</kwd></kwd-group><funding-group><award-group award-type="grant"><funding-source><institution-wrap><institution>KAKENHI</institution><institution-id institution-id-type="DOI">10.13039/501100001691</institution-id></institution-wrap></funding-source><award-id>22K12150</award-id></award-group><award-group award-type="grant"><funding-source><institution-wrap><institution>MEXT KAKENHI</institution></institution-wrap></funding-source><award-id>21H05027, 22H03645, 25H01144</award-id></award-group></funding-group><counts><page-count count="15"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec><title>Introduction</title><p>The development of novel therapeutics is a lengthy and costly endeavor, typically spanning 12&#8211;15 years with costs exceeding $1 billion USD [<xref rid="ref1" ref-type="bibr">1</xref>]. A cornerstone of drug discovery relies on identifying and designing ligands that target key proteins involved in disease pathways. In an effort to accelerate this process, computational techniques such as structure-based virtual screening (VS) have emerged as powerful tools, allowing researchers to evaluate large libraries of drug-like molecules <italic toggle="yes">in silico</italic>. VS has become increasingly popular in the field of drug discovery as <italic toggle="yes">in silico</italic> techniques are much faster and cheaper than traditional <italic toggle="yes">in vitro</italic> approaches. Additionally, new computational methods are continuously being developed and improved for accuracy, speed, and reliability [<xref rid="ref2" ref-type="bibr">2</xref>]. Molecular docking is a key component of VS; it predicts the binding conformations and affinities of protein&#8211;ligand complexes, making it the primary approach when the 3D-structure of a target protein is available [<xref rid="ref3" ref-type="bibr">3</xref>]. As advances in structural biology now allow for the rapid and accurate generation of 3D protein structures [<xref rid="ref4" ref-type="bibr">4</xref>], further development of molecular docking tools has become increasingly important.</p><p>Traditional docking approaches, first introduced in the 1980s [<xref rid="ref3" ref-type="bibr">3</xref>, <xref rid="ref5" ref-type="bibr">5</xref>], primarily follow a search-and-score framework, exploring a vast space of possible ligand poses and predicting optimal binding conformations based on scoring functions that estimate protein&#8211;ligand binding strength. These methods are often computationally demanding due to the high dimensionality of the conformational space for both the ligand and the protein. Early methods addressed this challenge by treating both the ligand and protein as rigid bodies, reducing the degrees of freedom to six (three translational and three rotational) [<xref rid="ref3" ref-type="bibr">3</xref>, <xref rid="ref6" ref-type="bibr">6</xref>]. Although this significantly improved computational efficiency, the rigid docking assumption oversimplifies the binding process since, in reality, both ligands and proteins undergo dynamic conformational changes upon interaction. Consequently, these early models often perform poorly in many cases and fail to generalize across different docking tasks, making them less suitable for large-scale VS. To balance computational efficiency with accuracy, most modern molecular docking approaches allow ligand flexibility while keeping the protein rigid [<xref rid="ref7" ref-type="bibr">7</xref>, <xref rid="ref8" ref-type="bibr">8</xref>]. However, modeling receptor flexibility remains crucial for accurately and reliably predicting ligand binding, yet it remains a challenge for traditional methods. This difficulty arises from the exponential growth of the search space and the limitations of conventional scoring algorithms, which are not designed to accommodate protein flexibility.</p><p>In recent years, deep learning (DL) models for predicting protein&#8211;ligand binding structure (see <xref rid="f1" ref-type="fig">Fig. 1</xref>) have rapidly transformed the field of molecular docking. In 2022, St&#228;rk <italic toggle="yes">et&#160;al</italic>. [<xref rid="ref9" ref-type="bibr">9</xref>] developed EquiBind, an equivariant graph neural network (EGNN) based approach for predicting the 3D structure of protein&#8211;ligand complexes. Their method utilized an EGNN to identify &#8220;key points&#8221; on both the ligand and protein, then used the Kabsch algorithm to find the optimal rotation matrix that minimizes the root mean squared deviation (RMSD) between the two sets of key points. This approach was quickly followed by TankBind [<xref rid="ref10" ref-type="bibr">10</xref>], which instead opted to use a trigonometry-aware GNN method to predict a distance matrix between protein residues and ligand atoms, as well as a binding affinity score. They then deployed a multi-dimensional scaling method to reconstruct the 3D structure of the protein&#8211;ligand complex from the distance matrix. Although transformative, these early DL based models had clear limitations&#8212;most notably, they failed to outperform classical docking methods [<xref rid="ref11" ref-type="bibr">11</xref>], and often predicted physically implausible complexes, i.e. improper bond angles and lengths.</p><fig position="float" id="f1" orientation="portrait"><label>Figure 1</label><caption><p>Predicting protein&#8211;ligand complexes from protein and ligand pairs using deep learning. Figure adapted from [<xref rid="ref12" ref-type="bibr">12</xref>].</p></caption><graphic position="float" orientation="portrait" xlink:href="bbaf454f1.jpg"/></fig><p>To address these challenges, Corso <italic toggle="yes">et&#160;al</italic>. [<xref rid="ref13" ref-type="bibr">13</xref>] developed DiffDock, which introduced diffusion models to molecular docking. To train their model, Corso <italic toggle="yes">et&#160;al</italic>. [<xref rid="ref13" ref-type="bibr">13</xref>] used experimentally determined protein&#8211;ligand complexes from PDBBind, and progressively added noise to the ligand&#8217;s degrees of freedom (translation, rotation, and torsion angles). An SE(3)-EGNN then learns a denoising score function to iteratively refine the ligand&#8217;s pose back to a plausible binding configuration. After training, the model predicts complex structures for previously unseen protein&#8211;ligand inputs. Corso <italic toggle="yes">et&#160;al</italic>. [<xref rid="ref13" ref-type="bibr">13</xref>] demonstrated that DiffDock achieved state-of-the-art accuracy on a PDBBind test set, while operating at a fraction of the computational cost compared with traditional methods. Additionally, DiffDock predicted more physically plausible structures when compared to previous DL methods (i.e. EquiBind and TankBind).</p><p>Early DL docking methods, such as EquiBind and DiffDock, drew criticism for being unfairly compared to traditional approaches. Unlike conventional methods that assume the binding pocket is known, these models perform blind docking&#8212;a harder task for which classical algorithms are not designed for. To enable fairer comparison, Yu <italic toggle="yes">et&#160;al</italic>. [<xref rid="ref11" ref-type="bibr">11</xref>] separated blind docking into pocket identification and ligand docking. They found that DL models outperform traditional methods in identifying pockets, but underperform when docking into known pockets. This suggests DL models may focus more on locating binding sites than on accurate pose prediction. A similar issue was noted in the EquiBind study, leading to a hybrid proposal: use DL to predict the binding site, then refine poses with conventional docking. Despite these limitations, DL-based docking remains a promising and rapidly evolving field.</p><p>Before their application in structure prediction of protein&#8211;ligand complexes, machine learning techniques have been widely used to enhance scoring functions in traditional docking. These enhancements enabled faster and more scalable predictions of binding affinities compared to conventional energy-based calculations. Notable graph neural network (GNN) based approaches include IGN [<xref rid="ref14" ref-type="bibr">14</xref>], GIGN [<xref rid="ref15" ref-type="bibr">15</xref>], and PIGNet [<xref rid="ref16" ref-type="bibr">16</xref>]. These methods employ GNNs to predict stability or binding affinity of potential protein&#8211;ligand complexes. They then rank these complexes based on the likelihood of their predicted interactions reflecting true biological interactions. However, since this review focuses on DL methods for structure prediction rather than binding affinity prediction, we do not explore these methods in detail. For a comprehensive overview of recent advancements in DL-based scoring functions, we direct readers to existing review articles [<xref rid="ref17" ref-type="bibr">17</xref>].</p><sec id="sec1"><title>Protein flexibility</title><p>The majority of DL approaches follow in the footsteps of traditional docking methods by only accommodating ligand flexibility while largely treating the protein receptor as rigid. This oversimplification presents significant challenges in real-world scenarios, such as cross-docking, apo-docking, or cases involving computationally predicted protein structures. <xref rid="TB1" ref-type="table">Table 1</xref> summarizes common docking tasks within the field of MD. These challenges arise from the fact that proteins are inherently flexible and can undergo substantial conformational changes upon ligand binding&#8212;a phenomenon known as the <italic toggle="yes">induced fit effect</italic>. As a result, the binding pocket of an apo structure may differ significantly from its ligand-bound (holo) counterpart. Without accounting for these induced fit effects, docking methods trained primarily on holo structures, such as those in PDBBind, struggle to accurately predict binding poses when docking to apo conformations. Addressing this issue has driven recent advances in DL-based docking that incorporate protein flexibility.</p><table-wrap position="float" id="TB1" orientation="portrait"><label>Table 1</label><caption><p>Overview of protein&#8211;ligand docking tasks, which evaluate molecular docking models across varying degrees of structural uncertainty and practical relevance to test their robustness, generalization, and applicability in drug discovery pipelines.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col span="2" align="left"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Docking task</th><th align="left" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">
<bold>Re-docking</bold>
</td><td align="left" rowspan="1" colspan="1">Involves docking a ligand back into the bound (holo) conformation of the receptor. This task evaluates whether a model can recover the original binding pose from an idealized protein&#8211;ligand complex. DL models trained on datasets like PDBBind typically perform well here, but may overfit to the ideal geometries, limiting their ability to generalize to unseen or non-ideal cases.</td></tr><tr><td align="left" rowspan="1" colspan="1">
<bold>Flexible re-docking</bold>
</td><td align="left" rowspan="1" colspan="1">Uses holo structures with randomized binding-site sidechains to introduce local perturbations. Evaluates model robustness to minor conformational changes.</td></tr><tr><td align="left" rowspan="1" colspan="1">
<bold>Cross-docking</bold>
</td><td align="left" rowspan="1" colspan="1">Ligands are docked to alternative receptor conformations (e.g. from different ligand complexes). This is illustrated in <xref rid="f2" ref-type="fig">Fig. 2</xref>. This simulates real-world cases where ligands are docked to proteins in unknown conformational states.</td></tr><tr><td align="left" rowspan="1" colspan="1">
<bold>Apo-docking</bold>
</td><td align="left" rowspan="1" colspan="1">Uses unbound (apo) receptor structures, typically obtained from crystal structures or computational predictions. This is a highly realistic setting for drug discovery, requiring models to infer the induced fit and accommodate structural differences between the unbound and bound states.</td></tr><tr><td align="left" rowspan="1" colspan="1">
<bold>Blind docking</bold>
</td><td align="left" rowspan="1" colspan="1">Requires prediction of both the ligand pose and the binding site location. It is the most challenging and least constrained task. However, it is less common in practical settings where binding sites are often known.</td></tr></tbody></table></table-wrap><fig position="float" id="f2" orientation="portrait"><label>Figure 2</label><caption><p>Visual illustration of re-docking versus cross-docking. Figure adapted from [<xref rid="ref18" ref-type="bibr">18</xref>].</p></caption><graphic position="float" orientation="portrait" xlink:href="bbaf454f2.jpg"/></fig><p>Conformational changes during docking are often small and localized, largely due to ligand size and energetic constraint. A key component of flexible docking methods is the ability to predict how an unbound (apo) protein undergoes conformational changes to adopt its bound (holo) state upon ligand binding. Because conformational changes tend to occur near the binding site, accurately capturing sidechain flexibility is especially important in docking, as it directly impacts pose prediction performance [<xref rid="ref19" ref-type="bibr">19</xref>]. Early diffusion-based methods, such as DiffDock, partially accounted for protein flexibility by coarsely representing protein structures, allowing small conformational adjustments at the residue level within the binding pocket. While this reduced sensitivity to atomic misplacements and mitigated the need for exhaustive sampling, it still treated flexibility in an indirect manner, limiting its ability to handle significant conformational rearrangements.</p><p>Building on these efforts, Chen <italic toggle="yes">et&#160;al</italic>. [<xref rid="ref20" ref-type="bibr">20</xref>] developed FlexPose, enabling end-to-end flexible modeling of the 3D structure of protein&#8211;ligand complexes irrespective of input protein conformation (apo or holo). Similarly, aligned diffusion Schr&#246;dinger Bridges [<xref rid="ref21" ref-type="bibr">21</xref>] have been proposed as a method to predict conformational transitions between apo and holo states by treating them as paired data, offering a novel framework for modeling protein flexibility. However, this method has yet to be evaluated for flexible protein&#8211;ligand docking, leaving it as an open avenue for future research.</p><p>Another emerging direction focuses on identifying cryptic pockets&#8212;transient binding sites hidden in static structures but revealed through protein dynamics. Methods like DynamicBind [<xref rid="ref22" ref-type="bibr">22</xref>] are capable of revealing these cryptic pockets by using equivariant geometric diffusion networks to model protein backbone and sidechain flexibility. Notably, they do so at a fraction of the computational cost of traditional molecular dynamics simulations and turn previously &#8220;undruggable&#8221; proteins into potential drug targets. In a compelling case study involving the histone methyltransferase SETD2, DynamicBind accurately predicted the binding pose of EZM0414, a selective inhibitor currently in Phase I clinical trials, targeting a cryptic pocket that was unseen in any training structure. The predicted pose achieved a ligand RMSD of 1.4 &#197;, despite the absence of similar ligands or pocket conformations in training data. These results underscore the promise of flexibility-aware DL methods for discovering novel druggable sites and improving docking in challenging settings like cross-docking and apo-docking.</p><p>By modeling protein flexibility, these approaches move beyond the rigid-receptor paradigm, enabling more realistic and robust docking predictions. This review provides a comprehensive overview of emerging models and techniques that account for protein flexibility, serving as a valuable resource for researchers exploring the latest advances at the intersection of DL and molecular docking.</p></sec></sec><sec><title>Criticisms</title><p>In 2023, early DL methods like EquiBind, TankBind, and DiffDock underwent further testing to assess their reported performance. Two key criticisms emerged:
</p><list list-type="bullet"><list-item id="item1"><p>Overfitting to training datasets that lead to an inability to generalize.</p></list-item><list-item id="item2"><p>A tendency to generate physically implausible structures.</p></list-item></list><sec id="sec2"><title>PoseBusters and the problem of generalizability</title><p>Many DL docking models&#8212;including DiffDock, DeepDock, EquiBind, TankBind, and Uni-Mol&#8212;are trained on datasets composed of experimentally resolved protein&#8211;ligand complexes, most commonly PDBBind. However, because PDBBind exclusively comprises experimentally determined structures, its composition is inherently biased toward protein families with extensive structural characterization. For instance, kinases constitute a disproportionately large portion of the dataset, whereas G protein&#8211;coupled receptors (GPCRs) and ion channels are underrepresented, with the latter accounting for &#161;5% of entries. This underrepresentation arises from experimental challenges in resolving the 3D structures of both protein families. Moreover, PDBBind suffers from significant data leakage due to high sequence and chemical similarity between training and test sets, especially within the commonly used &#8220;core set,&#8221; which contains smaller ligands and better-resolved structures but overlaps heavily with training data. This issue is also evident in benchmarks like the Astex Diverse set, where a significant proportion of test complexes resemble those seen during training. This inflates performance metrics and masks poor generalization to novel targets.</p><p>To quantify and highlight the impact of such similarity-induced bias, some studies [<xref rid="ref23" ref-type="bibr">23</xref>] have adopted sequence-based and ligand scaffold-based splits, which reveal a substantial drop in performance, confirming that similarity biases strongly affect model predictions. Similarly, time-based splits have been proposed as a practical compromise to mitigate leakage [<xref rid="ref9" ref-type="bibr">9</xref>, <xref rid="ref24" ref-type="bibr">24</xref>]. In this approach, models are trained only on complexes released before a cutoff year (e.g. 2019) and evaluated on newer entries (after 2019). While this mimics real-world deployment more closely and helps reduce overlap, it does not fully resolve the problem as newly tested compounds often target previously studied proteins, and vice versa, resulting in persistent similarity between training and test sets.</p><p>The performance of these models further declines when evaluated on deliberately curated, out-of-distribution datasets. The PoseBusters benchmark set [<xref rid="ref25" ref-type="bibr">25</xref>], for example, was designed to assess the generalization capabilities of DL models, includes protein&#8211;ligand complexes released after 2021 and ensuring they are absent from standard training sets like PDBBind 2020. This benchmark revealed that all tested DL models produced higher RMSD values on average, highlighting their struggle to generalize beyond their training data. In contrast, classical docking approaches, such as AutoDock Vina and GOLD, demonstrated more consistent performance across standard evaluation metrics.</p><p>These findings suggest that while DL-based methods can capture certain geometric features under favorable conditions, they lack the inductive biases necessary for robust generalization across the broader chemical and biological space encountered in real-world applications.</p></sec><sec id="sec3"><title>The problem of physically implausible predictions</title><p>Beyond generalization issues, DL-based docking methods often generate physically implausible ligand poses that look accurate based on RMSD scores but break basic physical and chemical rules. The widespread reliance on RMSD as a primary evaluation metric has masked these deficiencies, as a low RMSD relative to a crystallographic pose does not guarantee physical plausibility. <xref rid="TB2" ref-type="table">Table 2</xref> summarizes some commonly used metrics for evaluating protein&#8211;ligand docking models.</p><table-wrap position="float" id="TB2" orientation="portrait"><label>Table 2</label><caption><p>Summary of protein&#8211;ligand docking evaluation metrics.</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col span="2" align="left"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Metric</th><th align="left" rowspan="1" colspan="1">Description</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">RMSD <inline-formula><tex-math notation="LaTeX" id="ImEquation1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\leq $\end{document}</tex-math></inline-formula> 2 &#197;</td><td align="left" rowspan="1" colspan="1">Measures the average distance between atoms of the predicted and experimental ligand poses. An RMSD of 2 &#197;or less is typically considered a successful prediction. Models are typically assessed on the percentage of predicted structures which have RMSD <inline-formula><tex-math notation="LaTeX" id="ImEquation2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\leq $\end{document}</tex-math></inline-formula> 2 &#197;.</td></tr><tr><td align="left" rowspan="1" colspan="1">RMSD <inline-formula><tex-math notation="LaTeX" id="ImEquation3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\leq $\end{document}</tex-math></inline-formula> 2 &#197;&amp; PB-Valid</td><td align="left" rowspan="1" colspan="1">Combines the RMSD threshold with validation checks from the PoseBusters (PB) software suite. A pose is successful if it has an RMSD <inline-formula><tex-math notation="LaTeX" id="ImEquation4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\leq $\end{document}</tex-math></inline-formula> 2 &#197;and passes PB-Valid checks, which include assessments of chemical and physical plausibility.</td></tr><tr><td align="left" rowspan="1" colspan="1">lDDT-PLI</td><td align="left" rowspan="1" colspan="1">Evaluates the accuracy of predicted protein&#8211;ligand interactions by comparing the local atomic environments of the predicted and experimental structures. Higher lDDT-PLI scores indicate better agreement with the reference, focusing on the quality of the binding interface.</td></tr><tr><td align="left" rowspan="1" colspan="1">PLIF-EMD</td><td align="left" rowspan="1" colspan="1">Assesses how well the predicted protein&#8211;ligand interaction patterns match the experimental data by calculating the Earth Mover&#8217;s Distance between predicted and actual interaction fingerprints. Lower PLIF-EMD values indicate a closer match to the native interaction patterns.</td></tr><tr><td align="left" rowspan="1" colspan="1">BiSyRMSD</td><td align="left" rowspan="1" colspan="1">Measures the absolute deviation of a predicted ligand pose from the experimentally determined structure, focusing on the binding site and accounting for any symmetry in the ligand to provide a more precise assessment of docking accuracy.</td></tr></tbody></table></table-wrap><p>Benchmark studies such as PoseBusters [<xref rid="ref25" ref-type="bibr">25</xref>] found that over 50% of ligand poses generated by DL models failed stringent validity checks, exhibiting critical errors such as incorrect stereochemistry, unrealistic bond lengths and angles, and severe steric clashes with the protein receptor. These errors introduce significant strain energies, making such predictions unlikely to correspond to biologically relevant binding modes. In contrast, classical docking methods such as AutoDock Vina and CCDC GOLD produced invalid structures in only 2%&#8211;3% of cases, demonstrating a superior ability to enforce physical constraints.</p><p>These findings indicate that early DL docking models often miss important biophysical constraints needed for accurate predictions. In the next sections, we will discuss the architectural innovations that lead to the surge of DL-based approaches for molecular docking before discussing new advancements that attempt to resolve the criticisms mentioned above.</p></sec></sec><sec><title>Architectural innovations</title><p>GNNs have had a profound impact on molecular docking, from their early adoption in DL-based scoring functions like IGN and PIGNET to their central role in structure prediction models such as EquiBind and DiffDock. Their success stems from their ability to process graph-structured data, making them well-suited for modeling proteins, ligands, and their interactions.</p><sec id="sec4"><title>Ligand and protein graph representations</title><p>Ligands are typically modeled as molecular graphs, where nodes represent atoms and edges represent chemical bonds or spatial proximity (e.g. <inline-formula><tex-math notation="LaTeX" id="ImEquation5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$k$\end{document}</tex-math></inline-formula>-nearest neighbor graphs [<xref rid="ref9" ref-type="bibr">9</xref>]). Each node carries a feature vector encoding atomic properties such as element type, valence, and chirality, along with 3D coordinates to preserve spatial context. Edges encode bond-specific features like type, length, and directionality&#8212;often computed using cheminformatics toolkits like RDKit.</p><p>Proteins can also be represented as graphs, generally at one of two resolution levels: atomic (fine-grained) or residue (coarse-grained). Coarse-grained graphs are common in recent models due to their ability to capture higher-order structural features, such as backbone orientation, while significantly reducing graph size and improving computational efficiency. As with ligands, protein graphs include rich node and edge features along with spatial coordinates.</p><p>To model protein&#8211;ligand interactions, DL models construct intermolecular graphs that integrate binding site information. For example, EquiBind connects nodes between the protein graph (<inline-formula><tex-math notation="LaTeX" id="ImEquation6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$G_{p}$\end{document}</tex-math></inline-formula>) and the ligand graph (<inline-formula><tex-math notation="LaTeX" id="ImEquation7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$G_{l}$\end{document}</tex-math></inline-formula>) by edges, enabling the model to identify key binding interactions between the two structures. A more exhaustive approach, KarmaDock [<xref rid="ref26" ref-type="bibr">26</xref>], creates a fully connected intermolecular graph, linking every node in <inline-formula><tex-math notation="LaTeX" id="ImEquation8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$G_{l}$\end{document}</tex-math></inline-formula> to every node in <inline-formula><tex-math notation="LaTeX" id="ImEquation9">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$G_{p}$\end{document}</tex-math></inline-formula>. While comprehensive, this method is computationally expensive due to quadratic edge scaling. To balance accuracy and efficiency, DiffDock employs KNN-based intermolecular graphs, connecting only spatially proximal protein and ligand nodes. This strategy reduces computational complexity while preserving key geometric and chemical constraints for accurate binding predictions.</p></sec><sec id="sec5"><title>Graph neural networks</title><p>In a GNN each node of an input graph is first initialized with a feature vector <inline-formula><tex-math notation="LaTeX" id="ImEquation10">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathbf{h}_{i}^{(0)}$\end{document}</tex-math></inline-formula> which is then updated iteratively through message passing. At each layer <inline-formula><tex-math notation="LaTeX" id="ImEquation11">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$l$\end{document}</tex-math></inline-formula>, every node <inline-formula><tex-math notation="LaTeX" id="ImEquation12">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$v_{i}$\end{document}</tex-math></inline-formula> aggregates messages from its neighbors <inline-formula><tex-math notation="LaTeX" id="ImEquation13">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$N(i)$\end{document}</tex-math></inline-formula> using a learnable message function <inline-formula><tex-math notation="LaTeX" id="ImEquation14">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\psi $\end{document}</tex-math></inline-formula>, which computes </p><disp-formula><tex-math notation="LaTeX" id="DmEquation1">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*} &amp; \mathbf{m}_{i}^{(l)} = \bigoplus_{j \in N(i)} \psi\big(\mathbf{h}_{i}^{(l)}, \mathbf{h}_{j}^{(l)}, \mathbf{e}_{ij}\big), \end{align*}$$\end{document}</tex-math></disp-formula><p>where <inline-formula><tex-math notation="LaTeX" id="ImEquation15">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\bigoplus $\end{document}</tex-math></inline-formula> denotes a permutation-invariant aggregation operation (such as summation or averaging) and <inline-formula><tex-math notation="LaTeX" id="ImEquation16">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\psi $\end{document}</tex-math></inline-formula> includes trainable parameters (e.g. weights of a feedforward neural network). The node then updates its representation via another learnable function <inline-formula><tex-math notation="LaTeX" id="ImEquation17">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\phi $\end{document}</tex-math></inline-formula> such that </p><disp-formula><tex-math notation="LaTeX" id="DmEquation2">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*} &amp; \mathbf{h}_{i}^{(l+1)} = \phi\big(\mathbf{h}_{i}^{(l)}, \mathbf{m}_{i}^{(l)}\big), \end{align*}$$\end{document}</tex-math></disp-formula><p>i.e. the node representation is updated using a learnable function which takes as input the node representation from the previous layer, and the &#8220;message&#8221; from its neighbors. After several message-passing layers, each node&#8217;s representation encodes information from an increasingly large neighborhood, capturing both local and global graph structure. For graph-level tasks, a readout function <inline-formula><tex-math notation="LaTeX" id="ImEquation18">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\rho $\end{document}</tex-math></inline-formula> aggregates the final node embeddings <inline-formula><tex-math notation="LaTeX" id="ImEquation19">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\{\mathbf{h}_{i}^{(L)}\}$\end{document}</tex-math></inline-formula> into a fixed-size representation, </p><disp-formula><tex-math notation="LaTeX" id="DmEquation3">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*} &amp; \mathbf{h}_{G} = \rho\big(\{\mathbf{h}_{i}^{(L)}: i \in V\}\big), \end{align*}$$\end{document}</tex-math></disp-formula><p>which is then fed into further learnable layers to yield the final prediction.</p><sec id="sec5a"><title>Equivariant graph neural networks</title><p>Equivariance is a critical property for models that predict 3D protein&#8211;ligand complex structures. As illustrated in <xref rid="f3" ref-type="fig">Fig. 3</xref>, it ensures that a transformation applied to the input (e.g. a rotation or translation) leads to a predictable transformation of the output. Formally, a neural network <inline-formula><tex-math notation="LaTeX" id="ImEquation20">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$ \mathcal{F} $\end{document}</tex-math></inline-formula> is equivariant to a transformation <inline-formula><tex-math notation="LaTeX" id="ImEquation21">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$ \mathcal{T} $\end{document}</tex-math></inline-formula> if, for any input <inline-formula><tex-math notation="LaTeX" id="ImEquation22">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\mathcal{X}$\end{document}</tex-math></inline-formula> and some corresponding transformation <inline-formula><tex-math notation="LaTeX" id="ImEquation23">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$ \mathcal{T}^{\prime} $\end{document}</tex-math></inline-formula> in the output space, it holds that </p><disp-formula><tex-math notation="LaTeX" id="DmEquation4">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*} &amp; \mathcal{F}(\mathcal{T}(\mathcal{X})) = \mathcal{T}^{\prime}\big(\mathcal{F}(\mathcal{X})\big). \end{align*}$$\end{document}</tex-math></disp-formula><fig position="float" id="f3" orientation="portrait"><label>Figure 3</label><caption><p>Illustration of the equivariance property in neural networks, where transformations applied to the input (e.g. rotation or translation) produce corresponding transformations in the output, ensuring consistent predictions regardless of molecular orientation.</p></caption><graphic position="float" orientation="portrait" xlink:href="bbaf454f3.jpg"/></fig><p>This means that every layer in the network must transform its features in a way that respects the symmetry of the underlying physical system&#8212;typically the special Euclidean group <inline-formula><tex-math notation="LaTeX" id="ImEquation24">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$ \textrm{SE}(3) $\end{document}</tex-math></inline-formula> for 3D space. In molecular docking, such equivariance guarantees that the predicted complex structure remains consistent regardless of arbitrary rotations or translations of the input molecules. For example, <sc>EquiBind</sc>, enforces <inline-formula><tex-math notation="LaTeX" id="ImEquation25">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$ \textrm{SE}(3) $\end{document}</tex-math></inline-formula> equivariance by conditioning its message passing on the invariant squared distance <inline-formula><tex-math notation="LaTeX" id="ImEquation26">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$||\mathbf{x}_{i}^{(l)} - \mathbf{x}_{j}^{(l)}||^{2}$\end{document}</tex-math></inline-formula>, ensuring that messages </p><disp-formula><tex-math notation="LaTeX" id="DmEquation5">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*} &amp; \psi\big(\mathbf{h}_{i}^{(l)}, \mathbf{h}_{j}^{(l)}, \|\mathbf{x}_{i}^{(l)} - \mathbf{x}_{j}^{(l)}\|^{2}\big) \end{align*}$$\end{document}</tex-math></disp-formula><p>remains unchanged under rotations and translations. While this approach is simple and data-efficient&#8212;often outperforming non-equivariant models with less training data&#8212;it may not fully capture richer geometric details. As a result, more advanced methods, such as those used in <sc>DiffDock</sc>, incorporate group-theoretic frameworks by representing node and edge features as irreducible representations via spherical harmonics. These representations are combined through operations like the Clebsch&#8211;Gordon tensor product to rigorously maintain <inline-formula><tex-math notation="LaTeX" id="ImEquation27">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$ \textrm{E}(3) $\end{document}</tex-math></inline-formula> equivariance throughout the network while allowing the model to learn complex, informative geometric relationships. Although EGNNs are generally more challenging to design and scale when compared to their non-equivariant counterparts, practitioners benefit from the <bold>e3nn</bold> [<xref rid="ref27" ref-type="bibr">27</xref>] library&#8212;a widely used PyTorch-based tool that simplifies the implementation and development of EGNNs.</p></sec><sec id="sec5b"><title>Is equivariance all you need?</title><p>Recent studies have questioned whether hard-coded equivariance is essential for effective DL&#8211;based molecular docking [<xref rid="ref28" ref-type="bibr">28&#8211;30</xref>]. With sufficient training data, some argue that it&#8217;s more practical to use scalable architectures that learn symmetries via data augmentation, rather than enforcing them explicitly&#8212;an approach exemplified by models like AlphaFold3 and Chai-1, which prioritize scalability over strict equivariance.</p><p>However, for smaller, specialized models&#8212;such as those focused on protein&#8211;ligand docking&#8212;equivariance remains relevant. Unlike large biomolecular networks, designed for scalability, these models can leverage explicit symmetry constraints to enhance accuracy and generalization, particularly in data-scarce settings. Equivariance ensures rotational and translational consistency without excessive data augmentation, making it a principled approach to enforcing biophysical constraints.</p><p>Despite these advantages, there are practical challenges. Equivariant models often introduce computational overhead and are challenging to scale. A hybrid approach may offer the best balance, combining equivariance to capture fundamental symmetries with data augmentation to model complex, emergent behaviors. Future research comparing these strategies on protein&#8211;ligand interaction datasets will be key to determining the optimal approach.</p></sec></sec><sec id="sec6"><title>Transformers</title><p>Transformer-based models have advanced protein&#8211;ligand interaction prediction by capturing long-range dependencies via self-attention mechanisms. Uni-Mol [<xref rid="ref31" ref-type="bibr">31</xref>] introduced pair-biased attention to jointly encode atom- and pair-level features, predicting ligand atom coordinates from pairwise distance matrices. Building on this, GAABind [<xref rid="ref32" ref-type="bibr">32</xref>] incorporated triangular attention, inspired by AlphaFold2, to better capture the geometric structure of binding pockets.</p><p>Despite their strengths, transformer models typically require large training datasets and rely on separate pose generation and scoring steps, which can lead to high computational costs and physically implausible conformations (e.g. steric clashes). CarsiDock [<xref rid="ref33" ref-type="bibr">33</xref>] addresses these limitations through large-scale pretraining followed by fine-tuning on crystallized complexes to improve generalization.</p><p>More recently, Interformer [<xref rid="ref34" ref-type="bibr">34</xref>] introduced a unified Graph-Transformer architecture that models both local and global interactions using specialized intra- and inter-molecular attention blocks. It incorporates a mixture density network to explicitly model non-covalent interactions (e.g. hydrogen bonds, hydrophobic forces) and uses contrastive learning to improve pose sensitivity, generating more physically realistic docking predictions.</p></sec><sec id="sec7"><title>Diffusion models</title><p>Diffusion models have recently emerged as a powerful tool in computational structural biology, enabling more accurate and scalable modeling of molecular structures. Earlier approaches such as variational autoencoders (VAEs) and generative adversarial networks (GANs) often struggled to handle the geometric constraints of molecules or scale effectively to their high-dimensional representations [<xref rid="ref35" ref-type="bibr">35</xref>]. In contrast, diffusion models offer a robust framework to learn complex structural distributions, leading to significant advances in tasks like protein design, molecular docking [<xref rid="ref24" ref-type="bibr">24</xref>], and biomolecular interaction prediction [<xref rid="ref36" ref-type="bibr">36</xref>].</p><fig position="float" id="f4" orientation="portrait"><label>Figure 4</label><caption><p>Fixed protein diffusion process, where ligands from experimentally determined complexes are progressively noised with random translations, rotations, and torsional perturbations, and a diffusion model is trained to reverse this process to denoise and redock ligands into the correct binding pocket (adapted from [<xref rid="ref24" ref-type="bibr">24</xref>]).</p></caption><graphic position="float" orientation="portrait" xlink:href="bbaf454f4.jpg"/></fig><p>Score-based diffusion models, one of the earliest formulations of diffusion modeling, operate by learning to reverse a stochastic process that incrementally degrades data into noise, as illustrated in <xref rid="f4" ref-type="fig">Fig. 4</xref>. The forward process transforms a data sample <inline-formula><tex-math notation="LaTeX" id="ImEquation28">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$ x_{0} \sim p_{0} $\end{document}</tex-math></inline-formula> into Gaussian noise <inline-formula><tex-math notation="LaTeX" id="ImEquation29">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$ x_\tau \sim p_\tau $\end{document}</tex-math></inline-formula> via a stochastic differential equation (SDE): </p><disp-formula id="deqn01"><label>(1)</label><tex-math notation="LaTeX" id="DmEquation6">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*}&amp; dx = f(x,t) dt + g(t) dB\end{align*}$$\end{document}</tex-math></disp-formula><p>
where <inline-formula><tex-math notation="LaTeX" id="ImEquation30">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$ f(x,t) $\end{document}</tex-math></inline-formula> is the drift term, and <inline-formula><tex-math notation="LaTeX" id="ImEquation31">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$ g(t) dB $\end{document}</tex-math></inline-formula> injects Gaussian noise through Brownian motion. The diffusion model is then trained to reverse this process, by approximating another SDE which transforms Gaussian noise to samples from the data distribution: </p><disp-formula id="deqn02"><label>(2)</label><tex-math notation="LaTeX" id="DmEquation7">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*}&amp; dx = \left[f(x,t) - g(t)^{2} \nabla \log p_{t}(x) \right] dt + g(t) dB\end{align*}$$\end{document}</tex-math></disp-formula><p>
where <inline-formula><tex-math notation="LaTeX" id="ImEquation32">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$ \nabla \log p_{t}(x) $\end{document}</tex-math></inline-formula> (the Stein score) guides samples toward high-probability regions. Since <inline-formula><tex-math notation="LaTeX" id="ImEquation33">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$ p_{t}(x) $\end{document}</tex-math></inline-formula> is intractable, the score function is approximated by a neural network <inline-formula><tex-math notation="LaTeX" id="ImEquation34">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$ s_\theta (x,t) $\end{document}</tex-math></inline-formula>, trained via denoising score matching: </p><disp-formula id="deqn03"><label>(3)</label><tex-math notation="LaTeX" id="DmEquation8">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
\begin{align*}&amp; L = \mathbb{E}_{x_{0} \sim p_{0}, x_{t} \sim p_{t|0}} \left[ \|s_\theta(x,t) - \nabla \log p_{t|0}(x|x_{0})\|^{2} \right]\end{align*}$$\end{document}</tex-math></disp-formula><p>
where <inline-formula><tex-math notation="LaTeX" id="ImEquation35">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$ p_{t|0}(x | x_{0}) $\end{document}</tex-math></inline-formula> is an analytically tractable conditional distribution. Sampling is performed by discretizing the reverse SDE using Euler&#8211;Maruyama integration.</p><sec id="sec7a"><title>Equivariance in diffusion models</title><p>Equivariance is a crucial property in diffusion models applied to molecular systems. Since molecular structures should maintain the same probability distribution regardless of their orientation in 3D space, diffusion models must be invariant to translations and rotations. To ensure translation invariance, molecular structures are zero-centered by subtracting the center of mass during training and inference. Rotation invariance is achieved if the diffusion model&#8217;s score function is equivariant, which can be implemented using an EGNN to approximate the score function. Selecting an appropriate neural network architecture is essential for ensuring that diffusion models generalize well and generate physically meaningful molecular structures.</p></sec><sec id="sec7b"><title>Future of diffusion models</title><p>Researchers are actively exploring robust generalizations to score based diffusion models such as flow matching and Schr&#246;dinger bridges to further enhance diffusion-based generative modeling. Flow Matching, for example, learns a vector field which induces a continuous normalizing flow that transports a source distribution to a target distribution. These advancements balance stochastic and deterministic elements in the modeling process, offering flexible prior distributions without requiring explicit formulations. As the field evolves, diffusion models, and their spin-offs, are poised to remain fundamental tools in structural biology, continually improving our understanding of protein&#8211;ligand interactions.</p><p>In later sections, we discuss how diffusion-based approaches have been utilized to model protein flexibility, leading to improved performance and accuracy on challenging cross-docking tasks and apo docking scenarios. By modeling flexibility, these methods generate more physically realistic binding poses and better generalize to unseen protein conformations.</p></sec></sec></sec><sec><title>Flexible methods</title><p>The evolution of molecular docking programs is characterized by models which fall into three categories:
</p><list list-type="bullet"><list-item id="item3"><p>
<bold>Fully rigid docking</bold>: Both proteins and ligands are treated as rigid structures.</p></list-item><list-item id="item4"><p>
<bold>Ligand flexible methods</bold>: Proteins are assumed to be rigid while ligands are flexible.</p></list-item><list-item id="item5"><p>
<bold>Protein flexible methods</bold>: Both the ligand and protein are flexible.</p></list-item></list><p>In recent years, growing interest in accounting for protein conformational changes induced by ligand docking has lead to the emergence of two promising strategies:
</p><list list-type="bullet"><list-item id="item6"><p>
<bold>Flexible docking</bold> methods model the structural transition from the apo to the holo state of a protein, enabling docking to conformations that better reflect induced-fit effects.</p></list-item><list-item id="item7"><p>
<bold>Co-folding</bold> methods model the structure of protein&#8211;ligand complexes directly from input data (typically sequences) in a single task, inherently modeling induced-fit effects.</p></list-item></list><p>In the following sections, we review recent advancements in protein flexible DL molecular docking methods, evaluating their strengths and limitations.</p><sec id="sec8"><title>Flexible docking</title><p>Flexible docking methods predict the conformational changes proteins undergo upon ligand binding. Some traditional search-based docking methods incorporate protein flexibility by adding sidechain torsions to the search space [<xref rid="ref37" ref-type="bibr">37</xref>]. However, this significantly increases dimensionality, making it challenging to identify optimal binding poses. Nonetheless, the importance of incorporating protein flexibility has been understood for decades. For example, Zavodsky and Kuhn&#8217;s 2005 study [<xref rid="ref38" ref-type="bibr">38</xref>], found that while rigid docking failed in half of the cases, accounting for sidechain flexibility enabled the successful re-docking of all 63 ligands in their study.</p><p>Conformational changes during docking are often small and localized, largely due to ligand size and energetic constraints. As a result, many flexible docking methods focus on sidechain flexibility, though some recent approaches also model backbone flexibility [<xref rid="ref22" ref-type="bibr">22</xref>, <xref rid="ref39" ref-type="bibr">39</xref>, <xref rid="ref40" ref-type="bibr">40</xref>] to capture larger conformal shifts. These methods are typically more scalable than traditional sampling techniques or DL&#8211;based co-folding approaches, which can take minutes per prediction. Flexible docking methods generally fall into two main categories:
</p><list list-type="bullet"><list-item id="item8"><p>
<bold>Implicitly</bold> flexible models account for flexibility by adjusting scoring functions or using coarse representations to partially account for potential conformational changes. By implicitly considering protein flexibility, these methods improve docking accuracy without incurring the computational cost of explicit simulations.</p></list-item><list-item id="item9"><p>
<bold>Explicitly</bold> flexible models directly predict the changes to atomic coordinates of proteins during the docking process. Techniques such as flexible sidechain modeling allow for a more realistic representation of the protein&#8217;s conformational space, leading to more accurate predictions of ligand binding.</p></list-item></list><p>A summary of recent DL-based molecular docking models, including their architectures, handling of protein flexibility, and input representations, can be found in <xref rid="TB3" ref-type="table">Table 3</xref>, and benchmark performance of a handful of these methods can be found in <xref rid="TB4" ref-type="table">Table 4</xref>.</p><table-wrap position="float" id="TB3" orientation="portrait"><label>Table 3</label><caption><p>Prominent deep learning models for molecular docking, categorized by architectural features (e.g. GNNs, diffusion models, transformers, equivariant mechanisms), treatment of protein flexibility (fixed, implicit, explicit), and input representations (sequence-based, structure-based, hybrid).</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col span="2" align="left"/><col span="3" align="left"/><col span="4" align="left"/><col span="5" align="left"/><col span="6" align="left"/><col span="7" align="left"/><col span="8" align="left"/><col span="9" align="left"/><col span="10" align="left"/><col span="11" align="left"/><col span="12" align="left"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Models</th><th colspan="4" align="left" rowspan="1">Architecture</th><th colspan="4" align="left" rowspan="1">Protein Flexibility</th><th colspan="3" align="left" rowspan="1">Input Representation</th></tr><tr><td rowspan="1" colspan="1"/><th align="left" rowspan="1" colspan="1">GNN</th><th align="left" rowspan="1" colspan="1">Diffusion</th><th align="left" rowspan="1" colspan="1">Transformer</th><th align="left" rowspan="1" colspan="1">Equivariant</th><th align="left" rowspan="1" colspan="1">Fixed</th><th align="left" rowspan="1" colspan="1">Implicit</th><th align="left" rowspan="1" colspan="1">Explicit</th><th align="left" rowspan="1" colspan="1">Co-folding</th><th align="left" rowspan="1" colspan="1">Sequence</th><th align="left" rowspan="1" colspan="1">Structure</th><th align="left" rowspan="1" colspan="1">Hybrid</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">EquiBind [<xref rid="ref9" ref-type="bibr">9</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">TankBind [<xref rid="ref10" ref-type="bibr">10</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">DiffDock [<xref rid="ref13" ref-type="bibr">13</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">KarmaDock [<xref rid="ref26" ref-type="bibr">26</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">GAABind [<xref rid="ref32" ref-type="bibr">32</xref>]</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">Interformer [<xref rid="ref34" ref-type="bibr">34</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">CarsiDock [<xref rid="ref33" ref-type="bibr">33</xref>]</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">SurfDock [<xref rid="ref42" ref-type="bibr">42</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="left" rowspan="1" colspan="1">EDM-Dock [<xref rid="ref43" ref-type="bibr">43</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">FLEXVDW [<xref rid="ref41" ref-type="bibr">41</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">QuickBind [<xref rid="ref44" ref-type="bibr">44</xref>]</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="left" rowspan="1" colspan="1">DiffDock-Pocket [<xref rid="ref19" ref-type="bibr">19</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">FlexPose [<xref rid="ref20" ref-type="bibr">20</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">DynamicBind [<xref rid="ref22" ref-type="bibr">22</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">FlexDock [<xref rid="ref45" ref-type="bibr">45</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">Re-Dock [<xref rid="ref46" ref-type="bibr">46</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">FlexiDock [<xref rid="ref39" ref-type="bibr">39</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">CarsiDock-Flex [<xref rid="ref47" ref-type="bibr">47</xref>]</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">ApoDock [<xref rid="ref40" ref-type="bibr">40</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">FlowDock [<xref rid="ref48" ref-type="bibr">48</xref>]</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">DiffBindFR [<xref rid="ref39" ref-type="bibr">39</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">UMOL [<xref rid="ref49" ref-type="bibr">49</xref>]</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">RoseTTAFold All-Atom [<xref rid="ref50" ref-type="bibr">50</xref>]</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td></tr><tr><td align="left" rowspan="1" colspan="1">AlphaFold3 [<xref rid="ref36" ref-type="bibr">36</xref>]</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">Chai-1 [<xref rid="ref51" ref-type="bibr">51</xref>]</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">Boltz-1 [<xref rid="ref52" ref-type="bibr">52</xref>]</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/></tr><tr><td align="left" rowspan="1" colspan="1">NeuralPLexer3 [<xref rid="ref53" ref-type="bibr">53</xref>]</td><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td><td rowspan="1" colspan="1"/><td rowspan="1" colspan="1"/><td align="left" rowspan="1" colspan="1">&#10003;</td></tr></tbody></table></table-wrap><table-wrap position="float" id="TB4" orientation="portrait"><label>Table 4</label><caption><p>Comparison of selected docking models, showing success rate (ligand RMSD &lt; 2 &#197;) on the PoseBusters Benchmark, PB validity (successful dockings passing physical plausibility checks), and flexibility method categorized as explicit (E), implicit (I), or co-folding (C).</p></caption><table frame="hsides" rules="groups"><colgroup span="1"><col align="left" span="1"/><col span="2" align="left"/><col span="3" align="left"/><col span="4" align="left"/><col span="5" align="left"/></colgroup><thead><tr><th align="left" rowspan="1" colspan="1">Model</th><th align="left" rowspan="1" colspan="1">Flexibility</th><th align="left" rowspan="1" colspan="1">Success rate (%)</th><th align="left" rowspan="1" colspan="1">PB validity (%)</th><th align="left" rowspan="1" colspan="1">Flexibility method</th></tr></thead><tbody><tr><td align="left" rowspan="1" colspan="1">EquiBind [<xref rid="ref9" ref-type="bibr">9</xref>]</td><td align="left" rowspan="1" colspan="1">
<inline-formula>
<tex-math notation="LaTeX" id="ImEquation36">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\times $\end{document}</tex-math>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">2.0</td><td align="left" rowspan="1" colspan="1">0.0</td><td align="left" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" rowspan="1" colspan="1">DiffDock [<xref rid="ref24" ref-type="bibr">24</xref>]</td><td align="left" rowspan="1" colspan="1">
<inline-formula>
<tex-math notation="LaTeX" id="ImEquation37">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\times $\end{document}</tex-math>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">38</td><td align="left" rowspan="1" colspan="1">12</td><td align="left" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" rowspan="1" colspan="1">CarsiDock [<xref rid="ref33" ref-type="bibr">33</xref>]</td><td align="left" rowspan="1" colspan="1">
<inline-formula>
<tex-math notation="LaTeX" id="ImEquation38">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\times $\end{document}</tex-math>
</inline-formula>
</td><td align="left" rowspan="1" colspan="1">79.7</td><td align="left" rowspan="1" colspan="1">47.7</td><td align="left" rowspan="1" colspan="1">&#8211;</td></tr><tr><td align="left" rowspan="1" colspan="1">Re-Dock [<xref rid="ref46" ref-type="bibr">46</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">50.7</td><td align="left" rowspan="1" colspan="1">32.8</td><td align="left" rowspan="1" colspan="1">E</td></tr><tr><td align="left" rowspan="1" colspan="1">DiffBindFR-Smina [<xref rid="ref56" ref-type="bibr">56</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">50.2</td><td align="left" rowspan="1" colspan="1">49.1</td><td align="left" rowspan="1" colspan="1">E</td></tr><tr><td align="left" rowspan="1" colspan="1">DynamicBind [<xref rid="ref22" ref-type="bibr">22</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">67.8</td><td align="left" rowspan="1" colspan="1">34.0</td><td align="left" rowspan="1" colspan="1">E</td></tr><tr><td align="left" rowspan="1" colspan="1">FlowDock [<xref rid="ref48" ref-type="bibr">48</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">64.5</td><td align="left" rowspan="1" colspan="1">37.5</td><td align="left" rowspan="1" colspan="1">E</td></tr><tr><td align="left" rowspan="1" colspan="1">SurfDock [<xref rid="ref42" ref-type="bibr">42</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">82</td><td align="left" rowspan="1" colspan="1">74</td><td align="left" rowspan="1" colspan="1">I</td></tr><tr><td align="left" rowspan="1" colspan="1">AlphaFold3 [<xref rid="ref36" ref-type="bibr">36</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">80.4</td><td align="left" rowspan="1" colspan="1">73.1</td><td align="left" rowspan="1" colspan="1">C</td></tr><tr><td align="left" rowspan="1" colspan="1">NeuralPLexer3 [<xref rid="ref53" ref-type="bibr">53</xref>]</td><td align="left" rowspan="1" colspan="1">&#10003;</td><td align="left" rowspan="1" colspan="1">80.2</td><td align="left" rowspan="1" colspan="1">77.9</td><td align="left" rowspan="1" colspan="1">C</td></tr></tbody></table></table-wrap></sec><sec id="sec9"><title>Implicit flexibility</title><sec id="sec9a"><title>Atomic and residue-level representations</title><p>Implicit flexibility methods aim to capture conformal changes without explicitly sampling multiple receptor states, offering a compelling tradeoff between accuracy and efficiency. FLEXVDW [<xref rid="ref41" ref-type="bibr">41</xref>] exemplifies this at an atomic level, it implicitly accounts for protein flexibility by refining Van der Waals (VDW) interaction energy predictions without explicit conformational change modeling. It leverages multiple holo structures from PDBBind during training to learn how binding pockets deform across different ligands. At inference time, however, it requires only a single receptor conformation, yet still estimates the most favorable VDW interactions as if multiple conformers were available. The model architecture combines 3D equivariant convolution layers over atomic point clouds with pairwise interaction modules to capture fine-grained spatial relationships between ligand and receptor atoms. Integrated into the Glide docking suite, FLEXVDW improves pose prediction accuracy for targets with substantial flexibility while maintaining performance on rigid systems. Trained on the PDBBind 2019 refined set and evaluated on 615 cross-docked pairs, FLEXVDW demonstrates significant gains in challenging flexible cases without compromising efficiency.</p><p>EDM-Dock [<xref rid="ref43" ref-type="bibr">43</xref>] builds on a similar philosophy of flexibility through abstraction, using an EGNN to predict Euclidean distance matrices (EDMs) between ligand atoms and the protein&#8217;s coarse-grained C<inline-formula><tex-math notation="LaTeX" id="ImEquation39">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula> &#8220;skeleton.&#8221; Unlike FLEXVDW, which operates on full-atom input, EDM-Dock frees sidechains during pose generation, allowing them to respond dynamically to the ligand via an energy minimization step that lightly restrains the backbone. This enables local flexibility near the binding site without needing a full receptor ensemble. Trained on 53 000 BioLiP complexes, EDM-Dock outperforms GeauxDock by 0.77&#197; in redocking RMSD and shows stronger cross-docking performance than both GeauxDock and AutoDock Vina, achieving up to 2.00&#197; improvements and more frequent near-native poses (within 2&#197;&#8211;3.5&#197;). Together, FLEXVDW and EDM-Dock illustrate how implicitly flexible representations&#8212;whether through multiple training conformers or coarse-grained distance predictions&#8212;can improve docking accuracy while remaining computationally tractable.</p></sec><sec id="sec9b"><title>Surface representations</title><p>Surface-based representations offer an alternative strategy for modeling protein flexibility by abstracting away atomic-level rigidity while retaining key geometric and chemical features relevant to binding. A foundational method in this space is MaSIF [<xref rid="ref54" ref-type="bibr">54</xref>], which uses geometric DL to operate directly on protein surfaces. It generates a high-resolution mesh, decomposed into overlapping geodesic patches that capture local properties such as curvature, shape, hydropathy, and electrostatics. These features are processed with geodesic convolutional networks to produce rotation-invariant surface &#8220;fingerprints&#8221;&#8212;compact vector encodings that characterize local binding environments while remaining robust to small conformational changes.</p><p>Building on this idea, SurfDock [<xref rid="ref42" ref-type="bibr">42</xref>] represents the current state-of-the-art among surface-based docking frameworks with implicit protein flexibility. SurfDock integrates MaSIF-derived surface fingerprints with ESM-2 language model embeddings to construct a rich, multimodal representation of the protein&#8217;s binding site. This surface-level abstraction not only reduces the need for fixed atomic coordinates but also enables the model to operate within a more physically plausible binding geometry, allowing it to accommodate moderate structural changes without explicitly simulating receptor flexibility. Within an SE(3)-equivariant reverse diffusion framework, SurfDock iteratively refines an initial ligand pose by optimizing rotations, translations, and torsions&#8212;guided by the learned surface descriptors. An optional energy minimization step further adjusts ligand positions post-docking, especially in cases where the protein undergoes slight conformational shifts.</p><p>Trained on the PDBBind2020 dataset, SurfDock integrates three complementary levels of protein information&#8212;sequence, residue graph, and surface geometry&#8212;into a unified generative framework. It achieves strong generalization and outperforms both classical and DL-based baselines on a range of benchmarks, including PoseBusters, Astex, and DEKOIS 2.0. Notably, the practical applications of SurfDock are highlighted by its strong performance in VS tasks, particularly on the DEKOIS 2.0 benchmark&#8212;a challenging dataset consisting of 81 protein targets, each paired with 40 known actives and 1200 structurally similar decoys designed to test a model&#8217;s ability to distinguish true binders. By achieving an enrichment factor (EF0.5%) of 21.00, SurfDock demonstrates its ability to efficiently and reliably prioritize active compounds from large chemical libraries, making it a valuable tool for early-stage hit identification in structure-based drug discovery pipelines.</p><p>By leveraging surface geometry as a flexible proxy for binding site conformation, SurfDock exemplifies how DL models can implicitly capture the effects of protein flexibility without requiring explicit modeling.</p></sec></sec><sec id="sec10"><title>Explicit flexibility</title><p>DL-based approaches offer a key advantage over traditional methods as they can explicitly model protein flexibility without suffering from intractable computational costs. Notably, DiffDock-Pocket and CarsiDock-Flex represent recent advances in this direction. Both methods extend earlier models by incorporating protein conformational changes into the generative process, significantly improving docking accuracy and enhancing applicability in VS settings.</p><p>
<bold>DiffDock to DiffDock-Pocket.</bold> In many downstream tasks&#8212;such as binding affinity calculations&#8212;the accuracy of sidechain conformations is as critical as that of the ligand pose. Recognizing this, DiffDock-Pocket [<xref rid="ref19" ref-type="bibr">19</xref>] extends DiffDock by incorporating pocket-specific docking and explicit receptor flexibility into a diffusion-based framework. Rather than predicting full atomic coordinates, it models conformational changes in a reduced transformation space defined by ligand translation, rotation, torsion angles, and nearby sidechain torsions. Residues within 3.5&#197; of the ligand are treated as flexible, with their torsion angles refined through a reverse diffusion process guided by SE(3)-equivariant tensor field networks.</p><p>To ensure accurate modeling of flexibility, particularly when using in silico-generated apo protein structures, such as those from ESMFold or ColabFold, DiffDock-Pocket employs a sidechain conformer matching procedure during training. This aligns sidechain torsion angles of predicted structures with experimentally resolved holo structures, while incorporating a steric clash penalty that discourages unrealistic atom overlaps. This approach addresses previous limitations of DL models by improving the physical plausibility of generated complexes. Additionally, it enables the model to maintain strong docking performance even on noisy or unbound structures.</p><p>Empirically, DiffDock-Pocket achieves a correct pose (RMSD &lt; 2&#197;) in 49.8% of cases on the PDBBind benchmark, outperforming prior flexible docking methods, and retains high accuracy for in silico structures (41.7% for ESMFold; 39.5% for ColabFold). The model was also evaluated in cross-docking scenarios, where ligands are docked into alternate protein conformations unseen during training. This metric is particularly relevant when pocket sizes are unevenly distributed&#8212;a common challenge in VS. Notably, DiffDock-Pocket out performed prior flexible docking methods despite not being trained on cross-docked structures and using an out-of-distribution pocket definition. When aligned to the training-time pocket definition, its performance improves further, underscoring its strong generalization capabilities. </p><p>However, the precise contribution of receptor flexibility to docking accuracy remains difficult to isolate. While methods like GNINA and DiffDock-Pocket improve physical realism&#8212;e.g. by passing more PoseBusters checks&#8212;the extent to which these improvements stem from flexible modeling remains unclear. These results underscore the promise of incorporating torsional flexibility but also highlight the need for systematic, large-scale evaluations to fully understand when and how flexibility enhances docking performance.</p><p>
<bold>CarsiDock to CarsiDock-Flex.</bold> In 2023, Cai <italic toggle="yes">et&#160;al</italic>.[<xref rid="ref33" ref-type="bibr">33</xref>] introduced CarsiDock, a novel DL approach pre-trained on millions of predicted protein&#8211;ligand complexes. The model consisted of two main components: a neural network that predicted protein&#8211;ligand atomic distance matrices and a geometry optimization step that reconstructed valid binding poses by refining translation, rotation, and torsion. CarsiDock demonstrated strong generalization, achieving a top-1 success rate of 79.7% on the PoseBusters benchmark, outperforming both classical and DL-based docking methods. However, its PB-validity score, which assesses physical plausibility, reached only 47.7%, slightly below AutoDock Vina (51.2%), though still superior to other learning-based docking approaches.</p><p>A major limitation of CarsiDock was its assumption of a rigid protein structure, preventing it from capturing ligand-induced conformational changes. To address this, the research team extended its approach to cross-docking scenarios, where proteins adopt different conformations upon ligand binding. In 2025, they introduced CarsiDock-Flex [<xref rid="ref47" ref-type="bibr">47</xref>], which incorporates receptor flexibility by refining protein pockets before docking. At the core of this new framework is CarsiInduce, an equivariant DL model designed to shift ESMFold-predicted protein structures toward holo-like conformations. CarsiDock-Flex operates in two stages. First, CarsiInduce refines the binding pocket by adjusting residue positions to better approximate the ligand-bound state, effectively mimicking the induced-fit effect. Once the binding site is refined, CarsiDock re-docks the ligand, ensuring that the final binding pose is optimized within the adjusted structure.</p><p>The introduction of CarsiInduce significantly improved binding pocket predictions. On the PoseBusters-ESMFold dataset, the model increased the fraction of accurately predicted binding pockets (RMSD <inline-formula><tex-math notation="LaTeX" id="ImEquation40">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\leq $\end{document}</tex-math></inline-formula> 2.0 &#197;) from 71.97% to 80.81%, successfully refining 38.89% of previously mispredicted pockets. This refinement led to improved docking accuracy, with CarsiDock-Flex achieving a top-1 success rate of 56.57% (RMSD <inline-formula><tex-math notation="LaTeX" id="ImEquation41">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\leq $\end{document}</tex-math></inline-formula> 2.5 &#197;), outperforming CarsiDock (50.25%) and reducing the average docking RMSD from 3.270 &#197; to 3.024 &#197;. These results highlight the impact of explicitly modeling protein flexibility in docking, offering a promising direction for future developments in molecular docking and structure-based drug discovery. </p></sec><sec id="sec11"><title>Advancements in diffusion-based flexible docking</title><p>Recent years have seen significant progress in applying diffusion models to flexible molecular docking. In this section we discuss recent advancements in diffusion-based flexible docking approaches. One advantage of diffusion models is that they, in principle, allow flexible docking to multi-chain receptors. However, since commonly used training datasets (e.g. PDBBind) are dominated by single-chain structures, recent studies [<xref rid="ref55" ref-type="bibr">55</xref>] indicate significant performance and generalization challenges when applied to more complex multi-chain complexes.</p><p>More recently, alternative formulations such as flow matching and Schr&#246;dinger bridge methods have emerged, offering optimal transport-inspired pathways that can model &#8220;bridges&#8221; between data distributions of unbound (apo) and bound (holo) protein structures, learning vector fields, or stochastic trajectories that model conformational changes upon ligand binding. By exploiting the inherent alignment in such data, these methods capture underlying structural correspondences, leading to more accurate and physically plausible predictions of protein flexibility.</p><p>Early approaches, such as DiffBindFR [<xref rid="ref56" ref-type="bibr">56</xref>], employed score-based diffusion techniques to predict protein&#8211;ligand complexes. Like prior methods, DiffBindFR uses an SE(3)-equivariant diffusion model to approximate the score function of the reverse denoising SDE, enabling the prediction of bound protein&#8211;ligand complexes from unbound input pairs. However, unlike previous approaches which assume rigid protein pockets, DiffBindFR jointly denoises ligand movements and pocket sidechain torsions. Trained on the PDBBind v2020 dataset, DiffBindFR achieved a docking success rate of 51.2% (RMSD <inline-formula><tex-math notation="LaTeX" id="ImEquation42">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\leq $\end{document}</tex-math></inline-formula> 2 &#197;) and a 77.4% accuracy in recovering native-like sidechain conformations, outperforming traditional rigid docking methods.</p><p>Similar to DiffBindFR, FlexiDock [<xref rid="ref39" ref-type="bibr">39</xref>] operates within the score-based diffusion framework but adopts a compositional approach by using two coordinated diffusion models&#8212;one for ligand flexibility and one for receptor backbone and sidechain flexibility. Inspired by the induced-fit mechanism, it refines receptor structures during inference by gradually transforming apo conformations toward holo-like states while maintaining ligand stability (Though some literature labels FlexiDock as co-folding, we classify it as flexible docking. Unlike co-folding models which are trained end-to-end to predict complexes jointly, FlexiDock uses two separately trained diffusion models for protein and ligand, combined only at inference. It also requires a pre-folded apo protein structure rather than predicting complexes directly from sequence.). Trained on PDBBind and the Salda&#241;o apo&#8211;holo dataset, FlexiDock improved receptor RMSD accuracy from 56.58% (DiffDock) to 60.5% on PDBBind and increased receptor accuracy by over 6% on the Salda&#241;o dataset. By preserving a physically valid ligand pose throughout the process, FlexiDock excels in cross-docking and apo-docking tasks, mitigating the performance drop observed in rigid docking methods.</p><p>Advancements in diffusion modeling introduced flow matching techniques, offering direct mappings between protein conformational states. While traditional flow matching presents an attractive option for flexible docking, its direct application can result in a complex learning task with suboptimal performance due to strict marginal constraints and the common scenario of single holo-structures for training data. To address this, FlexDock [<xref rid="ref45" ref-type="bibr">45</xref>] proposed unbalanced flow matching as a generalization that relaxes these constraints, allowing for simpler, more learnable flows between unbound and bound states. This relaxation leads to substantial empirical improvements: on the PDBBind dataset, FlexDock increases the rate of high-accuracy protein conformation predictions (all-atom RMSD &lt; 1 &#197;) from 39.8% to 44.1%, while maintaining docking accuracy comparable to other state-of-the-art models (ligand RMSD &lt; 2 &#197;) and achieving faster inference speeds. On the PoseBusters benchmark, FlexDock outperforms several co-folding methods achieving a 46% docking success rate and demonstrating its generalization capability.</p><p>Building on the principles of flow matching, FlowDock [<xref rid="ref48" ref-type="bibr">48</xref>] introduces a deep geometric flow matching model based on conditional flow matching, designed to directly map unbound (apo) protein structures to their bound (holo) counterparts. Like FlexDock, FlowDock addresses the challenges of naively training such models by developing a generalized version of UFM. This involves defining a coupling distribution <inline-formula><tex-math notation="LaTeX" id="ImEquation43">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$q(z)$\end{document}</tex-math></inline-formula> using apo-to-holo assessment filters (e.g. RMSD and TM-score) to measure structural similarity between unbound and bound protein structures, ensuring suitable training pairs. Representing protein&#8211;ligand complexes as geometric graphs, FlowDock refines docking predictions using a variance diminishing ordinary differential equation (VD-ODE). Integrating ESMFold-predicted structures and harmonic ligand priors, it effectively captures ligand placement and protein flexibility. This model also provides predicted structural confidence scores and binding affinity values for its generated protein&#8211;ligand complex structures, enabling fast VS. Benchmarking on the PoseBusters dataset demonstrated a 51% blind docking success rate with apo protein input structures, surpassing many generative models like single-sequence AlphaFold3. Additionally, it ranked among the top five binding affinity predictors in CASP16, highlighting its efficacy in both docking and affinity prediction tasks.</p><p>In an alternative approach to flow matching, Re-Dock [<xref rid="ref46" ref-type="bibr">46</xref>] introduces diffusion bridge processes to jointly model ligand binding and receptor sidechain movements, emphasizing realistic induced-fit interactions. Unlike standard diffusion methods that rely on Gaussian noise perturbations, Re-Dock reformulates docking as a probabilistic induced-fit simulation, directly conditioning generative trajectories on physically valid protein&#8211;ligand interactions. This approach ensures more physically constrained docking outcomes, making Re-Dock particularly suitable for scenarios requiring high accuracy in flexible binding environments.</p><p>Collectively, these diffusion-based approaches illustrate a key advancement in molecular docking: the explicit modeling of both ligand and receptor flexibility significantly enhances docking accuracy. The field has evolved from early score-based diffusion methods&#8212;which refined ligand poses in isolation&#8212;to more integrated frameworks like DiffBindFR and FlexiDock that jointly model sidechain torsions and induced-fit effects. Flow matching techniques have further expanded the modeling capacity by learning direct, continuous mappings between unbound and bound protein states, enabling more realistic conformational transitions. However, challenges in data alignment and marginal constraints have led to the development of more robust formulations, such as Unbalanced Flow Matching, which improves learnability and generalization without sacrificing physical plausibility. Recent innovations like Re-Dock push this paradigm further by conditioning generative processes on chemically valid interactions, yielding docking predictions grounded in biophysical realism. Taken together, these advancements reflect a broader shift toward generative frameworks that unify flexibility, geometric constraints, and biochemical prior knowledge&#8212;offering a principled and scalable path toward next-generation flexible docking.</p></sec><sec id="sec12"><title>Encoder&#8211;decoder approaches</title><p>While diffusion-based methods offer one strategy for modeling protein flexibility through iterative generative sampling, encoder-decoder architectures provide an alternative pathway. These models explicitly learn a mapping from apo protein structures and candidate ligands to refined holo-like complexes, often integrating principles from classical physics-based approaches to enhance the physical plausibility of their predictions. Below, we review two representative encoder-decoder-based methods&#8212;ApoDock [<xref rid="ref40" ref-type="bibr">40</xref>] and FlexPose [<xref rid="ref20" ref-type="bibr">20</xref>]&#8212;and discuss their strategies for capturing induced-fit dynamics.</p><p>ApoDock introduces a flexible docking pipeline that couples ligand-conditioned sidechain packing with classical physics-based docking to refine protein&#8211;ligand poses. In contrast to rigid docking approaches, it leverages the ApoPack module, a message-passing neural network trained on holo-apo protein pairs, to predict holo-like sidechain conformations given the ligand and backbone context. This ensures that receptor flexibility is accounted for before the ligand is placed. Once the sidechain torsion angles are predicted, classical docking tools such as Smina or Gnina use the refined protein structure to sample ligand poses. A mixture density network-based scoring function, ApoScore, then re-ranks these poses according to their physical plausibility, integrating DL predictions with traditional scoring terms. Trained on PDBBind and a curated set of apo-holo pairs, ApoDock demonstrates strong performance in recovering ligand-induced conformational changes. It achieves a top-1 docking success rate of 52% on the PoseBuster benchmark, surpassing many rigid docking approaches, and attains a 46% success rate on the Apo2Holo dataset in cases demanding extensive sidechain rearrangements.</p><p>FlexPose builds on this trend of hybrid models by directly integrating biochemical priors and geometric structure into an end-to-end EGNN. While ApoDock relies on a sequential pipeline that first adapts the receptor before docking, FlexPose takes a joint modeling approach&#8212;encoding both protein and ligand atoms as nodes in a geometric graph and refining their conformations simultaneously through iterative message passing. To improve its handling of diverse conformations, FlexPose undergoes conformation-aware pre-training across a broad chemical space and incorporates low-confidence docking poses during training to enhance robustness. These strategies allow it to achieve a 64.8% success rate in predicting ligand binding poses using apo protein structures on APObind and PDBBind, significantly outperforming rigid docking methods. Additionally, it demonstrates a 70.5% success rate in cross-docking, highlighting its ability to model induced-fit interactions that rigid docking approaches often fail to capture.</p></sec></sec><sec><title>Co-folding approaches</title><sec id="sec13"><title>Sequence to structure, UMOL, and CASP 15</title><p>The development of UMOL [<xref rid="ref49" ref-type="bibr">49</xref>] and the recent inclusion of a sequence based ligand docking category in CASP15 mark a significant paradigm shift in molecular docking. This emerging class of methods, known as co-folding approaches, predicts the structure of protein&#8211;ligand complexes as a single task. Co-folding models like UMOL directly predict fully flexible, all-atom protein&#8211;ligand complexes from sequence, capturing both sidechain and backbone flexibility, without requiring predefined binding sites or template structures. This makes them especially well suited for de novo drug discovery and novel targets.</p><p>UMOL extends AlphaFold2&#8217;s EvoFormer architecture to jointly model protein&#8211;ligand interactions. The method processes the protein through multiple sequence alignment (MSA) features and encodes the ligand using a bond matrix derived from its SMILES representation. These are jointly embedded and passed through 48 EvoFormer blocks, enabling cross-communication between the protein and ligand representations. A 3D structure module then refines atomic coordinates through iterative updates, producing protein&#8211;ligand complexes that account for mutual flexibility. It also provides plDDT-based confidence scores that correlate with binding affinity, allowing it to distinguish strong binders from weak ones&#8212;a critical feature for early-stage drug discovery.</p><p>Additionally, the recent introduction of a sequence-based docking category in CASP15 underscores the field&#8217;s growing shift toward sequence-to-structure modeling. Recent DL models&#8212;such as Chai-1 and AlphaFold3&#8212;have extended this framework beyond protein&#8211;ligand prediction to encompass a broader range of biomolecular assemblies, including protein&#8211;nucleic acid and protein&#8211;protein complexes. The following section explores these models in greater detail, along with their current limitations.</p></sec><sec id="sec14"><title>Multi-modal foundation models</title><p>The emergence of sequence to structure models has driven the development of multi-modal foundation models such as AlphaFold3 [<xref rid="ref36" ref-type="bibr">36</xref>] and RoseTTAFold All-Atom [<xref rid="ref50" ref-type="bibr">50</xref>]. These co-folding models exhibit remarkable abilities in predicting a broad spectrum of biomolecular interactions, including protein&#8211;protein, protein&#8211;DNA, protein&#8211;RNA, and protein&#8211;ligand. Notably, their general-purpose nature does not compromise performance on specialized tasks like protein&#8211;ligand docking; rather, they achieve state-of-the-art performance across multiple protein&#8211;ligand benchmarks. This advantage likely stems from their extensive training on diverse biomolecular data, allowing them to learn fundamental principles of biomolecular interactions that transcend any single modality.</p><p>RFAA extends the RoseTTAFold2 backbone by incorporating a hybrid representation: one-dimensional sequence data, two-dimensional atomic graphs capturing covalent connectivity, and three-dimensional spatial features. It replaces generative diffusion components with a deterministic, regression-based architecture, using template-matching layers and a per-atom Frame Aligned Point Error loss function to enforce geometric consistency. This allows the model to directly refine atomic coordinates over successive layers. RFAA outperformed traditional docking tools such as AutoDock Vina in blind benchmarks like CAMEO and PoseBusters, achieving a 42% success rate (ligand RMSD &lt; 2.0 &#197;) demonstrating its generalization ability to novel protein and ligand clusters. Nevertheless, its reliance on generative mechanisms occasionally led to misassignment of ligand chirality and limited specificity in flexible binding pockets.</p><p>AlphaFold3 (AF3) represents a significant advancement over prior models like RFAA by integrating a diffusion-based structure module that iteratively refines atomic coordinates across multiple spatial scales. This generative approach enables AF3 to more accurately capture induced fit effects. AF3 also replaces the evoformer module used in AlphaFold2 with a pairformer module, which enhances the representation of pairwise atomic interactions and reduces reliance on multiple sequence alignments. By removing fixed stereochemical constraints and predefined bonding rules, AF3 increases its adaptability to a broader range of molecular types and chemical environments. On the PoseBusters benchmark, it achieved a 80% success rate (ligand RMSD &lt; 2.0 &#197;), outperforming RFAA and traditional docking tools like AutoDock Vina, particularly in flexible and chemically diverse binding scenarios. However, its generative nature occasionally produced non-physical ligand conformations and stereochemical inaccuracies, requiring post-processing to ensure structural plausibility.</p><p>Chai-1 [<xref rid="ref51" ref-type="bibr">51</xref>] built upon AF3&#8217;s architecture and training strategy, introducing the ability to be prompted with constraint features such as pocket conditioning and docking to apo structures, thereby enhancing prediction accuracy. During training, these constraints were randomly sampled from ground-truth structures using chain-wise and token-wise dropout, promoting robust predictions even with partial or noisy experimental data. On the PoseBusters benchmark, Chai-1 achieved a 81% success rate (ligand RMSD &lt; 2 &#197;), slightly surpassing AF3. Incorporating explicit docking constraints further increased accuracy to 85.5%. Chai-1 also performed strongly on the CASP15 monomer dataset, attaining a C<inline-formula><tex-math notation="LaTeX" id="ImEquation44">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula>-LDDT of 0.849. However, like AF3, it remained susceptible to occasional ligand chirality errors.</p><p>Despite recent progress, many models remained closed-source, limiting transparency and collaboration. Boltz-1 [<xref rid="ref52" ref-type="bibr">52</xref>] tackled this by introducing the first open-source DL model for biomolecular interaction modeling, matching the performance of AF3 and Chai-1. Like Chai-1, it supports both blind and pocket-specific docking. While based on the AF3 framework, Boltz-1 introduced key changes to improve training and structural accuracy&#8212;such as refining its MSA module, stabilizing transformer layers, and integrating a confidence model directly into the trunk. A Kabsch alignment step further improved structural output, and optimizations like chunked attention enhanced efficiency. Benchmarking showed Boltz-1 performing competitively in docking and structure prediction. On CASP15, it achieved a median C<inline-formula><tex-math notation="LaTeX" id="ImEquation45">\documentclass[12pt]{minimal}
\usepackage{amsmath}
\usepackage{wasysym}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage{upgreek}
\usepackage{mathrsfs}
\setlength{\oddsidemargin}{-69pt}
\begin{document}
$\alpha $\end{document}</tex-math></inline-formula>-LDDT of 0.849. In the PoseBusters benchmark, it kept ligand RMSD below 2 &#197;, capturing ligand flexibility and induced fit effects better than traditional methods. Though it still faced issues common to generative models&#8212;like stereochemical errors and occasional chain overlaps&#8212;its open-source design enabled ongoing improvements, making it a transparent and scalable alternative to proprietary tools.</p><p>The latest addition to the co-folding paradigm, NeuralPLexer3 (NP3) [<xref rid="ref53" ref-type="bibr">53</xref>] introduces a flow-based generative modeling framework for more efficient and physically realistic prediction of biomolecular complex structures. Unlike previous score-based diffusion models like AF3, NP3 leverages continuous normalizing flows (CNFs) and flow matching to generate atom-level accurate structures while significantly reducing computational overhead. Additionally, NP3 incorporated physics-informed priors and optimal transport symmetry correction, ensuring that protein&#8211;ligand interactions were modeled with greater structural accuracy and faster inference times. On the PoseBusters benchmark with PB-Validity checks, NP3 achieved a 77.9% success rate, surpassing AF3&#8217;s 73.1% and significantly outperforming traditional docking methods like Vina (59.7%) and GOLD (58.1%). NP3 also outperformed AF3 on the NPBench evaluation, particularly in modeling covalent ligand interactions and flexible protein&#8211;ligand complexes.</p></sec><sec id="sec15"><title>Limitations of co-folding approaches</title><p>Despite their impressive performance and features such as native support for multi-chain inputs and broader biomolecular interaction prediction, co-folding models like AlphaFold3 (AF3) and Chai-1 share a fundamental limitation common to many DL approaches in their limited ability to enforce core physical and chemical principles. While these models achieve high structural accuracy, recent studies [<xref rid="ref55" ref-type="bibr">55</xref>, <xref rid="ref57" ref-type="bibr">57</xref>, <xref rid="ref58" ref-type="bibr">58</xref>] suggest that they rely more on learned statistical patterns rather than on true molecular interaction principles. Masters et&#160;al. (2024) [<xref rid="ref58" ref-type="bibr">58</xref>] demonstrated that AF3 continues to predict nearly identical ligand poses even when critical binding residues are mutated&#8212;whether by removing key sidechain interactions, crowding the binding pocket, or introducing repulsive residues. This suggests that rather than predicting binding interactions based on an underlying physical energy landscape, current co-folding models primarily follow global structural motifs learned from their training data. Furthermore, they found that AF3 frequently assigns high-confidence scores to physically implausible structures, indicating a lack of self-correction mechanisms. Additionally, they found that co-folding models also perform poorly in multi-ligand environments, often generating steric clashes and unrealistic ligand placements, again suggesting that they do not learn the physical principles of biomolecular interactions.</p><p>Additionally, co-folding approaches have been criticized for overfitting to training data, excelling on familiar protein&#8211;ligand complexes but failing on novel targets. Morehead et&#160;al. [<xref rid="ref55" ref-type="bibr">55</xref>] found that while DL co-folding methods outperform traditional docking tools in standard benchmarks, they struggle to balance structural accuracy with chemical specificity, especially when confronted with novel protein sequences or uncommon binding sites. Additionally, they argue that AF3&#8217;s dependence on MSAs&#8212;which enhances accuracy for well-represented proteins&#8212;leads to performance degradation when evolutionary data is unavailable. In contrast, Chai-1, which integrates protein language models with MSAs, showed a more balanced performance, however, still struggled with out-of-distribution data. They also found that co-folding models struggle with under-represented protein classes, such as immune system and metal-transport proteins, revealing a bias toward well-characterized interactions. Collectively, these findings suggest that while co-folding models perform well within the scope of their training data, their generalizability remains a significant challenge.</p><p>While co-folding models represent a significant advancement, their limitations highlight the need for stronger physical constraints, improved generalization to novel proteins, and better modeling of multi-ligand interactions. Further research is required to understand these shortcomings and develop solutions, ensuring that co-folding methods can be reliably applied in drug discovery and <italic toggle="yes">de-novo</italic> drug design.</p></sec></sec><sec><title>Future directions</title><p>Having reviewed the evolution of DL methods in molecular docking&#8212;from the early rigid-body models to ligand-flexible and finally, fully flexible approaches&#8212;it is evident that these methods have dramatically advanced our ability to predict protein&#8211;ligand interactions. However, despite these advancements, several challenges persist.</p><p>Earlier studies, such as PoseBusters, highlighted the challenge of generalizing to proteins structurally distinct from those in the training data. Recent protein-flexible models have improved upon earlier rigid models by accounting for conformational changes in protein pockets, preventing the model from merely memorizing pocket configurations and thereby enhancing robustness and generalizability. While these models show promise in predicting interactions for unseen proteins, they still struggle with proteins that differ significantly from their training data. Beyond simply increasing dataset size and diversity, a promising approach&#8212;drawing inspiration from multi-biomolecular co-folding models&#8212;is to expand datasets beyond protein&#8211;ligand complexes to include interactions such as protein&#8211;protein, protein-DNA, and protein-RNA. This broader diversity may enable models to learn fundamental interaction principles rather than relying on pattern recognition.</p><p>From a practical standpoint, a promising strategy may be to analyze these models to identify the types of proteins they perform well on versus those they struggle with, allowing for more informed application. Analogous to mixture of experts in large reasoning models, a potential next step could involve splitting these models into smaller specialized &#8220;expert&#8221; models, fine-tuned neural networks designed for specific protein families to improve generalization and reliability.</p><p>Another common limitation of molecular docking models is their inability to adhere to strict physical laws which guide protein&#8211;ligand docking. Diffusion-based molecular docking programs, for example, frequently struggle with predicting physically implausible structures, generating unrealistic structures at a much higher rate than traditional docking methods that are based on physical simulations or energy calculations. To enhance physical plausibility, integrating principles from traditional physics-based methods, such as learning a funneled energy landscape [<xref rid="ref22" ref-type="bibr">22</xref>], seems highly promising, particularly in more difficult docking scenarios like multi-ligand environments where current methods still frequently predict steric clashes. A complementary approach involves training models on molecular dynamics simulation data, which, although slow to generate, offers a rich source of physically grounded trajectories. An emerging research direction is the use of flow matching models to learn and approximate molecular dynamics trajectories [<xref rid="ref59" ref-type="bibr">59</xref>]. These models aim to capture continuous, physically plausible transitions between molecular conformational states while substantially reducing the computational cost and time required to perform full-scale molecular dynamics simulations.</p><p>Docking in more challenging and biologically realistic scenarios remains a critical but underexplored frontier. Current methods are often benchmarked under simplified conditions, such as single-chain targets, idealized binding sites, and neglect of key environmental factors like solvent effects, metal ions, and cofactors. These limitations restrict the applicability of docking models to real-world systems. In particular, the performance of DL-based methods on multi-chain protein receptors remains poorly characterized, despite their significance in many practical applications. To identify potential limitations and guide future improvements, systematic investigation of model behavior in these more complex settings is essential. Addressing these gaps will require both the development of docking models that can handle such complexity and the design of more rigorous, biophysically meaningful evaluation metrics. Improved benchmarking in these more difficult settings is essential for assessing generalization and advancing the field.</p><p>For VS applications, balancing structural accuracy with computational efficiency remains a key challenge. Flexible docking methods provide a more favorable trade-off between speed, accuracy, and physical realism for high-throughput VS workflows. Although co-folding approaches can predict complexes without requiring predefined structures, their long inference times currently limit their practicality in large-scale VS. A potential solution may lie in model distillation, reducing the size and computational overhead of these models while preserving performance. Additionally, the introduction of more open-source models will likely allow researchers to more easily analyze and improve core components of models, such as the Pairformer, or diffusion module, which may further enhance both efficiency and accuracy.</p><boxed-text id="box01" position="float" orientation="portrait"><sec id="sec24a"><title>Key Points</title><list list-type="bullet"><list-item id="item10"><p>DL has transformed molecular docking, enabling faster and more accurate predictions than traditional methods while enabling more challenging tasks such as blind docking.</p></list-item><list-item id="item11"><p>Many traditional docking models assume rigid protein structures, but this simplification limits real-world applicability&#8212;especially in apo and cross-docking scenarios where proteins undergo significant conformational changes. Recent DL approaches have made notable progress in modeling protein flexibility, addressing a long-standing limitation of previous methods.</p></list-item><list-item id="item12"><p>This review categorizes recent flexible docking methods into implicit and explicit models, as well as flexible docking and co-folding strategies. It highlights key examples and explains how each method incorporates protein flexibility into their predictions.</p></list-item><list-item id="item13"><p>Diffusion-based generative models have emerged as powerful tools for modeling protein&#8211;ligand interactions. Techniques such as geometric flow matching and Schr&#246;dinger bridges enable models to learn transitions between unbound and bound protein states. These approaches are also central to many co-folding methods, which predict protein&#8211;ligand complexes directly from sequence data.</p></list-item><list-item id="item14"><p>Key challenges remain, including limited generalization to novel proteins, the generation of physically implausible structures, and reliance on weak evaluation metrics like RMSD. Future work should emphasize physically grounded modeling, improved validation criteria, and broader generalization through physics-informed machine learning.</p></list-item></list></sec></boxed-text><p>Conflict of interest: None declared.</p></sec></body><back><sec id="sec50b"><title>Funding</title><p>The authors would like to thank Duc Anh Nguyen for their valuable feedback and suggestions. Canh Hao Nguyen is partially supported by KAKENHI 22K12150. Hiroshi Mamitsuka has been partially supported by MEXT KAKENHI [21H05027, 22H03645, and 25H01144].</p></sec><sec sec-type="data-availability" id="sec50c"><title>Data availability</title><p>No new data were generated or analyzed in support of this study.</p></sec><ref-list id="bib1"><title>References</title><ref id="ref1"><label>1</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Hughes</surname> &#160;<given-names>JP</given-names></string-name>, <string-name name-style="western"><surname>Rees</surname> &#160;<given-names>S</given-names></string-name>, <string-name name-style="western"><surname>Kalindjian</surname> &#160;<given-names>SB</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Principles of early drug discovery</article-title>. <source><italic toggle="yes">Br J Pharmacol</italic></source> &#160;<year>2011</year>;<volume>162</volume>:<fpage>1239</fpage>&#8211;<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1111/j.1476-5381.2010.01127.x</pub-id><pub-id pub-id-type="pmid">21091654</pub-id><pub-id pub-id-type="pmcid">PMC3058157</pub-id></mixed-citation></ref><ref id="ref2"><label>2</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Gimeno</surname> &#160;<given-names>A</given-names></string-name>, <string-name name-style="western"><surname>Ojeda-Montes</surname> &#160;<given-names>MJ</given-names></string-name>, <string-name name-style="western"><surname>Tom&#225;s-Hern&#225;ndez</surname> &#160;<given-names>S</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>The light and dark sides of virtual screening: what is there to know?</article-title> &#160;<source><italic toggle="yes">Int J Mol Sci</italic></source> &#160;<year>2019</year>;<volume>20</volume>:<fpage>1375</fpage>. <pub-id pub-id-type="doi">10.3390/ijms20061375</pub-id><pub-id pub-id-type="pmid">30893780</pub-id><pub-id pub-id-type="pmcid">PMC6470506</pub-id></mixed-citation></ref><ref id="ref3"><label>3</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Stanzione</surname> &#160;<given-names>F</given-names></string-name>, <string-name name-style="western"><surname>Giangreco</surname> &#160;<given-names>I</given-names></string-name>, <string-name name-style="western"><surname>Cole</surname> &#160;<given-names>JC</given-names></string-name></person-group>. <article-title>Use of molecular docking computational tools in drug discovery</article-title>. <source><italic toggle="yes">Prog Med Chem</italic></source> &#160;<year>2021</year>. <comment>Epub 2021 May 27</comment>; <volume>60</volume>:<fpage>273</fpage>&#8211;<lpage>343</lpage>. <pub-id pub-id-type="doi">10.1016/bs.pmch.2021.01.004</pub-id><pub-id pub-id-type="pmid">34147204</pub-id></mixed-citation></ref><ref id="ref4"><label>4</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Abramson</surname> &#160;<given-names>J</given-names></string-name>, <string-name name-style="western"><surname>Adler</surname> &#160;<given-names>J</given-names></string-name>, <string-name name-style="western"><surname>Dunger</surname> &#160;<given-names>J</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Highly accurate protein structure prediction with alphafold</article-title>. <source><italic toggle="yes">Nature</italic></source> &#160;<year>2024</year>;<volume>630</volume>:<fpage>493</fpage>&#8211;<lpage>500</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-024-07487-w</pub-id><pub-id pub-id-type="pmid">38718835</pub-id><pub-id pub-id-type="pmcid">PMC11168924</pub-id></mixed-citation></ref><ref id="ref5"><label>5</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Kuntz</surname> &#160;<given-names>ID</given-names></string-name>, <string-name name-style="western"><surname>Blaney</surname> &#160;<given-names>JM</given-names></string-name>, <string-name name-style="western"><surname>Oatley</surname> &#160;<given-names>SJ</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>A geometric approach to macromolecule-ligand interactions</article-title>. <source><italic toggle="yes">J Mol Biol</italic></source> &#160;<year>1982</year>;<volume>161</volume>:<fpage>269</fpage>&#8211;<lpage>88</lpage>. <pub-id pub-id-type="doi">10.1016/0022-2836(82)90153-X</pub-id><pub-id pub-id-type="pmid">7154081</pub-id></mixed-citation></ref><ref id="ref6"><label>6</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Huang</surname> &#160;<given-names>S-Y</given-names></string-name>, <string-name name-style="western"><surname>Zou</surname> &#160;<given-names>X</given-names></string-name></person-group>. <article-title>Advances and challenges in protein-ligand docking</article-title>. <source><italic toggle="yes">Int J Mol Sci</italic></source> &#160;<year>2010</year>;<volume>11</volume>:<fpage>3016</fpage>&#8211;<lpage>34</lpage>. <pub-id pub-id-type="doi">10.3390/ijms11083016</pub-id><pub-id pub-id-type="pmid">21152288</pub-id><pub-id pub-id-type="pmcid">PMC2996748</pub-id></mixed-citation></ref><ref id="ref7"><label>7</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Trott</surname> &#160;<given-names>O</given-names></string-name>, <string-name name-style="western"><surname>Olson</surname> &#160;<given-names>AJ</given-names></string-name></person-group>. <article-title>Autodock vina: improving the speed and accuracy of docking with a new scoring function, efficient optimization, and multithreading</article-title>. <source><italic toggle="yes">J Comput Chem</italic></source> &#160;<year>2010</year>;<volume>31</volume>:<fpage>455</fpage>&#8211;<lpage>61</lpage>. <pub-id pub-id-type="doi">10.1002/jcc.21334</pub-id><pub-id pub-id-type="pmid">19499576</pub-id><pub-id pub-id-type="pmcid">PMC3041641</pub-id></mixed-citation></ref><ref id="ref8"><label>8</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Friesner</surname> &#160;<given-names>RA</given-names></string-name>, <string-name name-style="western"><surname>Banks</surname> &#160;<given-names>JL</given-names></string-name>, <string-name name-style="western"><surname>Murphy</surname> &#160;<given-names>RB</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Glide: a new approach for rapid, accurate docking and scoring. 1. Method and assessment of docking accuracy</article-title>. <source><italic toggle="yes">J Med Chem</italic></source> &#160;<year>2004</year>;<volume>47</volume>:<fpage>1739</fpage>&#8211;<lpage>49</lpage>. <pub-id pub-id-type="doi">10.1021/jm0306430</pub-id><pub-id pub-id-type="pmid">15027865</pub-id></mixed-citation></ref><ref id="ref9"><label>9</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name name-style="western"><surname>St&#228;rk</surname> &#160;<given-names>H</given-names></string-name>, <string-name name-style="western"><surname>Ganea</surname> &#160;<given-names>O-E</given-names></string-name>, <string-name name-style="western"><surname>Pattanaik</surname> &#160;<given-names>L</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<source>And Tommi Jaakkola</source>. <publisher-loc>Equibind</publisher-loc>: <publisher-name>geometric deep learning for drug binding structure prediction</publisher-name>, <year>2022</year>.</mixed-citation></ref><ref id="ref10"><label>10</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Wei</surname> &#160;<given-names>L</given-names></string-name>, <string-name name-style="western"><surname>Qifeng</surname> &#160;<given-names>W</given-names></string-name>, <string-name name-style="western"><surname>Zhang</surname> &#160;<given-names>J</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Tankbind: trigonometry-aware neural networks for drug-protein binding structure prediction</article-title> &#160;<comment>bioRxiv</comment>. <year>2022</year>.</mixed-citation></ref><ref id="ref11"><label>11</label><mixed-citation publication-type="other">Yu Y, Lu S, Gao Z <etal>et&#160;al</etal>. <article-title>Do deep learning models really outperform traditional approaches in molecular docking?</article-title> arXiv preprint arXiv:2302.07134, 2023.</mixed-citation></ref><ref id="ref12"><label>12</label><mixed-citation publication-type="other"><person-group person-group-type="author"><collab>Wikipedia contributors</collab></person-group>. <article-title>Docking (molecular)</article-title>. <comment><ext-link xlink:href="https://en.wikipedia.org/wiki/Docking_%28molecular%29" ext-link-type="uri">https://en.wikipedia.org/wiki/Docking_%28molecular%29</ext-link>, 2025 (11 May 2025, date last accessed)</comment>.</mixed-citation></ref><ref id="ref13"><label>13</label><mixed-citation publication-type="other">Corso G, St&#228;rk H, Jing B <etal>et&#160;al</etal>. <article-title>Diffdock: Diffusion steps, twists, and turns for molecular docking</article-title>. arXiv preprint arXiv:2210.01776, 2022.</mixed-citation></ref><ref id="ref14"><label>14</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western">Jiang D, Hsieh CY, Wu Z</string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>InteractionGraphNet: a novel and efficient deep graph representation learning framework for accurate protein&#8211;ligand interaction predictions</article-title>. <source>J Med Chem</source> 2021;<volume>64</volume>:18209&#8211;32. <pub-id pub-id-type="doi">10.1021/acs.jmedchem.1c01830</pub-id><pub-id pub-id-type="pmid">34878785</pub-id></mixed-citation></ref><ref id="ref15"><label>15</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Yang</surname> &#160;<given-names>Z</given-names></string-name>, <string-name name-style="western"><surname>Zhong</surname> &#160;<given-names>W</given-names></string-name>, <string-name name-style="western"><surname>Lv</surname> &#160;<given-names>Q</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Geometric interaction graph neural network for predicting protein&#8211;ligand binding affinities from 3D structures (GIGN)</article-title>. <source><italic toggle="yes">J Phys Chem Lett</italic></source> &#160;<year>2023</year>;<volume>14</volume>:<fpage>2020</fpage>&#8211;<lpage>33</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jpclett.2c03906</pub-id><pub-id pub-id-type="pmid">36794930</pub-id></mixed-citation></ref><ref id="ref16"><label>16</label><mixed-citation publication-type="book"><person-group person-group-type="author"><string-name name-style="western"><surname>Moon</surname> &#160;<given-names>S</given-names></string-name>, <string-name name-style="western"><surname>Zhung</surname> &#160;<given-names>W</given-names></string-name>, <string-name name-style="western"><surname>Yang</surname> &#160;<given-names>S</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<source>And Woo Youn Kim</source>. <publisher-loc>Pignet</publisher-loc>: <publisher-name>a physics-informed deep learning model toward generalized drug-target interaction predictions</publisher-name>, <year>2021</year>.<pub-id pub-id-type="doi" assigning-authority="pmc">10.1039/d1sc06946b</pub-id><pub-id pub-id-type="pmcid">PMC8966633</pub-id><pub-id pub-id-type="pmid">35432900</pub-id></mixed-citation></ref><ref id="ref17"><label>17</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western">Meli R, Morris GM, Biggin PC</string-name></person-group>. <article-title>Scoring functions for protein&#8211;ligand binding affinity prediction using structure-based deep learning: a review</article-title>. <source>Front Bioinform</source> 2022;<volume>2</volume>:885983. <pub-id pub-id-type="doi">10.3389/fbinf.2022.885983</pub-id><pub-id pub-id-type="pmcid">PMC7613667</pub-id><pub-id pub-id-type="pmid">36187180</pub-id></mixed-citation></ref><ref id="ref18"><label>18</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Bjerrum</surname> &#160;<given-names>EJ</given-names></string-name></person-group>. <article-title>Never use re-docking for estimation of docking accuracy</article-title>. <comment><ext-link xlink:href="https://www.cheminformania.com/never-use-re-docking-for-estimation-of-docking-accuracy/" ext-link-type="uri">https://www.cheminformania.com/never-use-re-docking-for-estimation-of-docking-accuracy/</ext-link>, May 2016 (22 April 2025, date last accessed).</comment></mixed-citation></ref><ref id="ref19"><label>19</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Plainer</surname> &#160;<given-names>M</given-names></string-name>, <string-name name-style="western"><surname>Toth</surname> &#160;<given-names>M</given-names></string-name>, <string-name name-style="western"><surname>Dobers</surname> &#160;<given-names>S</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>DiffDock-pocket: diffusion for pocket-level docking with sidechain flexibility.</article-title> &#160;<comment>arXiv preprint arXiv:2210.01776</comment>. <year>2023</year>.</mixed-citation></ref><ref id="ref20"><label>20</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Dong</surname> &#160;<given-names>T</given-names></string-name>, <string-name name-style="western"><surname>Yang</surname> &#160;<given-names>Z</given-names></string-name>, <string-name name-style="western"><surname>Zhou</surname> &#160;<given-names>J</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Equivariant flexible modeling of the protein&#8211;ligand binding pose with geometric deep learning</article-title>. <source><italic toggle="yes">J Chem Theory Comput</italic></source> &#160;<year>2023</year>;<volume>19</volume>:<fpage>8446</fpage>&#8211;<lpage>59</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jctc.3c00273</pub-id><pub-id pub-id-type="pmid">37938978</pub-id></mixed-citation></ref><ref id="ref21"><label>21</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Somnath</surname> &#160;<given-names>VR</given-names></string-name>, <string-name name-style="western"><surname>Pariset</surname> &#160;<given-names>M</given-names></string-name>, <string-name name-style="western"><surname>Hsieh</surname> &#160;<given-names>Y-P</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Aligned diffusion schr&#246;dinger bridges</article-title>. <year>2024</year>.</mixed-citation></ref><ref id="ref22"><label>22</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western">Lu W, Zhang J, Huang W</string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>DynamicBind: predicting ligand-specific protein-ligand complex structure with a deep equivariant generative model</article-title>. <source>Nat Commun</source> 2024;<volume>15</volume>:1071. <pub-id pub-id-type="doi">10.1038/s41467-024-45461-2</pub-id><pub-id pub-id-type="pmcid">PMC10844226</pub-id><pub-id pub-id-type="pmid">38316797</pub-id></mixed-citation></ref><ref id="ref23"><label>23</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Yang</surname> &#160;<given-names>J</given-names></string-name>, <string-name name-style="western"><surname>Shen</surname> &#160;<given-names>C</given-names></string-name>, <string-name name-style="western"><surname>Huang</surname> &#160;<given-names>N</given-names></string-name></person-group>. <article-title>Predicting or pretending: Artificial intelligence for protein-ligand interactions lack of sufficiently large and unbiased datasets</article-title>. <source><italic toggle="yes">Front Pharmacol</italic></source> &#160;<year>2020</year>;<volume>11</volume>:<fpage>69</fpage>. <pub-id pub-id-type="doi">10.3389/fphar.2020.00069</pub-id><pub-id pub-id-type="pmid">32161539</pub-id><pub-id pub-id-type="pmcid">PMC7052818</pub-id></mixed-citation></ref><ref id="ref24"><label>24</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Corso</surname> &#160;<given-names>G</given-names></string-name>, <string-name name-style="western"><surname>St&#228;rk</surname> &#160;<given-names>H</given-names></string-name>, <string-name name-style="western"><surname>Jing</surname> &#160;<given-names>B</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Diffdock: diffusion steps, twists, and turns for molecular docking</article-title>. In <source>International Conference on Learning Representations (ICLR)</source>, <year>2023</year>.</mixed-citation></ref><ref id="ref25"><label>25</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Buttenschoen</surname> &#160;<given-names>M</given-names></string-name>, <string-name name-style="western"><surname>Morris</surname> &#160;<given-names>GM</given-names></string-name>, <string-name name-style="western"><surname>Deane</surname> &#160;<given-names>CM</given-names></string-name></person-group>. <article-title>Posebusters: AI-based docking methods fail to generate physically valid poses or generalise to novel sequences</article-title>. <source><italic toggle="yes">Chem Sci</italic></source> &#160;<year>2024</year>;<volume>15</volume>:<fpage>3130</fpage>&#8211;<lpage>9</lpage>. The Royal Society of Chemistry, Cambridge, UK. <pub-id pub-id-type="doi">10.1039/D3SC04185A</pub-id><pub-id pub-id-type="pmid">38425520</pub-id><pub-id pub-id-type="pmcid">PMC10901501</pub-id></mixed-citation></ref><ref id="ref26"><label>26</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Zhang</surname> &#160;<given-names>X</given-names></string-name>, <string-name name-style="western"><surname>Zhang</surname> &#160;<given-names>O</given-names></string-name>, <string-name name-style="western"><surname>Shen</surname> &#160;<given-names>C</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Efficient and accurate large library ligand docking with karmadock</article-title>. <source><italic toggle="yes">Nat Comput Sci</italic></source> &#160;<year>2023</year>;<volume>3</volume>:<fpage>789</fpage>&#8211;<lpage>804</lpage>. <pub-id pub-id-type="doi">10.1038/s43588-023-00511-5</pub-id><pub-id pub-id-type="pmid">38177786</pub-id></mixed-citation></ref><ref id="ref27"><label>27</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Geiger</surname> &#160;<given-names>M</given-names></string-name>, <string-name name-style="western"><surname>Smidt</surname> &#160;<given-names>T</given-names></string-name></person-group>. <source>e3nn: Euclidean Neural Networks</source> &#160;<year>2022</year>.</mixed-citation></ref><ref id="ref28"><label>28</label><mixed-citation publication-type="other">Wang Y, Elhag AA, Jaitly N <etal>et&#160;al</etal>. <article-title>Swallowing the bitter pill: simplified scalable conformer generation</article-title>. arXiv preprint arXiv:2311.17932; 2024.</mixed-citation></ref><ref id="ref29"><label>29</label><mixed-citation publication-type="other">Brehmer J, Behrends S, de Haan P <etal>et&#160;al</etal>. <article-title>Does equivariance matter at scale?</article-title> arXiv preprint arXiv:2410.23179; 2025.</mixed-citation></ref><ref id="ref30"><label>30</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western">Thais S, Murnane D</string-name></person-group>. <article-title>Equivariance is not all you need: characterizing the utility of equivariant graph neural networks for particle physics tasks</article-title>. arXiv preprint arXiv:2311.03094; 2023.</mixed-citation></ref><ref id="ref31"><label>31</label><mixed-citation publication-type="other">Zhou G, Gao Z, Ding Q <etal>et&#160;al</etal>. <article-title>Uni-Mol: a universal 3D molecular representation learning framework</article-title>. ChemRxiv preprint 2023. <pub-id pub-id-type="doi">10.26434/chemrxiv-2022-jjm0j-v4</pub-id></mixed-citation></ref><ref id="ref32"><label>32</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Tan</surname> &#160;<given-names>H</given-names></string-name>, <string-name name-style="western"><surname>Wang</surname> &#160;<given-names>Z</given-names></string-name>, <string-name name-style="western"><surname>Guang</surname> &#160;<given-names>H</given-names></string-name></person-group>. <article-title>Gaabind: a geometry-aware attention-based network for accurate protein&#8211;ligand binding pose and binding affinity prediction</article-title>. <source><italic toggle="yes">Brief Bioinform</italic></source> &#160;<year>2023</year>;<volume>25</volume>:<fpage>12</fpage>. Oxford University Press, Oxford, UK. <pub-id pub-id-type="doi">10.1093/bib/bbad462</pub-id><pub-id pub-id-type="pmcid">PMC10724026</pub-id><pub-id pub-id-type="pmid">38102069</pub-id></mixed-citation></ref><ref id="ref33"><label>33</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Cai</surname> &#160;<given-names>H</given-names></string-name>, <string-name name-style="western"><surname>Shen</surname> &#160;<given-names>C</given-names></string-name>, <string-name name-style="western"><surname>Jian</surname> &#160;<given-names>T</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Carsidock: a deep learning paradigm for accurate protein&#8211;ligand docking and screening based on large-scale pre-training</article-title>. <source><italic toggle="yes">Chem Sci</italic></source> &#160;<year>2024</year>;<volume>15</volume>:<fpage>1449</fpage>&#8211;<lpage>71</lpage>. <pub-id pub-id-type="doi">10.1039/D3SC05552C</pub-id><pub-id pub-id-type="pmid">38274053</pub-id><pub-id pub-id-type="pmcid">PMC10806797</pub-id></mixed-citation></ref><ref id="ref34"><label>34</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Lai</surname> &#160;<given-names>H</given-names></string-name>, <string-name name-style="western"><surname>Wang</surname> &#160;<given-names>L</given-names></string-name>, <string-name name-style="western"><surname>Qian</surname> &#160;<given-names>R</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Interformer: an interaction-aware model for protein-ligand docking and affinity prediction</article-title>. <source><italic toggle="yes">Nat Commun</italic></source> &#160;<year>2024</year>;<volume>15</volume>:<fpage>10223</fpage>. <pub-id pub-id-type="doi">10.1038/s41467-024-54440-6</pub-id><pub-id pub-id-type="pmid">39587070</pub-id><pub-id pub-id-type="pmcid">PMC11589619</pub-id></mixed-citation></ref><ref id="ref35"><label>35</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Yim</surname> &#160;<given-names>J</given-names></string-name>, <string-name name-style="western"><surname>St&#228;rk</surname> &#160;<given-names>H</given-names></string-name>, <string-name name-style="western"><surname>Corso</surname> &#160;<given-names>G</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Diffusion models in protein structure and docking</article-title>. <source><italic toggle="yes">WIREs Comput Mol Sci</italic></source> &#160;<year>2024</year>;<volume>14</volume>:e1711. <pub-id pub-id-type="doi">10.1002/wcms.1711</pub-id></mixed-citation></ref><ref id="ref36"><label>36</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Abramson</surname> &#160;<given-names>J</given-names></string-name>, <string-name name-style="western"><surname>Adler</surname> &#160;<given-names>J</given-names></string-name>, <string-name name-style="western"><surname>Dunger</surname> &#160;<given-names>J</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Accurate structure prediction of biomolecular interactions with alphafold 3</article-title>. <source><italic toggle="yes">Nature</italic></source> &#160;<year>2024</year>;<volume>630</volume>:<fpage>493</fpage>&#8211;<lpage>500</lpage>. <pub-id pub-id-type="doi">10.1038/s41586-024-07487-w</pub-id><pub-id pub-id-type="pmid">38718835</pub-id><pub-id pub-id-type="pmcid">PMC11168924</pub-id></mixed-citation></ref><ref id="ref37"><label>37</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Ravindranath</surname> &#160;<given-names>PA</given-names></string-name>, <string-name name-style="western"><surname>Forli</surname> &#160;<given-names>S</given-names></string-name>, <string-name name-style="western"><surname>Goodsell</surname> &#160;<given-names>DS</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>AutoDockFR: advances in protein-ligand docking with explicitly specified binding site flexibility</article-title>. <source><italic toggle="yes">PLoS Comput Biol</italic></source> &#160;<year>2015</year>;<volume>11</volume>:<fpage>e1004586</fpage>. <pub-id pub-id-type="doi">10.1371/journal.pcbi.1004586</pub-id><pub-id pub-id-type="pmid">26629955</pub-id><pub-id pub-id-type="pmcid">PMC4667975</pub-id></mixed-citation></ref><ref id="ref38"><label>38</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Zavodszky</surname> &#160;<given-names>MI</given-names></string-name>, <string-name name-style="western"><surname>Kuhn</surname> &#160;<given-names>LA</given-names></string-name></person-group>. <article-title>Side-chain flexibility in protein&#8211;ligand binding: the minimal rotation hypothesis</article-title>. <source><italic toggle="yes">Protein Sci</italic></source> &#160;<year>2005</year>;<volume>14</volume>:<fpage>1104</fpage>&#8211;<lpage>14</lpage>. <pub-id pub-id-type="doi">10.1110/ps.041153605</pub-id><pub-id pub-id-type="pmid">15772311</pub-id><pub-id pub-id-type="pmcid">PMC2253453</pub-id></mixed-citation></ref><ref id="ref39"><label>39</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Wang</surname> &#160;<given-names>Z</given-names></string-name>, <string-name name-style="western"><surname>Srinivasan</surname> &#160;<given-names>B</given-names></string-name>, <string-name name-style="western"><surname>Shen</surname> &#160;<given-names>Z</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Flexidock: compositional diffusion models for flexible molecular docking</article-title>. <year>2023</year>.</mixed-citation></ref><ref id="ref40"><label>40</label><mixed-citation publication-type="other">Luo D, Qu X, Lu D <etal>et&#160;al</etal>. <article-title>ApoDock: ligand-conditioned sidechain packing for flexible molecular docking</article-title>. bioRxiv preprint 2024. <pub-id pub-id-type="doi">10.1101/2024.11.22.624942</pub-id></mixed-citation></ref><ref id="ref41"><label>41</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western">Suriana P, Paggi JM, Dror RO</string-name></person-group>. <article-title>FlexVDW: a machine learning approach to account for protein flexibility in ligand docking</article-title>. arXiv preprint arXiv:2303.11494; 2023. <ext-link xlink:href="https://arxiv.org/abs/2303.11494" ext-link-type="uri">https://arxiv.org/abs/2303.11494</ext-link></mixed-citation></ref><ref id="ref42"><label>42</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Cao</surname> &#160;<given-names>D</given-names></string-name>, <string-name name-style="western"><surname>Chen</surname> &#160;<given-names>M</given-names></string-name>, <string-name name-style="western"><surname>Zhang</surname> &#160;<given-names>R</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Surfdock is a surface-informed diffusion generative model for reliable and accurate protein&#8211;ligand complex prediction</article-title>. <source><italic toggle="yes">Nat Methods</italic></source> &#160;<year>2025</year>;<volume>22</volume>:<fpage>310</fpage>&#8211;<lpage>22</lpage>. <pub-id pub-id-type="doi">10.1038/s41592-024-02516-y</pub-id><pub-id pub-id-type="pmid">39604569</pub-id></mixed-citation></ref><ref id="ref43"><label>43</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Masters</surname> &#160;<given-names>M</given-names></string-name>, <string-name name-style="western"><surname>Mahmoud</surname> &#160;<given-names>A</given-names></string-name>, <string-name name-style="western"><surname>Wei</surname> &#160;<given-names>Y</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Deep learning model for efficient protein-ligand docking with implicit side-chain flexibility</article-title>. <source><italic toggle="yes">J Chem Inf Model</italic></source> &#160;<year>2023</year>;<volume>63</volume>:<fpage>1695</fpage>&#8211;<lpage>707</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jcim.2c01436</pub-id><pub-id pub-id-type="pmid">36916514</pub-id></mixed-citation></ref><ref id="ref44"><label>44</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western"><surname>Treyde</surname> &#160;<given-names>W</given-names></string-name>, <string-name name-style="western"><surname>Kim</surname> &#160;<given-names>SC</given-names></string-name>, <string-name name-style="western"><surname>Bouatta</surname> &#160;<given-names>N</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Quickbind: a light-weight and interpretable molecular docking model</article-title>. <year>2024</year>.</mixed-citation></ref><ref id="ref45"><label>45</label><mixed-citation publication-type="other">Corso G, Somnath VR, Getz N <etal>et&#160;al</etal>. <article-title>Flexible docking via unbalanced flow matching</article-title>. In: <source>ICML 2024 AI for Science Workshop</source>; 2024. Available from: <ext-link xlink:href="https://openreview.net/forum?id=aa7oakuB6W" ext-link-type="uri">https://openreview.net/forum?id=aa7oakuB6W</ext-link></mixed-citation></ref><ref id="ref46"><label>46</label><mixed-citation publication-type="other">Huang Y, Zhang O, Wu L <etal>et&#160;al</etal>. <article-title>Re-Dock: towards flexible and realistic molecular docking with diffusion bridge</article-title>. arXiv preprint arXiv:2402.11459; 2024. <ext-link xlink:href="https://arxiv.org/abs/2402.11459" ext-link-type="uri">https://arxiv.org/abs/2402.11459</ext-link></mixed-citation></ref><ref id="ref47"><label>47</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Shen</surname> &#160;<given-names>C</given-names></string-name>, <string-name name-style="western"><surname>Han</surname> &#160;<given-names>X</given-names></string-name>, <string-name name-style="western"><surname>Cai</surname> &#160;<given-names>H</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Improving the reliability of language model-predicted structures as docking targets through geometric graph learning</article-title>. <source><italic toggle="yes">J Med Chem</italic></source> &#160;<year>2025</year>;<volume>68</volume>:<fpage>1956</fpage>&#8211;<lpage>69</lpage>. <pub-id pub-id-type="doi">10.1021/acs.jmedchem.4c02740</pub-id><pub-id pub-id-type="pmid">39787296</pub-id></mixed-citation></ref><ref id="ref48"><label>48</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Morehead</surname> &#160;<given-names>A</given-names></string-name>, <string-name name-style="western"><surname>Cheng</surname> &#160;<given-names>J</given-names></string-name></person-group>. <source>Flowdock: geometric flow matching for generative protein-ligand docking and affinity prediction</source>, <year>2025</year>, <volume>41</volume>, <fpage>i198</fpage>, <lpage>i206</lpage>, <pub-id pub-id-type="doi">10.1093/bioinformatics/btaf187</pub-id>.<pub-id pub-id-type="pmcid">PMC12261468</pub-id><pub-id pub-id-type="pmid">40662794</pub-id></mixed-citation></ref><ref id="ref49"><label>49</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western">Bryant P, Kelkar A, Guljas A</string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Structure prediction of protein-ligand complexes from sequence information with Umol</article-title>. <source>Nat Commun</source> 2024;<volume>15</volume>:4536. <pub-id pub-id-type="doi">10.1038/s41467-024-48837-6</pub-id><pub-id pub-id-type="pmcid">PMC11133481</pub-id><pub-id pub-id-type="pmid">38806453</pub-id></mixed-citation></ref><ref id="ref50"><label>50</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western"><surname>Krishna</surname> &#160;<given-names>R</given-names></string-name>, <string-name name-style="western"><surname>Wang</surname> &#160;<given-names>J</given-names></string-name>, <string-name name-style="western"><surname>Ahern</surname> &#160;<given-names>W</given-names></string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Generalized biomolecular modeling and design with rosettafold all-atom</article-title>. <source><italic toggle="yes">Science</italic></source> &#160;<year>2024</year>;<volume>384</volume>. <pub-id pub-id-type="doi">10.1126/science.adl2528</pub-id><pub-id pub-id-type="pmid">38452047</pub-id></mixed-citation></ref><ref id="ref51"><label>51</label><mixed-citation publication-type="other">Chai Discovery Team, Boitreaud J, Dent J <etal>et&#160;al</etal>. <article-title>Chai-1: decoding the molecular interactions of life</article-title>. bioRxiv preprint 2024. <pub-id pub-id-type="doi">10.1101/2024.10.10.615955</pub-id></mixed-citation></ref><ref id="ref52"><label>52</label><mixed-citation publication-type="other">Wohlwend J, Corso G, Passaro S <etal>et&#160;al</etal>. <article-title>Boltz-1: democratizing biomolecular interaction modeling</article-title>. bioRxiv preprint 2024. <pub-id pub-id-type="doi">10.1101/2024.11.19.624167</pub-id></mixed-citation></ref><ref id="ref53"><label>53</label><mixed-citation publication-type="other">Qiao Z, Ding F, Dresselhaus T <etal>et&#160;al</etal>. <article-title>NeuralPLexer3: accurate biomolecular complex structure prediction with flow models</article-title>. arXiv preprint arXiv:2412.10743; 2024. <ext-link xlink:href="https://arxiv.org/abs/2412.10743" ext-link-type="uri">https://arxiv.org/abs/2412.10743</ext-link></mixed-citation></ref><ref id="ref54"><label>54</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western">Gainza P, Sverrisson F, Monti F</string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>Deciphering interaction fingerprints from protein molecular surfaces using geometric deep learning</article-title>. <source>Nature Methods</source> 2020;<volume>17</volume>:184&#8211;92. <pub-id pub-id-type="doi">10.1038/s41592-019-0666-6</pub-id><pub-id pub-id-type="pmid">31819266</pub-id></mixed-citation></ref><ref id="ref55"><label>55</label><mixed-citation publication-type="other">Morehead A, Giri N, Liu J <etal>et&#160;al</etal>. <article-title>Assessing the potential of deep learning for protein&#8211;ligand docking</article-title>. arXiv preprint arXiv:2405.14108; 2025. <ext-link xlink:href="https://arxiv.org/abs/2405.14108" ext-link-type="uri">https://arxiv.org/abs/2405.14108</ext-link></mixed-citation></ref><ref id="ref56"><label>56</label><mixed-citation publication-type="journal"><person-group person-group-type="author"><string-name name-style="western">Zhu J, Gu Z, Pei J</string-name> &#160;<etal>et&#160;al</etal>.</person-group> &#160;<article-title>DiffBindFR: an SE(3) equivariant network for flexible protein&#8211;ligand docking</article-title>. <source>Chem Sci</source> 2023;<volume>15</volume>:7926&#8211;42. <pub-id pub-id-type="doi">10.1039/D3SC06803J</pub-id><pub-id pub-id-type="pmcid">PMC11134415</pub-id><pub-id pub-id-type="pmid">38817560</pub-id></mixed-citation></ref><ref id="ref57"><label>57</label><mixed-citation publication-type="other">&#352;krinjar P, Eberhardt J, Durairaj J <etal>et&#160;al</etal>. <article-title>Have protein&#8211;ligand co-folding methods moved beyond memorisation?</article-title> bioRxiv preprint 2025. <pub-id pub-id-type="doi">10.1101/2025.02.03.636309</pub-id></mixed-citation></ref><ref id="ref58"><label>58</label><mixed-citation publication-type="other"><person-group person-group-type="author"><string-name name-style="western">Masters MR, Mahmoud AH, Lill MA</string-name></person-group>. <article-title>Do deep learning models for co-folding learn the physics of protein&#8211;ligand interactions?</article-title> bioRxiv preprint 2024. <pub-id pub-id-type="doi">10.1101/2024.06.03.597219</pub-id></mixed-citation></ref><ref id="ref59"><label>59</label><mixed-citation publication-type="journal">Jing B <etal>et&#160;al</etal>. <article-title>Generative modeling of molecular dynamics trajectories</article-title>. <source>Advances in Neural Information Processing Systems</source> 2024;<volume>37</volume>:40534&#8211;64.</mixed-citation></ref></ref-list></back></article>
        
    </metadata>
</record>
    </GetRecord>

</OAI-PMH>