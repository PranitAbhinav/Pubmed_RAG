


<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
    <responseDate>2025-09-09T13:47:43Z</responseDate>
    <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:12405463" metadataPrefix="pmc">https://pmc.ncbi.nlm.nih.gov/api/oai/v1/mh/</request>
    
    <GetRecord>
        <record>
    <header>
    <identifier>oai:pubmedcentral.nih.gov:12405463</identifier>
    <datestamp>2025-09-04</datestamp>
    
        
        <setSpec>scirep</setSpec>
        
    
        
        <setSpec>pmc-open</setSpec>
        
    
</header>
    <metadata>
        
        <article xmlns="https://jats.nlm.nih.gov/ns/archiving/1.4/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.4/ https://jats.nlm.nih.gov/archiving/1.4/xsd/JATS-archivearticle1-4.xsd" article-type="research-article" xml:lang="en" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-id journal-id-type="pmc-domain-id">1579</journal-id><journal-id journal-id-type="pmc-domain">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12405463</article-id><article-id pub-id-type="pmcid-ver">PMC12405463.1</article-id><article-id pub-id-type="pmcaid">12405463</article-id><article-id pub-id-type="pmcaiid">12405463</article-id><article-id pub-id-type="pmid">40897750</article-id><article-id pub-id-type="doi">10.1038/s41598-025-01319-1</article-id><article-id pub-id-type="publisher-id">1319</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>A dual-stream deep learning framework for skin cancer classification using histopathological-inherited and vision-based feature extraction</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name name-style="western"><surname>Almutairi</surname><given-names initials="SA">Saleh Ateeq</given-names></name><address><email>smoutiri@taibahu.edu.sa</email></address><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/01xv1nn60</institution-id><institution-id institution-id-type="GRID">grid.412892.4</institution-id><institution-id institution-id-type="ISNI">0000 0004 1754 9358</institution-id><institution>Department of Computer Science and Informatics, Applied College, </institution><institution>Taibah University, </institution></institution-wrap>Madinah, 41461 Saudi Arabia </aff></contrib-group><pub-date pub-type="epub"><day>2</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type="pmc-issue-id">478255</issue-id><elocation-id>32301</elocation-id><history><date date-type="received"><day>16</day><month>2</month><year>2025</year></date><date date-type="accepted"><day>5</day><month>5</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-04 00:25:59.930"><day>04</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri content-type="pmc-pdf" xlink:href="41598_2025_Article_1319.pdf"/><abstract id="Abs1"><p id="Par1">Skin cancer, particularly melanoma, remains one of the most life-threatening forms of cancer worldwide, with early detection being critical for improving patient outcomes. Traditional diagnostic methods, such as dermoscopy and histopathology, are often limited by subjectivity, interobserver variability, and resource constraints. To address these challenges, this study proposes a dual-stream deep learning framework that combines histopathological-inherited and vision-based feature extraction for accurate and efficient skin lesion diagnosis. The framework uses the U-Net architecture for precise lesion segmentation, followed by a dual-stream approach: the first stream employs Virchow2, a pretrained model, to extract high-level histopathological embeddings, whereas the second stream uses Nomic, a vision-based model, to capture spatial and contextual information. The extracted features are fused and integrated to create a comprehensive representation of the lesion, which is then classified via a multilayer perceptron (MLP). The proposed approach is evaluated on the HAM10000 dataset, achieving a mean accuracy of 96.25% and a mean F1 score of 93.79% across 10 trials. Ablation studies demonstrate the importance of both feature streams, with the removal of either stream resulting in significant performance degradation. Comparative analysis with existing studies highlights the superiority of the proposed framework, which outperforms traditional single-modality approaches. The results underscore the potential of the dual-stream framework to enhance skin cancer diagnosis, offering a robust, interpretable, and scalable solution for clinical applications.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Classification</kwd><kwd>Deep learning (DL)</kwd><kwd>Feature extraction</kwd><kwd>Histopathology</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Cancer</kwd><kwd>Computational biology and bioinformatics</kwd><kwd>Medical research</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Skin cancer is one of the most prevalent and life-threatening types of cancer worldwide, with melanoma representing its deadliest form<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Melanoma accounts for the majority of skin cancer-related deaths, despite constituting a smaller proportion of skin cancer cases than nonmelanoma types such as basal cell carcinoma and squamous cell carcinoma<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR3">3</xref></sup>.</p><p id="Par3">According to the World Health Organization (WHO), approximately 132,000 new cases of melanoma are diagnosed annually worldwide<sup><xref ref-type="bibr" rid="CR4">4</xref>,<xref ref-type="bibr" rid="CR5">5</xref></sup>, and this number continues to rise due to factors such as increased ultraviolet (UV) radiation exposure, aging populations, and improved diagnostic capabilities<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. The aggressive nature of melanoma, coupled with its ability to metastasize rapidly, underscores the importance of early detection<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>.</p><p id="Par4">When diagnosed at an early stage, the five-year survival rate for patients with melanoma exceeds 99%, but this rate decreases to less than 30% for patients with advanced-stage disease. These statistics highlight the critical need for accurate and timely diagnostic tools to improve patient outcomes and reduce mortality rates<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>.</p><p id="Par5">Traditional diagnostic methods for skin cancer rely on dermatologists using tools like dermoscopy and visual inspection<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. Dermoscopy, which magnifies and illuminates the skin, improves accuracy but depends heavily on clinician expertise<sup><xref ref-type="bibr" rid="CR11">11</xref>,<xref ref-type="bibr" rid="CR12">12</xref></sup>. Histopathological examination remains the &#8220;gold standard&#8221;, providing definitive diagnostic details through biopsy and microscopic analysis<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. However, it is invasive, time-consuming, and inaccessible in resource-limited settings<sup><xref ref-type="bibr" rid="CR14">14</xref>,<xref ref-type="bibr" rid="CR15">15</xref></sup>.</p><p id="Par6">Challenges with traditional methods include interobserver variability, leading to misdiagnoses, and subjective reliance on visual interpretation<sup><xref ref-type="bibr" rid="CR16">16</xref>&#8211;<xref ref-type="bibr" rid="CR18">18</xref></sup>. Histopathology can also be error-prone with inadequate samples. These limitations, compounded by a shortage of specialists, highlight the need for objective, scalable, and efficient diagnostic tools<sup><xref ref-type="bibr" rid="CR19">19</xref>,<xref ref-type="bibr" rid="CR20">20</xref></sup>.</p><p id="Par7">Whole slide imaging (WSI) has transformed pathology by digitizing glass slides, enabling applications in diagnostics, education, and research<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>. WSI facilitates telepathology and algorithm-based decision support but faces adoption challenges, including technical issues and diagnostic difficulties<sup><xref ref-type="bibr" rid="CR22">22</xref>,<xref ref-type="bibr" rid="CR23">23</xref></sup>. AI integration has expanded WSI&#8217;s potential, particularly in cancer diagnosis, where AI algorithms improve accuracy and reduce turnaround times<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup>. AI also enhances radiology, pharmacology, and personalized medicine, identifying patterns imperceptible to humans<sup><xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup>.</p><p id="Par8">Despite its promise, skepticism about AI persists due to interpretability concerns, ethical considerations, and unclear reimbursement opportunities<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. Effective communication is needed to build trust and demonstrate AI&#8217;s benefits<sup><xref ref-type="bibr" rid="CR21">21</xref></sup>.</p><p id="Par9">AI, particularly deep learning (DL), offers solutions for skin cancer diagnosis by analyzing dermatoscopic and histopathological images<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>. Using datasets like HAM10000, AI models identify lesion patterns and assist clinicians, improving efficiency and accessibility in underserved areas<sup><xref ref-type="bibr" rid="CR27">27</xref>&#8211;<xref ref-type="bibr" rid="CR29">29</xref></sup>. This integration represents a paradigm shift, enhancing outcomes<sup><xref ref-type="bibr" rid="CR30">30</xref>,<xref ref-type="bibr" rid="CR31">31</xref></sup>. DL automates feature extraction, with CNNs achieving state-of-the-art performance in lesion segmentation and classification<sup><xref ref-type="bibr" rid="CR32">32</xref>&#8211;<xref ref-type="bibr" rid="CR35">35</xref></sup>. For example, U-Net captures fine-grained lesion details critical for accurate classification<sup><xref ref-type="bibr" rid="CR36">36</xref>,<xref ref-type="bibr" rid="CR37">37</xref></sup>. Challenges remain, such as overfitting, driven by limited, diverse datasets<sup><xref ref-type="bibr" rid="CR38">38</xref>,<xref ref-type="bibr" rid="CR39">39</xref></sup>.</p><p id="Par10">To address these challenges, researchers have begun exploring hybrid approaches that combine multiple DL techniques<sup><xref ref-type="bibr" rid="CR40">40</xref></sup>. For example, pretrained models such as Virchow2, which utilize histopathological data, can extract high-level embeddings that capture intricate patterns in skin lesions. Similarly, vision-based approaches such as Nomic can complement these embeddings by providing additional contextual information.</p><p id="Par11">Therefore, the current study aims to address the challenges associated with skin lesion diagnosis by using advanced DL techniques and a dual-stream methodology to improve skin cancer detection accuracy, efficiency, and interpretability. Using the HAM10000 (HAM10K) dataset, this research employs the U-Net architecture for precise lesion segmentation, ensuring accurate localization of affected areas, critical for subsequent feature extraction and classification.</p><p id="Par12">To enhance diagnostic performance, this study introduces a dual-stream approach: the first track uses Virchow2, a pretrained model that uses dermatoscopic data to extract high-level embeddings capturing intricate patterns in skin lesions, whereas the second track employs a vision-based approach in which Nomic is used to extract spatial and contextual information from these images. This study aims to create a comprehensive diagnostic system to improve classification accuracy and robustness by integrating these methodologies.</p><p id="Par13">The rest of the manuscript is organized in the following manner: Section 2 provides an overview of the relevant research. Section 3 introduces the proposed approach for skin lesion diagnosis. Section 4 evaluates the proposed dual-stream methodology, and its performance was assessed via a comprehensive set of metrics. Finally, Section 5 concludes the work and summarizes potential directions for further research.</p></sec><sec id="Sec2"><title>Related studies</title><p id="Par14">The field of skin lesion classification has undergone significant advancements in recent years, with DL techniques playing a pivotal role in improving diagnostic accuracy. In 2022, Ali et al. introduced a multiclass skin cancer classification system using EfficientNets, achieving an F1 score of 87% and a top-1 accuracy of 87.91% on the HAM10000 dataset<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>. Their work demonstrated the effectiveness of transfer learning and fine-tuning in handling imbalanced datasets. However, the study highlighted that increased model complexity does not always lead to better performance, suggesting the need for more efficient architectures.</p><p id="Par15">Similarly, Shetty et al. proposed a machine learning and convolutional neural network (CNN) approach for skin lesion classification, achieving a training accuracy of 95.18% and a testing accuracy of 86.43% with a customized CNN model<sup><xref ref-type="bibr" rid="CR42">42</xref></sup>. Their work emphasized the importance of data augmentation and k-fold cross-validation to address class imbalance and improve model robustness. Despite these advancements, the reliance on traditional CNNs has limited their ability to capture more complex patterns in skin lesions, and the significant gap between training accuracy and testing accuracy indicates potential overfitting.</p><p id="Par16">In 2024, Adebiyi et al. explored multimodal learning by combining skin lesion images with patient metadata (age, sex, and lesion location) via the ALBEF model<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>. Their approach achieved an accuracy of 94.11% and an AUCROC of 0.9426 but with a lower recall of 90.19%, outperforming traditional DL models that rely solely on images. This study demonstrated the potential of multimodal learning to improve diagnostic accuracy, particularly in primary care settings where access to expert dermatologists is limited. However, the study was limited by the limited metadata available, suggesting that incorporating additional clinical data could further enhance performance.</p><p id="Par17">Recent advancements in transformer-based models have also contributed to the field. Xin et al. introduced an improved transformer network for skin cancer classification, achieving state-of-the-art performance on the ISIC dataset<sup><xref ref-type="bibr" rid="CR44">44</xref></sup>. Their work demonstrated the potential of transformers to capture long-range dependencies in dermatoscopic images, but it also highlighted the challenges of training such models on limited medical datasets.</p><p id="Par18">In addition, Mao et al. proposed the medical supervised masked autoencoder (MSMAE), which introduced a better masking strategy and fine-tuning schedule for medical image classification<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. Their approach improved the efficiency of transformer-based models, making them more suitable for skin lesion classification tasks.</p><p id="Par19">The reviewed studies collectively highlight several limitations that the current research aims to address. First, while models such as EfficientNets and SBXception have strong performance, they often struggle with class imbalance and computational efficiency. The current study addresses this issue by employing data augmentation and a dual-stream approach that balances accuracy and computational overhead. Second, while multimodal learning, as demonstrated by Adebiyi et al., has shown promise, it is limited by the availability of metadata<sup><xref ref-type="bibr" rid="CR43">43</xref></sup>.</p><p id="Par20">Third, while transformer-based models proposed by Xin et al.<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> and Mao et al.<sup><xref ref-type="bibr" rid="CR45">45</xref></sup> have shown potential, they require significant computational resources. The current study integrates efficient feature extraction methods and pretrained models to reduce computational complexity while maintaining high accuracy. The current research aims to create a robust and efficient diagnostic system for skin lesion classification by addressing these limitations.</p></sec><sec id="Sec3"><title>Methodology</title><p id="Par21">In this section, we introduce the proposed approach for skin lesion diagnosis (see Fig.&#160;<xref rid="Fig1" ref-type="fig">1</xref>), which uses a dual-stream framework combining histopathological-inherited and vision-based feature extraction. The methodology begins with Data Acquisition, where dermatoscopic images are collected and preprocessed to ensure high-quality input for the model. These images are then passed through a U-Net Model for precise lesion segmentation, generating predicted regions of interest (ROIs) that highlight the affected areas.</p><p id="Par22">The segmented ROIs are processed in parallel by two streams: the embedded stream network using Virchow2 for histopathologically inherited feature extraction and the vision stream network using Nomic for vision-based feature extraction. The features extracted from both streams are fused and integrated to create a comprehensive representation of the lesion, which is then used for classification. The model is trained via a carefully designed loss function and evaluated via a set of performance metrics to ensure robust and accurate diagnosis.<fig id="Fig1" position="float" orientation="portrait"><label>Fig. 1</label><caption><p>Graphical representation of the proposed dual-stream framework for skin lesion diagnosis. It consists of: (1) Data Acquisition, where dermatoscopic images are collected and preprocessed; (2) U-Net Model, which performs lesion segmentation to generate predicted ROIs; (3) Embedded Stream Network, which uses Virchow2 to extract histopathologically inherited features; (4) Vision Stream Network, which uses Nomic to extract vision-based features; (5) Feature Fusion and Integration, where features from both streams are combined; and (6) Performance Evaluation, where the model&#8217;s performance is assessed via metrics such as accuracy and F1 score.</p></caption><graphic id="MO1" position="float" orientation="portrait" xlink:href="41598_2025_1319_Fig1_HTML.jpg"/></fig></p><sec id="Sec4"><title>Preprocessing of the dataset</title><p id="Par23">The HAM10K dataset, consisting of 10,015 dermatoscopic images across seven common skin lesion types, was preprocessed to ensure consistency and compatibility with the dual-stream framework. The preprocessing pipeline included several key steps to enhance data quality and facilitate robust model training.</p><p id="Par24">First, all images were resized to a uniform resolution of <inline-formula id="IEq1"><alternatives><tex-math id="d33e384">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$224 \times 224$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq1.gif"/></alternatives></inline-formula> pixels to standardize input dimensions while preserving critical details for feature extraction. This resizing step also ensured computational efficiency during training and inference. To address class imbalance, a combination of undersampling and oversampling techniques was applied, ensuring that each skin lesion type was adequately represented in the training set.</p><p id="Par25">Next, data augmentation was performed to simulate real-world variations in lesion appearance and improve the model&#8217;s generalizability. Augmentation techniques included random rotations, horizontal and vertical flipping, scaling, and color jittering. These transformations introduced variability in lesion orientation, size, and color, mimicking the diversity observed in clinical settings.</p><p id="Par26">The dataset was then split into training, validation, and testing subsets using a stratified approach to maintain proportional representation of each lesion type across all splits. Specifically, 70% of the dataset was allocated for training, 15% for validation, and the remaining 15% for testing. This partitioning ensures a robust evaluation of the model&#8217;s performance on unseen data, as described in Equation&#160;<xref rid="Equ1" ref-type="disp-formula">1</xref> where <italic toggle="yes">N</italic> represents the total number of images in the dataset.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e400">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} N_{\text {train}} = 0.7 \times N, \quad N_{\text {val}} = 0.15 \times N, \quad N_{\text {test}} = 0.15 \times N \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ1.gif"/></alternatives></disp-formula>Finally, pixel intensity normalization was applied to all images to standardize the range of input values. Each image&#8217;s pixel values were scaled to the range [0,&#160;1], ensuring consistent input distributions for the deep learning models. This normalization step improved convergence speed during training and enhanced the stability of the model. By implementing these preprocessing steps, the dataset was optimized for use in the dual-stream framework, enabling accurate and efficient training while ensuring the model&#8217;s ability to generalize across diverse clinical scenarios.</p></sec><sec id="Sec5"><title>Lesion segmentation using U-Net</title><p id="Par27">Accurate lesion segmentation is a critical step in skin lesion diagnosis, as it enables precise localization of affected areas, which is essential for subsequent feature extraction and classification<sup><xref ref-type="bibr" rid="CR46">46</xref></sup>. This study employs the U-Net architecture for lesion segmentation because of its proven effectiveness in medical image analysis tasks.</p><p id="Par28">U-Net is a convolutional neural network (CNN) designed explicitly for biomedical image segmentation and is characterized by its encoder-decoder structure with skip connections that facilitate the preservation of spatial information<sup><xref ref-type="bibr" rid="CR47">47</xref></sup>. The encoder captures contextual features through a series of convolutional and pooling layers. In contrast, the decoder reconstructs the spatial resolution of the feature maps to produce a pixel-wise segmentation mask<sup><xref ref-type="bibr" rid="CR48">48</xref></sup>.</p><p id="Par29">The U-Net architecture is defined by the following key components: an encoder, a decoder, and a loss function. The encoder consists of a series of convolutional layers followed by max-pooling layers<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>. Each convolutional layer applies a set of filters to the input image, extracting hierarchical features. The output of the <italic toggle="yes">i</italic>-th convolutional layer can be expressed as in Equation&#160;<xref rid="Equ2" ref-type="disp-formula">2</xref>, where <inline-formula id="IEq2"><alternatives><tex-math id="d33e437">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq2.gif"/></alternatives></inline-formula> represents the feature maps at layer <italic toggle="yes">i</italic>, <inline-formula id="IEq3"><alternatives><tex-math id="d33e447">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$W_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq3.gif"/></alternatives></inline-formula> and <inline-formula id="IEq4"><alternatives><tex-math id="d33e453">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$b_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq4.gif"/></alternatives></inline-formula> are the learnable weights and biases, <inline-formula id="IEq5"><alternatives><tex-math id="d33e459">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$*$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq5.gif"/></alternatives></inline-formula> denotes the convolution operation, and <inline-formula id="IEq6"><alternatives><tex-math id="d33e465">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq6.gif"/></alternatives></inline-formula> is the activation function (e.g., ReLU). The max pooling operation reduces the spatial dimensions of the feature maps, enabling the network to capture broader contextual information.<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="d33e471">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} F_i = \sigma (W_i *F_{i-1} + b_i) \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ2.gif"/></alternatives></disp-formula>The decoder pathway reconstructs the spatial resolution of the feature maps through upsampling and convolutional layers. The upsampling operation is followed by concatenation with the corresponding feature maps from the encoder pathway via skip connections<sup><xref ref-type="bibr" rid="CR50">50</xref></sup>. This process can be described as in Equation&#160;<xref rid="Equ3" ref-type="disp-formula">3</xref>, where <inline-formula id="IEq7"><alternatives><tex-math id="d33e486">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F_j$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq7.gif"/></alternatives></inline-formula> represents the feature maps at the <italic toggle="yes">j</italic>-th decoder layer, and <inline-formula id="IEq8"><alternatives><tex-math id="d33e495">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {Upsample}(\cdot )$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq8.gif"/></alternatives></inline-formula> denotes the upsampling operation. The skip connections ensure that fine-grained details from the encoder are preserved, enabling precise localization of lesion boundaries.<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="d33e501">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} F_j = \sigma (W_j *\text {Upsample}(F_{j+1}) + b_j) \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ3.gif"/></alternatives></disp-formula>The utilized U-Net model (see Fig.&#160;<xref rid="Fig2" ref-type="fig">2</xref>) is trained via the Dice loss function, which is particularly suitable for segmentation tasks with imbalanced class distributions<sup><xref ref-type="bibr" rid="CR51">51</xref></sup>. The Dice loss is defined as in Equation&#160;<xref rid="Equ4" ref-type="disp-formula">4</xref>, where <inline-formula id="IEq9"><alternatives><tex-math id="d33e519">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_p$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq9.gif"/></alternatives></inline-formula> and <inline-formula id="IEq10"><alternatives><tex-math id="d33e525">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{y}_p$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq10.gif"/></alternatives></inline-formula> represent the ground truth and predicted segmentation masks, respectively, for pixel <italic toggle="yes">p</italic>. Dice loss minimizes the discrepancy between the predicted and ground truth masks, ensuring accurate lesion segmentation.<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="d33e534">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \mathcal {L}_{\text {Dice}} = 1 - 2 \times \frac{\sum _p y_p \times \hat{y}_p}{\sum _p y_p + \sum _p \hat{y}_p} \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ4.gif"/></alternatives></disp-formula><fig id="Fig2" position="float" orientation="portrait"><label>Fig. 2</label><caption><p>Schematic of the utilized U-Net model showing the training and inference phases.</p></caption><graphic id="MO2" position="float" orientation="portrait" xlink:href="41598_2025_1319_Fig2_HTML.jpg"/></fig></p><p id="Par30">One of the key advantages of U-Net is its reduced complexity compared with other segmentation models, such as DeepLab or Mask R-CNN. This reduction in complexity not only simplifies the training process but also reduces latency, making the model more suitable for real-time applications<sup><xref ref-type="bibr" rid="CR52">52</xref></sup>. Furthermore, the lesions in the HAM10000 dataset are generally large enough to be effectively analyzed via a moderately complex model such as U-Net<sup><xref ref-type="bibr" rid="CR53">53</xref></sup>.</p><p id="Par31">Employing an overly complex and advanced model for this task would be computationally inefficient and unnecessary, as the additional complexity would not yield significant improvements in segmentation accuracy. Instead, U-Net strikes an optimal balance between performance and computational efficiency, making it an ideal choice for this study.</p><p id="Par32">The output of the U-Net model is a binary segmentation mask that precisely delineates the lesion region. This mask then isolates the lesion from the background, enabling focused feature extraction and classification in subsequent pipeline stages. By utilizing U-Net&#8217;s ability to capture fine-grained details while maintaining computational efficiency, this study ensures accurate and efficient lesion segmentation, laying the foundation for robust skin lesion diagnosis.</p></sec><sec id="Sec6"><title>Dual-stream feature extraction</title><p id="Par33">Recent advancements in transformer-based architectures, such as Vision Transformers (ViTs) and Swin Transformers<sup><xref ref-type="bibr" rid="CR54">54</xref>&#8211;<xref ref-type="bibr" rid="CR56">56</xref></sup>, have demonstrated superior performance in capturing long-range dependencies within images, making them particularly well-suited for medical image analysis<sup><xref ref-type="bibr" rid="CR57">57</xref></sup>. In line with this, the proposed dual-stream framework utilizes transformer-based models for both feature extraction streams.</p><p id="Par34">Specifically, Virchow2, a self-supervised Vision Transformer pretrained on 3.1 million whole-slide histopathology images, is employed to extract high-level histopathological features. Its architecture inherently excels at capturing intricate patterns and subtle morphological characteristics of skin lesions, enabling state-of-the-art performance in various computational pathology tasks<sup><xref ref-type="bibr" rid="CR58">58</xref>,<xref ref-type="bibr" rid="CR59">59</xref></sup>. Similarly, Nomic, the second stream&#8217;s feature extractor, utilizes a vision-based transformer architecture to analyze spatial and contextual information from dermatoscopic images<sup><xref ref-type="bibr" rid="CR60">60</xref></sup>.</p><p id="Par35">By integrating these transformer-based models, the framework ensures optimal extraction of both global and local features, addressing the reviewer&#8217;s recommendation to enhance the capture of subtle lesion characteristics. This design choice underscores the robustness of the proposed approach in overcoming the challenges associated with skin lesion diagnosis.</p><p id="Par36">Virchow2 incorporates a multilayer perceptron (MLP) with gated linear unit (GLU) style gating and sigmoid linear unit (SiLU) activation functions. The GLU mechanism allows for dynamic feature selection, enhancing the model&#8217;s ability to focus on relevant patterns in the histopathological data. Register tokens are ignored during processing, ensuring that only meaningful patch and class tokens contribute to the final embeddings.</p><p id="Par37">The embeddings are derived from both class tokens <inline-formula id="IEq11"><alternatives><tex-math id="d33e596">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{c}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq11.gif"/></alternatives></inline-formula> and patch tokens <inline-formula id="IEq12"><alternatives><tex-math id="d33e602">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{P}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq12.gif"/></alternatives></inline-formula>, which are concatenated to form a comprehensive feature representation as in Equation&#160;<xref rid="Equ5" ref-type="disp-formula">5</xref>, where <inline-formula id="IEq13"><alternatives><tex-math id="d33e611">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{c} \in \mathbb {R}^d$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq13.gif"/></alternatives></inline-formula> is the class token, <inline-formula id="IEq14"><alternatives><tex-math id="d33e617">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{P}_i \in \mathbb {R}^d$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq14.gif"/></alternatives></inline-formula> represents the <italic toggle="yes">i</italic>-th patch token, <italic toggle="yes">N</italic> is the number of patches, and <inline-formula id="IEq15"><alternatives><tex-math id="d33e630">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Vert$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq15.gif"/></alternatives></inline-formula> denotes concatenation. This ensures that the model captures both the global and local features critical for accurate diagnosis.<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="d33e636">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \textbf{E}_{\text {Virchow2}} = \left[ \textbf{c} \, \Vert \, \frac{1}{N} \times \sum _{i=1}^{N} \textbf{P}_i \right] \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ5.gif"/></alternatives></disp-formula>The second track employs Nomic, a vision-based feature extraction model, to analyze spatial and contextual information from the same set of images. Nomic processes the images through a transformer-based architecture, extracting embeddings <inline-formula id="IEq16"><alternatives><tex-math id="d33e643">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_{\text {Nomic}} \in \mathbb {R}^d$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq16.gif"/></alternatives></inline-formula> that encode visual patterns such as texture, color, and structural irregularities. The Nomic model uses a Nomic Processor to preprocess the input images, followed by a transformer backbone that generates a last hidden state. The class token from this hidden state is extracted and normalized via L2 normalization, as defined in Equation&#160;<xref rid="Equ6" ref-type="disp-formula">6</xref>, to ensure consistent scaling of the feature vectors where <inline-formula id="IEq17"><alternatives><tex-math id="d33e653">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{h}_{\text {cls}} \in \mathbb {R}^d$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq17.gif"/></alternatives></inline-formula> represents the class token extracted from the last hidden state of the transformer.<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="d33e659">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \textbf{E}_{\text {Nomic}} = \frac{\textbf{h}_{\text {cls}}}{\Vert \textbf{h}_{\text {cls}}\Vert _2} \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ6.gif"/></alternatives></disp-formula>Combining these embeddings with the histopathological-inherited features from Virchow2 provides the model with a more holistic understanding of the lesion characteristics, enhancing its diagnostic capabilities. The fusion process integrates global and local histopathological features with spatial and contextual visual patterns, enabling the model to achieve superior performance in skin lesion classification.</p></sec><sec id="Sec7"><title>Feature fusion and integration</title><p id="Par38">The features extracted from both tracks are integrated through a fusion mechanism that combines the strengths of histopathologically inherited and vision-based data. The embeddings from Virchow2 and Nomic are normalized and concatenated, ensuring that the model retains the discriminative power of both feature sets. The fusion process can be expressed as in Equation&#160;<xref rid="Equ7" ref-type="disp-formula">7</xref>, where <inline-formula id="IEq18"><alternatives><tex-math id="d33e673">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {Norm}(\cdot )$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq18.gif"/></alternatives></inline-formula> denotes L2 normalization.<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="d33e679">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \textbf{E}_{\text {fused}}= &amp; \text {Norm}(\textbf{E}_{\text {Virchow2}}) \, \Vert \, \text {Norm}(\textbf{E}_{\text {Nomic}}) \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ7.gif"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="d33e685">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {Norm}(\textbf{x})= &amp; \frac{\textbf{x}}{\Vert \textbf{x}\Vert _2} \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ8.gif"/></alternatives></disp-formula>This fusion process is followed by a series of fully connected (FC) dense layers, which further refine the combined feature representation. Each dense block consists of an FC layer, a LeakyReLU activation function, and a dropout layer to prevent overfitting. Integrating these features is critical for capturing the complex interplay between morphological and visual characteristics, enabling the model to make more informed decisions during classification.</p></sec><sec id="Sec8"><title>Classification model</title><p id="Par39">The classification model is built upon a deep neural network architecture incorporating fused feature representations. The model consists of a multilayer perceptron (MLP) with dropout and batch normalization layers to prevent overfitting and improve generalizability. The output logits <inline-formula id="IEq19"><alternatives><tex-math id="d33e696">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{z}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq19.gif"/></alternatives></inline-formula> are computed via Equation&#160;<xref rid="Equ9" ref-type="disp-formula">9</xref>, where <inline-formula id="IEq20"><alternatives><tex-math id="d33e705">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{W}_1, \textbf{W}_2$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq20.gif"/></alternatives></inline-formula> are weight matrices, <inline-formula id="IEq21"><alternatives><tex-math id="d33e711">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{b}_1, \textbf{b}_2$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq21.gif"/></alternatives></inline-formula> are bias terms, and ReLU is the activation function.<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="d33e717">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \textbf{z} = \textbf{W}_2 \times \text {ReLU}(\textbf{W}_1 \times \textbf{E}_{\text {fused}} + \textbf{b}_1) + \textbf{b}_2 \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ9.gif"/></alternatives></disp-formula>The final probabilities <inline-formula id="IEq22"><alternatives><tex-math id="d33e725">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{p}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq22.gif"/></alternatives></inline-formula> are obtained via the Softmax function presented in Equation&#160;<xref rid="Equ10" ref-type="disp-formula">10</xref>, where <italic toggle="yes">C</italic> is the number of classes.<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="d33e737">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} p_i = \frac{\exp (z_i)}{\sum _{j=1}^C \exp (z_j)} \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ10.gif"/></alternatives></disp-formula>The model is trained via a cross-entropy loss function, as expressed in Equation&#160;<xref rid="Equ11" ref-type="disp-formula">11</xref>, where <inline-formula id="IEq23"><alternatives><tex-math id="d33e747">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq23.gif"/></alternatives></inline-formula> is the ground truth label. The inclusion of both histopathologically inherited and vision-based features allows the model to achieve high classification accuracy while maintaining robustness to variations in the input data.<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="d33e754">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \mathcal {L} = -\sum _{i=1}^C y_i \times \log (p_i) \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ11.gif"/></alternatives></disp-formula>To ensure efficient training and convergence, the learning rate was dynamically adjusted using a ReduceLROnPlateau scheduler. This scheduler reduces the learning rate when the validation loss plateaus, as defined in Equation&#160;<xref rid="Equ12" ref-type="disp-formula">12</xref>, where <inline-formula id="IEq24"><alternatives><tex-math id="d33e764">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\eta _t$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq24.gif"/></alternatives></inline-formula> is the learning rate at epoch <italic toggle="yes">t</italic>, <inline-formula id="IEq25"><alternatives><tex-math id="d33e773">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\eta _{t-1}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq25.gif"/></alternatives></inline-formula> is the learning rate at the previous epoch, and <inline-formula id="IEq26"><alternatives><tex-math id="d33e779">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\alpha$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq26.gif"/></alternatives></inline-formula> is the reduction factor applied when the validation loss stops improving.<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="d33e786">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \eta _t = {\left\{ \begin{array}{ll} \eta _{t-1} &amp; \text {if } \mathcal {L}_{\text {val}}(t) &lt; \mathcal {L}_{\text {val}}(t-1), \\ \alpha \times \eta _{t-1} &amp; \text {if } \mathcal {L}_{\text {val}}(t) \ge \mathcal {L}_{\text {val}}(t-1). \end{array}\right. } \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ12.gif"/></alternatives></disp-formula>This approach effectively balances computational efficiency and model performance, ensuring stable convergence without excessive computational overhead. While techniques such as Cosine Annealing or OneCycleLR can be potential alternatives for improving convergence speed and avoiding local minima, the current strategy already demonstrates robust performance across multiple trials. Nevertheless, future suggestions is to explore the integration of these advanced learning rate schedules to further optimize the model&#8217;s training dynamics and enhance its adaptability to complex datasets.</p></sec><sec id="Sec9"><title>Model evaluation and validation</title><p id="Par40">The performance of the proposed model is evaluated via a comprehensive set of metrics, including precision, recall, F1 score, accuracy, and specificity. These metrics are computed via weighted averaging techniques to account for class imbalance in the dataset<sup><xref ref-type="bibr" rid="CR61">61</xref></sup>. For example, weighted precision <inline-formula id="IEq27"><alternatives><tex-math id="d33e801">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {precision}_{\text {weighted}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq27.gif"/></alternatives></inline-formula> is calculated via Equation&#160;<xref rid="Equ13" ref-type="disp-formula">13</xref>, where <inline-formula id="IEq28"><alternatives><tex-math id="d33e810">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$w_i = \frac{n_i}{N}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq28.gif"/></alternatives></inline-formula> is the weight for class <italic toggle="yes">i</italic>, <inline-formula id="IEq29"><alternatives><tex-math id="d33e820">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq29.gif"/></alternatives></inline-formula> is the number of samples in class <italic toggle="yes">i</italic>, and <italic toggle="yes">N</italic> is the total number of samples.<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="d33e832">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {Precision}_{\text {weighted}} = \sum _{i=1}^C w_i \times \frac{\text {TP}_i}{\text {TP}_i + \text {FP}_i} \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ13.gif"/></alternatives></disp-formula>Similarly, the weighted F1 score is computed via Equation&#160;<xref rid="Equ14" ref-type="disp-formula">14</xref>, where <inline-formula id="IEq30"><alternatives><tex-math id="d33e842">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {precision}_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq30.gif"/></alternatives></inline-formula> and <inline-formula id="IEq31"><alternatives><tex-math id="d33e849">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {recall}_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq31.gif"/></alternatives></inline-formula> are the precision and recall for class <italic toggle="yes">i</italic>, respectively<sup><xref ref-type="bibr" rid="CR62">62</xref></sup>.<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="d33e862">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {F1}_{\text {weighted}} = \sum _{i=1}^C w_i \times \frac{2 \times \text {Precision}_i \times \text {Recall}_i}{\text {Precision}_i + \text {Recall}_i} \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ14.gif"/></alternatives></disp-formula>Weighted accuracy is defined as in Equation&#160;<xref rid="Equ15" ref-type="disp-formula">15</xref>, where <inline-formula id="IEq32"><alternatives><tex-math id="d33e872">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {TN}_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq32.gif"/></alternatives></inline-formula> and <inline-formula id="IEq33"><alternatives><tex-math id="d33e879">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\text {FN}_i$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq33.gif"/></alternatives></inline-formula> are the true negatives and false negatives for class <italic toggle="yes">i</italic>, respectively.<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="d33e888">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \text {Accuracy}_{\text {weighted}} = \sum _{i=1}^C w_i \times \frac{\text {TP}_i + \text {TN}_i}{\text {TP}_i + \text {TN}_i + \text {FP}_i + \text {FN}_i} \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ15.gif"/></alternatives></disp-formula>The model is validated through a series of experiments, including k-fold cross-validation and independent testing on unseen data. Additionally, dimensionality reduction techniques such as PCA, t-SNE, and UMAP are employed to visualize the feature embeddings, providing insights into the model&#8217;s ability to separate different classes in the latent space. For example, PCA reduces the dimensionality of the embeddings <inline-formula id="IEq34"><alternatives><tex-math id="d33e895">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_{\text {fused}} \in \mathbb {R}^d$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq34.gif"/></alternatives></inline-formula> to <inline-formula id="IEq35"><alternatives><tex-math id="d33e901">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_{\text {PCA}} \in \mathbb {R}^2$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq35.gif"/></alternatives></inline-formula> via Equation&#160;<xref rid="Equ16" ref-type="disp-formula">16</xref>, where <inline-formula id="IEq36"><alternatives><tex-math id="d33e911">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{U}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq36.gif"/></alternatives></inline-formula> and <inline-formula id="IEq37"><alternatives><tex-math id="d33e917">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\Sigma$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq37.gif"/></alternatives></inline-formula> are the eigenvectors and eigenvalues of the covariance matrix of <inline-formula id="IEq38"><alternatives><tex-math id="d33e923">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_{\text {fused}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq38.gif"/></alternatives></inline-formula>.<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="d33e929">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} \textbf{E}_{\text {PCA}} = \textbf{U} \times \Sigma \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_1319_Article_Equ16.gif"/></alternatives></disp-formula>The confusion matrix is used to analyze the model&#8217;s performance at the granular level, identifying potential areas for improvement. The results demonstrate that the dual-stream approach significantly enhances diagnostic accuracy compared to single-track models, making it a promising solution for detecting skin cancer.</p><p id="Par41">To illustrate the feature extraction and fusion process, consider a skin lesion image <inline-formula id="IEq39"><alternatives><tex-math id="d33e938">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{I}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq39.gif"/></alternatives></inline-formula>. Virchow2 first processes the image to extract histopathologically inherited features <inline-formula id="IEq40"><alternatives><tex-math id="d33e944">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_{\text {Virchow2}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq40.gif"/></alternatives></inline-formula>, and Nomic is used to extract vision-based features <inline-formula id="IEq41"><alternatives><tex-math id="d33e950">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_{\text {Nomic}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq41.gif"/></alternatives></inline-formula>. These features are normalized and concatenated to form the fused embedding <inline-formula id="IEq42"><alternatives><tex-math id="d33e956">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_{\text {fused}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq38.gif"/></alternatives></inline-formula>. The fused embedding is then passed through the classification model to produce the final probabilities <inline-formula id="IEq43"><alternatives><tex-math id="d33e962">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{p}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq22.gif"/></alternatives></inline-formula>, as shown in the following flow: <list list-type="order"><list-item><p id="Par42">Input Image: <inline-formula id="IEq44"><alternatives><tex-math id="d33e975">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{I}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq39.gif"/></alternatives></inline-formula></p></list-item><list-item><p id="Par43">Feature Extraction:<list list-type="bullet"><list-item><label/><p id="Par44">Virchow2: <inline-formula id="IEq45"><alternatives><tex-math id="d33e989">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_{\text {Virchow2}} = \left[ \textbf{c} \, \Vert \, \frac{1}{N} \times \sum _{i=1}^{N} \textbf{P}_i \right]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq45.gif"/></alternatives></inline-formula></p></list-item><list-item><label/><p id="Par45">Nomic: <inline-formula id="IEq46"><alternatives><tex-math id="d33e997">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_{\text {Nomic}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq41.gif"/></alternatives></inline-formula></p></list-item></list></p></list-item><list-item><p id="Par46">Feature Fusion: <inline-formula id="IEq47"><alternatives><tex-math id="d33e1007">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{E}_{\text {fused}} = \text {Norm}(\textbf{E}_{\text {Virchow2}}) \, \Vert \, \text {Norm}(\textbf{E}_{\text {Nomic}})$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq47.gif"/></alternatives></inline-formula></p></list-item><list-item><p id="Par47">Classification: <inline-formula id="IEq48"><alternatives><tex-math id="d33e1017">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\textbf{z} = \textbf{W}_2 \times \text {ReLU}(\textbf{W}_1 \times \textbf{E}_{\text {fused}} + \textbf{b}_1) + \textbf{b}_2$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq48.gif"/></alternatives></inline-formula></p></list-item><list-item><p id="Par48">Output Probabilities: <inline-formula id="IEq49"><alternatives><tex-math id="d33e1027">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$p_i = \frac{\exp (z_i)}{\sum _{j=1}^C \exp (z_j)}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_1319_Article_IEq49.gif"/></alternatives></inline-formula></p></list-item></list></p></sec><sec id="Sec10"><title>The proposed framework pseudocode</title><p id="Par49">The pseudocode for the proposed framework is outlined in Algorithm&#160;1. The algorithm provides a step-by-step description of the dual-stream deep learning framework for skin lesion diagnosis, encompassing data preprocessing, feature extraction using Virchow2 and Nomic, feature fusion, model training, and performance evaluation. Each step is designed to ensure robustness, generalizability, and interpretability of the model while addressing challenges such as class imbalance and complex lesion characteristics. The modular structure of the pseudocode reflects the key components of the framework, enabling reproducibility and facilitating future enhancements.</p><p id="Par50">
<fig position="anchor" id="Figa" orientation="portrait"><label>Algorithm 1</label><caption><p>The proposed dual-stream framework for skin lesion diagnosis.</p></caption><graphic position="anchor" id="MO3" orientation="portrait" xlink:href="41598_2025_1319_Figa_HTML.jpg"/></fig>
</p></sec></sec><sec id="Sec11"><title>Experiments and discussion</title><p id="Par51">The HAM10K dataset, which consists of 10,015 dermatoscopic images across seven common skin lesion types, was utilized in this study. To evaluate the proposed dual-stream framework, 70% of the dataset was used for training, 15% for validation, and the remaining 15% for testing. This split ensures a robust assessment of the model&#8217;s generalization capabilities on unseen data. The performance was assessed via a comprehensive set of metrics, including precision, recall, F1 score, accuracy, and specificity.</p><sec id="Sec12"><title>Segmentation performance</title><p id="Par52">The U-Net architecture was employed for precise lesion segmentation, ensuring accurate localization of affected areas. Figure&#160;<xref rid="Fig3" ref-type="fig">3</xref> shows the learning and validation curves for segmentation, including metrics such as loss, accuracy, Dice coefficient, intersection over union (IoU), recall, precision, and other key performance indicators. The curves indicate stable convergence during training, with minimal overfitting, demonstrating the model&#8217;s ability to generalize well to unseen data.<fig id="Fig3" position="float" orientation="portrait"><label>Fig. 3</label><caption><p>Training and validation curves for the U-Net segmentation model. The plots depict the loss, accuracy, Dice coefficient, intersection over union (IoU), recall, and precision over epochs, demonstrating stable convergence and robust performance.</p></caption><graphic id="MO4" position="float" orientation="portrait" xlink:href="41598_2025_1319_Fig3_HTML.jpg"/></fig></p><p id="Par53">Quantitative results from the segmentation process are summarized in Table&#160;<xref rid="Tab1" ref-type="table">1</xref>, showcasing the model&#8217;s robust performance across multiple evaluation metrics. Notably, the model achieved a Dice coefficient of 0.9169 , an IoU of 0.8493 , and a mean accuracy of 96.79% , reflecting its effectiveness in delineating lesion boundaries even in challenging cases with irregular shapes and textures. Additionally, the precision and recall values of 0.9146 and 0.9225 , respectively, highlight the model&#8217;s balanced performance in identifying both positive and negative regions. The Focal Tversky Loss and BCELoss were minimized to 0.1514 and 0.0754 , respectively, further confirming the model&#8217;s strong optimization during training.<table-wrap id="Tab1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Quantitative results for U-Net segmentation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Metric</th><th align="left" colspan="1" rowspan="1">Value</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">BCE Loss</td><td align="left" colspan="1" rowspan="1">0.0754</td></tr><tr><td align="left" colspan="1" rowspan="1">Dice Coefficient</td><td align="left" colspan="1" rowspan="1">0.9169</td></tr><tr><td align="left" colspan="1" rowspan="1">F1 Score</td><td align="left" colspan="1" rowspan="1">0.9169</td></tr><tr><td align="left" colspan="1" rowspan="1">Focal Tversky Loss</td><td align="left" colspan="1" rowspan="1">0.1514</td></tr><tr><td align="left" colspan="1" rowspan="1">IoU</td><td align="left" colspan="1" rowspan="1">0.8493</td></tr><tr><td align="left" colspan="1" rowspan="1">MSE Loss</td><td align="left" colspan="1" rowspan="1">0.0213</td></tr><tr><td align="left" colspan="1" rowspan="1">Mean Absolute Distance</td><td align="left" colspan="1" rowspan="1">0.0432</td></tr><tr><td align="left" colspan="1" rowspan="1">Precision</td><td align="left" colspan="1" rowspan="1">0.9146</td></tr><tr><td align="left" colspan="1" rowspan="1">Recall</td><td align="left" colspan="1" rowspan="1">0.9225</td></tr><tr><td align="left" colspan="1" rowspan="1">Tversky Index</td><td align="left" colspan="1" rowspan="1">0.9169</td></tr><tr><td align="left" colspan="1" rowspan="1">Accuracy</td><td align="left" colspan="1" rowspan="1">96.79%</td></tr><tr><td align="left" colspan="1" rowspan="1">Overall Loss</td><td align="left" colspan="1" rowspan="1">0.0754</td></tr></tbody></table></table-wrap></p><p id="Par54">Figure&#160;<xref rid="Fig4" ref-type="fig">4</xref> shows three prediction samples from the U-Net model. The columns represent the original dermatoscopic images, ground truth masks, and predicted masks, respectively. The visual results highlight the model&#8217;s ability to accurately segment skin lesions, even in cases with complex boundaries and varying textures. For example, the model successfully identifies lesions with heterogeneous pigmentation and irregular edges, which are often challenging for traditional segmentation methods. This level of precision is crucial for downstream tasks such as feature extraction and classification, as accurate segmentation ensures that the extracted features represent the lesion&#8217;s true characteristics.<fig id="Fig4" position="float" orientation="portrait"><label>Fig. 4</label><caption><p>Visualization of U-Net segmentation predictions. The columns display (from left to right) the following: original dermatoscopic images, ground truth masks, and predicted masks. The results highlight the model&#8217;s ability to segment skin lesions with complex boundaries and varying textures accurately.</p></caption><graphic id="MO5" position="float" orientation="portrait" xlink:href="41598_2025_1319_Fig4_HTML.jpg"/></fig></p></sec><sec id="Sec13"><title>Classification performance</title><p id="Par55">The classification performance of the proposed dual-stream approach is summarized in Table&#160;<xref rid="Tab2" ref-type="table">2</xref>. The results of over 10 trials were reported, with metrics including precision, recall, F1 score, accuracy, and specificity. The maximum, mean, standard deviation (Std), and confidence interval (CI) values are also provided. The model achieves a mean accuracy of 96.25% and a mean F1 score of 93.79%, demonstrating its robustness and consistency across trials. The high specificity values (mean of 91.82%) further indicate the model&#8217;s ability to correctly identify negative cases, which is critical for reducing false positives in skin cancer diagnosis.</p><p id="Par56">The precision and recall values, which are particularly important for medical applications, are consistently high across all trials. This indicates that the model strikes a balance between minimizing false positives (high precision) and false negatives (high recall), ensuring reliable diagnostic outcomes. The low standard deviation and narrow confidence intervals for all the metrics further underscore the stability and reproducibility of the proposed approach.<table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Performance metrics of the proposed dual-stream approach on the HAM10K dataset. The results are reported over 10 trials, including precision, recall, F1 score, accuracy, and specificity. The maximum, mean, standard deviation (Std), and confidence interval (CI) values are also provided.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Trials</th><th align="left" colspan="1" rowspan="1">Precision</th><th align="left" colspan="1" rowspan="1">Recall</th><th align="left" colspan="1" rowspan="1">F1</th><th align="left" colspan="1" rowspan="1">Accuracy</th><th align="left" colspan="1" rowspan="1">Specificity</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Trial 1</td><td align="left" colspan="1" rowspan="1">93.16%</td><td align="left" colspan="1" rowspan="1">93.21%</td><td align="left" colspan="1" rowspan="1">93.19%</td><td align="left" colspan="1" rowspan="1">95.85%</td><td align="left" colspan="1" rowspan="1">91.37%</td></tr><tr><td align="left" colspan="1" rowspan="1">Trial 2</td><td align="left" colspan="1" rowspan="1">93.80%</td><td align="left" colspan="1" rowspan="1">93.88%</td><td align="left" colspan="1" rowspan="1">93.84%</td><td align="left" colspan="1" rowspan="1">96.29%</td><td align="left" colspan="1" rowspan="1">92.08%</td></tr><tr><td align="left" colspan="1" rowspan="1">Trial 3</td><td align="left" colspan="1" rowspan="1">94.20%</td><td align="left" colspan="1" rowspan="1">94.21%</td><td align="left" colspan="1" rowspan="1">94.20%</td><td align="left" colspan="1" rowspan="1">96.39%</td><td align="left" colspan="1" rowspan="1">91.87%</td></tr><tr><td align="left" colspan="1" rowspan="1">Trial 4</td><td align="left" colspan="1" rowspan="1">93.43%</td><td align="left" colspan="1" rowspan="1">93.48%</td><td align="left" colspan="1" rowspan="1">93.45%</td><td align="left" colspan="1" rowspan="1">96.30%</td><td align="left" colspan="1" rowspan="1">92.33%</td></tr><tr><td align="left" colspan="1" rowspan="1">Trial 5</td><td align="left" colspan="1" rowspan="1">94.20%</td><td align="left" colspan="1" rowspan="1">94.21%</td><td align="left" colspan="1" rowspan="1">94.21%</td><td align="left" colspan="1" rowspan="1">96.60%</td><td align="left" colspan="1" rowspan="1">92.66%</td></tr><tr><td align="left" colspan="1" rowspan="1">Trial 6</td><td align="left" colspan="1" rowspan="1">93.97%</td><td align="left" colspan="1" rowspan="1">93.88%</td><td align="left" colspan="1" rowspan="1">93.93%</td><td align="left" colspan="1" rowspan="1">96.16%</td><td align="left" colspan="1" rowspan="1">91.10%</td></tr><tr><td align="left" colspan="1" rowspan="1">Trial 7</td><td align="left" colspan="1" rowspan="1">94.12%</td><td align="left" colspan="1" rowspan="1">94.15%</td><td align="left" colspan="1" rowspan="1">94.13%</td><td align="left" colspan="1" rowspan="1">96.57%</td><td align="left" colspan="1" rowspan="1">92.37%</td></tr><tr><td align="left" colspan="1" rowspan="1">Trial 8</td><td align="left" colspan="1" rowspan="1">94.27%</td><td align="left" colspan="1" rowspan="1">94.21%</td><td align="left" colspan="1" rowspan="1">94.24%</td><td align="left" colspan="1" rowspan="1">96.55%</td><td align="left" colspan="1" rowspan="1">92.54%</td></tr><tr><td align="left" colspan="1" rowspan="1">Trial 9</td><td align="left" colspan="1" rowspan="1">93.62%</td><td align="left" colspan="1" rowspan="1">93.61%</td><td align="left" colspan="1" rowspan="1">93.62%</td><td align="left" colspan="1" rowspan="1">95.73%</td><td align="left" colspan="1" rowspan="1">90.52%</td></tr><tr><td align="left" colspan="1" rowspan="1">Trial 10</td><td align="left" colspan="1" rowspan="1">93.07%</td><td align="left" colspan="1" rowspan="1">93.15%</td><td align="left" colspan="1" rowspan="1">93.11%</td><td align="left" colspan="1" rowspan="1">96.10%</td><td align="left" colspan="1" rowspan="1">91.39%</td></tr><tr><td align="left" colspan="1" rowspan="1">Max</td><td align="left" colspan="1" rowspan="1">94.27%</td><td align="left" colspan="1" rowspan="1">94.21%</td><td align="left" colspan="1" rowspan="1">94.24%</td><td align="left" colspan="1" rowspan="1">96.60%</td><td align="left" colspan="1" rowspan="1">92.66%</td></tr><tr><td align="left" colspan="1" rowspan="1">Mean</td><td align="left" colspan="1" rowspan="1">93.78%</td><td align="left" colspan="1" rowspan="1">93.80%</td><td align="left" colspan="1" rowspan="1">93.79%</td><td align="left" colspan="1" rowspan="1">96.25%</td><td align="left" colspan="1" rowspan="1">91.82%</td></tr><tr><td align="left" colspan="1" rowspan="1">Std</td><td align="left" colspan="1" rowspan="1">0.00444</td><td align="left" colspan="1" rowspan="1">0.00415</td><td align="left" colspan="1" rowspan="1">0.00428</td><td align="left" colspan="1" rowspan="1">0.00298</td><td align="left" colspan="1" rowspan="1">0.00704</td></tr><tr><td align="left" colspan="1" rowspan="1">CI</td><td align="left" colspan="1" rowspan="1">0.00275</td><td align="left" colspan="1" rowspan="1">0.00257</td><td align="left" colspan="1" rowspan="1">0.00266</td><td align="left" colspan="1" rowspan="1">0.00184</td><td align="left" colspan="1" rowspan="1">0.00437</td></tr></tbody></table></table-wrap></p></sec><sec id="Sec14"><title>Ablation studies</title><p id="Par57">Ablation studies were conducted to evaluate the contribution of each component in the dual-stream framework. Table&#160;<xref rid="Tab3" ref-type="table">3</xref> presents the performance metrics when either the embedding branch (Virchow2) or the vision branch (Nomic) is removed. The results show a significant decrease in performance when the embedding branch is removed, with the precision and F1 score decreasing to 61.33% and 64.01%, respectively. This highlights the importance of histopathologically inherited features in capturing domain-specific patterns for accurate diagnosis. The embedding branch, which uses Virchow2, provides high-level morphological information critical for distinguishing between different skin lesions.</p><p id="Par58">Similarly, removing the vision branch results in a noticeable decline in performance, with precision and the F1 score decreasing to 85.51% and 85.50%, respectively. This underscores the complementary role of vision-based features in enhancing the model&#8217;s diagnostic capabilities. The vision branch, powered by Nomic, captures spatial and contextual information such as texture, color, and structural irregularities, which are essential for identifying subtle differences between lesion types. The ablation studies demonstrate that both branches contribute uniquely to the model&#8217;s performance, and their integration is key to achieving state-of-the-art results.<table-wrap id="Tab3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Performance metrics of ablation studies evaluating the contribution of each component in the dual-stream framework. The results are reported for scenarios where either the embedding branch (Virchow2) or the vision branch (Nomic) is removed.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Study</th><th align="left" colspan="1" rowspan="1">Precision</th><th align="left" colspan="1" rowspan="1">Recall</th><th align="left" colspan="1" rowspan="1">F1</th><th align="left" colspan="1" rowspan="1">Accuracy</th><th align="left" colspan="1" rowspan="1">Specificity</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Removal of Embedding</td><td align="left" colspan="1" rowspan="1">61.33%</td><td align="left" colspan="1" rowspan="1">66.93%</td><td align="left" colspan="1" rowspan="1">64.01%</td><td align="left" colspan="1" rowspan="1">75.02%</td><td align="left" colspan="1" rowspan="1">33.07%</td></tr><tr><td align="left" colspan="1" rowspan="1">Removal of Vision</td><td align="left" colspan="1" rowspan="1">85.51%</td><td align="left" colspan="1" rowspan="1">85.48%</td><td align="left" colspan="1" rowspan="1">85.50%</td><td align="left" colspan="1" rowspan="1">88.43%</td><td align="left" colspan="1" rowspan="1">82.81%</td></tr></tbody></table></table-wrap></p><p id="Par59">Figure&#160;<xref rid="Fig5" ref-type="fig">5</xref> visualizes the dimensionality reduction of the Virchow2 features via principal component analysis (PCA). The plot reflects the complexity of the problem and provides insights into the separability of different lesion classes in the feature space. The PCA visualization shows distinct clusters for different lesion types, indicating that the Virchow2 embeddings effectively capture discriminative features. However, some overlap between classes is observed, highlighting the challenges of distinguishing between visually similar lesions. This further emphasizes the importance of combining histopathologically inherited and vision-based features, as the dual-stream approach utilizes complementary information to improve class separability.<fig id="Fig5" position="float" orientation="portrait"><label>Fig. 5</label><caption><p>Dimensionality reduction of Virchow2 embeddings via principal component analysis (PCA). The plot illustrates the separability of different lesion classes in the feature space, highlighting the complexity of the problem and the importance of combining histopathologically inherited and vision-based features.</p></caption><graphic id="MO6" position="float" orientation="portrait" xlink:href="41598_2025_1319_Fig5_HTML.jpg"/></fig></p></sec><sec id="Sec15"><title>Comparative analysis</title><p id="Par60">The proposed dual-track approach outperforms traditional single-track methods, which rely solely on either extracted or vision-based features. Integrating both feature types enables the model to capture a more comprehensive representation of skin lesions, improving diagnostic accuracy. For example, while extracted features effectively identify morphological patterns, they may struggle with lesions that exhibit similar structures but differ in texture or color. Conversely, vision-based features excel at capturing visual patterns but may lack the domain-specific knowledge required for precise diagnosis. By combining these two modalities, the proposed approach addresses the limitations of single-track methods and achieves superior performance.</p><p id="Par61">To contextualize the performance of the proposed dual-stream approach, a comparative analysis with existing studies using the HAM10000 dataset is presented in Table&#160;<xref rid="Tab4" ref-type="table">4</xref>. It highlights the methodologies, results, and key metrics of recent studies alongside the performance of the proposed framework.<table-wrap id="Tab4" position="float" orientation="portrait"><label>Table 4</label><caption><p>Comparison of the proposed dual-stream approach with existing methods using the HAM10000 dataset.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Study (Ref)</th><th align="left" colspan="1" rowspan="1">Year</th><th align="left" colspan="1" rowspan="1">Approach</th><th align="left" colspan="1" rowspan="1">Results (Accuracy)</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Chaturvedi et al.<sup><xref ref-type="bibr" rid="CR63">63</xref></sup></td><td align="left" colspan="1" rowspan="1">2021</td><td align="left" colspan="1" rowspan="1">MobileNet</td><td align="left" colspan="1" rowspan="1">An overall accuracy of 83.1% for seven classes</td></tr><tr><td align="left" colspan="1" rowspan="1">Xin et al.<sup><xref ref-type="bibr" rid="CR44">44</xref></sup></td><td align="left" colspan="1" rowspan="1">2022</td><td align="left" colspan="1" rowspan="1">Improved transformer network</td><td align="left" colspan="1" rowspan="1">94.3% accuracy</td></tr><tr><td align="left" colspan="1" rowspan="1">Agyenta et al.<sup><xref ref-type="bibr" rid="CR64">64</xref></sup></td><td align="left" colspan="1" rowspan="1">2022</td><td align="left" colspan="1" rowspan="1">DenseNet201</td><td align="left" colspan="1" rowspan="1">Accuracy of 99.12% for training and 86.91% for testing</td></tr><tr><td align="left" colspan="1" rowspan="1">Ali et al.<sup><xref ref-type="bibr" rid="CR41">41</xref></sup></td><td align="left" colspan="1" rowspan="1">2022</td><td align="left" colspan="1" rowspan="1">EfficientNets</td><td align="left" colspan="1" rowspan="1">An F1 score of 87% and a top-1 accuracy of 87.91%</td></tr><tr><td align="left" colspan="1" rowspan="1">Adebiyi et al.<sup><xref ref-type="bibr" rid="CR43">43</xref></sup></td><td align="left" colspan="1" rowspan="1">2024</td><td align="left" colspan="1" rowspan="1">Multimodal learning</td><td align="left" colspan="1" rowspan="1">Best accuracy (94.11%) and AUCROC (0.9426)</td></tr><tr><td align="left" colspan="1" rowspan="1">Proposed Dual-Stream</td><td align="left" colspan="1" rowspan="1">2025</td><td align="left" colspan="1" rowspan="1">U-Net + Virchow2 + Nomic</td><td align="left" colspan="1" rowspan="1">96.25% (See Table&#160;<xref rid="Tab2" ref-type="table">2</xref>)</td></tr></tbody></table></table-wrap></p><p id="Par62">The proposed dual-stream approach achieves an accuracy of 96.25%, outperforming several recent studies. For example, Chaturvedi et al.<sup><xref ref-type="bibr" rid="CR63">63</xref></sup> employed MobileNet for skin lesion classification, achieving an accuracy of 83.1%. While their approach is computationally efficient, it lacks the ability to integrate domain-specific features, which limits its ability to distinguish between visually similar lesions. Similarly, Xin et al.<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> proposed an improved transformer network, achieving an accuracy of 94.3%. Although their method utilizes advanced transformer architectures, it does not explicitly incorporate embeddings, which are critical for capturing morphological patterns.</p><p id="Par63">Agyenta et al.<sup><xref ref-type="bibr" rid="CR64">64</xref></sup> utilized DenseNet201, reporting a training accuracy of 99.12% but a testing accuracy of only 86.91%. This significant drop in performance suggests potential overfitting, highlighting the challenges of generalizing single-modality approaches to unseen data. In contrast, the proposed dual-stream approach demonstrates robust generalization, as evidenced by its consistent performance across multiple trials (see Table&#160;<xref rid="Tab2" ref-type="table">2</xref>).</p><p id="Par64">Ali et al.<sup><xref ref-type="bibr" rid="CR41">41</xref></sup> explored EfficientNets for multiclass skin lesion classification, achieving an F1 score of 87% and a top-1 accuracy of 87.91%. While their approach is effective, it relies solely on vision-based features, which may not fully capture the complexity of skin lesions. The proposed dual-stream framework addresses this limitation by integrating histopathologically inherited features, resulting in higher accuracy and F1 scores.</p><p id="Par65">Adebiyi et al.<sup><xref ref-type="bibr" rid="CR43">43</xref></sup> introduced a multimodal learning approach, achieving an accuracy of 94.11% and an AUCROC of 0.9426. Their method combines multiple data modalities, demonstrating the benefits of integrating diverse feature types. However, their approach does not explicitly utilize pretrained models such as Virchow2, which are specifically designed for dermatoscopic data. The proposed dual-stream approach builds on this idea by incorporating Virchow2 embeddings, further enhancing diagnostic accuracy.</p></sec><sec id="Sec16"><title>Clinical implications</title><p id="Par66">The high accuracy and robustness of the proposed framework have significant implications for clinical practice. Skin cancer diagnosis often requires expert dermatologists to analyze complex visual and morphological patterns, which can be time-consuming and prone to human error. The proposed dual-stream approach provides a reliable and efficient tool for assisting clinicians in diagnosing skin lesions, reducing the burden on healthcare systems and improving patient outcomes. The interpretability of the model, as demonstrated by PCA visualization and ablation studies, further enhances its clinical utility, as it allows clinicians to understand the basis for the model&#8217;s predictions.</p><p id="Par67">Moreover, the model&#8217;s ability to generalize across different trials and datasets suggests that it can be adapted to various clinical settings and populations. This is particularly important for skin cancer diagnosis, as lesion characteristics can vary significantly depending on factors such as skin type, age, and geographic location. The robustness of the proposed framework to such variations makes it a promising candidate for widespread deployment in dermatology.</p></sec><sec id="Sec17"><title>Limitations</title><p id="Par68">Despite its strong performance, the proposed approach has several limitations. First, the model relies on high-quality dermatoscopic images, which may not always be available in resource-limited settings. Future work could explore alternative imaging modalities or techniques for enhancing low-quality images. Second, while ablation studies demonstrate the importance of both feature types, further research is needed to optimize the fusion mechanism and explore alternative architectures for integrating histopathologically inherited and vision-based features.</p><p id="Par69">Additionally, the model&#8217;s performance could be further improved by incorporating additional data sources, such as patient metadata (e.g., age, sex, medical history) or multispectral imaging data. These extra inputs could provide valuable context for improving diagnostic accuracy and personalizing treatment plans. Finally, future work should validate the model on more extensive and diverse datasets to ensure its generalizability across different populations and clinical scenarios.</p></sec></sec><sec id="Sec18"><title>Conclusions and future directions</title><p id="Par70">This study presents a dual-stream DL framework for skin lesion diagnosis that combines histopathological-inherited and vision-based feature extraction to achieve state-of-the-art performance. The proposed approach uses advanced DL techniques to address key limitations of traditional diagnostic methods, such as subjectivity, interobserver variability, and resource constraints. The U-Net architecture ensures precise lesion segmentation, whereas the dual-stream feature extraction mechanism, which uses Virchow2 and Nomic, captures both the morphological and contextual information critical for accurate diagnosis. The fusion of these features enables the model to achieve a mean accuracy of 96.25% and a mean F1 score of 93.79% on the HAM10000 dataset, demonstrating its robustness and generalizability across multiple trials. Ablation studies highlight the complementary roles of histopathological and vision-based streams, with the removal of either stream leading to significant performance degradation. Comparative analysis with existing studies further validates the superiority of the proposed framework, which outperforms traditional single-modality approaches and recent multimodal methods. The interpretability of the model, as demonstrated by dimensionality reduction techniques such as PCA, provides valuable insights into the feature space, enhancing its clinical utility. The proposed framework has significant implications for clinical practice, offering a reliable and efficient tool for assisting dermatologists in diagnosing skin lesions. Its ability to generalize across diverse datasets and populations makes it a promising candidate for widespread deployment, particularly in resource-limited settings.</p><sec id="Sec19"><title>Future directions</title><p id="Par71">Future work will focus on key enhancements to strengthen the framework. First, the fusion mechanism will be optimized, potentially incorporating attention mechanisms or multi-scale feature fusion to better capture local and global spatial information. Second, additional data sources, such as patient metadata (e.g., age, sex, medical history) and advanced imaging modalities like thermal or multispectral data, multispectral or 3D imaging, will be integrated to enrich contextual information and enable personalized predictions. Third, the model will be validated on larger, diverse datasets, expanding the evaluation to include datasets that explicitly represent underrepresented populations, varied skin phototypes (e.g., Fitzpatrick skin types I-VI), and regional differences in lesion presentation. These improvements aim to revolutionize skin cancer diagnosis, enhance accuracy, improve outcomes, and reduce mortality, paving the way for clinical adoption.</p></sec></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type="data-availability"><title>Data Availability</title><p>This study used the HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions, which is available at: <ext-link ext-link-type="uri" xlink:href="https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T">https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/DBW86T</ext-link></p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par74">The author declares that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.</p></notes><notes id="FPar2"><title>Ethics approval and consent to participate</title><p id="Par75">Not applicable</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jindal</surname><given-names>M</given-names></name><etal/></person-group><article-title>Skin cancer management: Current scenario and future perspectives</article-title><source>Curr. Drug Saf.</source><year>2023</year><volume>18</volume><fpage>143</fpage><lpage>158</lpage><pub-id pub-id-type="pmid">35422227</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.2174/1574886317666220413113959</pub-id></element-citation><mixed-citation id="mc-CR1" publication-type="journal">Jindal, M. et al. Skin cancer management: Current scenario and future perspectives. <italic toggle="yes">Curr. Drug Saf.</italic><bold>18</bold>, 143&#8211;158 (2023).<pub-id pub-id-type="pmid">35422227</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.2174/1574886317666220413113959</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><mixed-citation publication-type="other">Papadopoulos, O., Karantonis, F.-F. &amp; Papadopulos, N.&#160;A. Non-melanoma skin cancer and cutaneous melanoma for the plastic and reconstructive surgeon. <italic toggle="yes">Non-Melanoma Skin Cancer and Cutaneous Melanoma: Surgical Treatment and Reconstruction</italic> 153&#8211;239 (2020).</mixed-citation></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Balaha</surname><given-names>HM</given-names></name><name name-style="western"><surname>Hassan</surname><given-names>AE-S</given-names></name></person-group><article-title>Skin cancer diagnosis based on deep transfer learning and sparrow search algorithm</article-title><source>Neural Comput. Appl.</source><year>2023</year><volume>35</volume><fpage>815</fpage><lpage>853</lpage></element-citation><mixed-citation id="mc-CR3" publication-type="journal">Balaha, H. M. &amp; Hassan, A.E.-S. Skin cancer diagnosis based on deep transfer learning and sparrow search algorithm. <italic toggle="yes">Neural Comput. Appl.</italic><bold>35</bold>, 815&#8211;853 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><mixed-citation publication-type="other">Church, K. <italic toggle="yes">Early Detection of Melanoma</italic>. Master&#8217;s thesis, school The College of St. Scholastica (2022).</mixed-citation></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shamshad</surname><given-names>MF</given-names></name><etal/></person-group><article-title>A comprehensive review on treatment modalities of malignant melanoma</article-title><source>J. Liaquat Natl. Hosp.</source><year>2023</year><volume>1</volume><fpage>90</fpage><lpage>102</lpage></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Shamshad, M. F. et al. A comprehensive review on treatment modalities of malignant melanoma. <italic toggle="yes">J. Liaquat Natl. Hosp.</italic><bold>1</bold>, 90&#8211;102 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Narayanan</surname><given-names>DL</given-names></name><name name-style="western"><surname>Saladi</surname><given-names>RN</given-names></name><name name-style="western"><surname>Fox</surname><given-names>JL</given-names></name></person-group><article-title>Ultraviolet radiation and skin cancer</article-title><source>Int. J. Dermatol.</source><year>2010</year><volume>49</volume><fpage>978</fpage><lpage>986</lpage><pub-id pub-id-type="pmid">20883261</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1111/j.1365-4632.2010.04474.x</pub-id></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Narayanan, D. L., Saladi, R. N. &amp; Fox, J. L. Ultraviolet radiation and skin cancer. <italic toggle="yes">Int. J. Dermatol.</italic><bold>49</bold>, 978&#8211;986 (2010).<pub-id pub-id-type="pmid">20883261</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1111/j.1365-4632.2010.04474.x</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gosman</surname><given-names>LM</given-names></name><name name-style="western"><surname>T&#259;poi</surname><given-names>D-A</given-names></name><name name-style="western"><surname>Costache</surname><given-names>M</given-names></name></person-group><article-title>Cutaneous melanoma: A review of multifactorial pathogenesis, immunohistochemistry, and emerging biomarkers for early detection and management</article-title><source>Int. J. Mol. Sci.</source><year>2023</year><volume>24</volume><fpage>15881</fpage><pub-id pub-id-type="pmid">37958863</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/ijms242115881</pub-id><pub-id pub-id-type="pmcid">PMC10650804</pub-id></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Gosman, L. M., T&#259;poi, D.-A. &amp; Costache, M. Cutaneous melanoma: A review of multifactorial pathogenesis, immunohistochemistry, and emerging biomarkers for early detection and management. <italic toggle="yes">Int. J. Mol. Sci.</italic><bold>24</bold>, 15881 (2023).<pub-id pub-id-type="pmid">37958863</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/ijms242115881</pub-id><pub-id pub-id-type="pmcid">PMC10650804</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Geller</surname><given-names>AC</given-names></name><name name-style="western"><surname>Swetter</surname><given-names>SM</given-names></name><name name-style="western"><surname>Brooks</surname><given-names>K</given-names></name><name name-style="western"><surname>Demierre</surname><given-names>M-F</given-names></name><name name-style="western"><surname>Yaroch</surname><given-names>AL</given-names></name></person-group><article-title>Screening, early detection, and trends for melanoma: Current status (2000&#8211;2006) and future directions</article-title><source>J. Am. Acad. Dermatol.</source><year>2007</year><volume>57</volume><fpage>555</fpage><lpage>572</lpage><pub-id pub-id-type="pmid">17870429</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.jaad.2007.06.032</pub-id></element-citation><mixed-citation id="mc-CR8" publication-type="journal">Geller, A. C., Swetter, S. M., Brooks, K., Demierre, M.-F. &amp; Yaroch, A. L. Screening, early detection, and trends for melanoma: Current status (2000&#8211;2006) and future directions. <italic toggle="yes">J. Am. Acad. Dermatol.</italic><bold>57</bold>, 555&#8211;572 (2007).<pub-id pub-id-type="pmid">17870429</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.jaad.2007.06.032</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Karakousis</surname><given-names>GC</given-names></name><name name-style="western"><surname>Czerniecki</surname><given-names>BJ</given-names></name></person-group><article-title>Diagnosis of melanoma</article-title><source>PET Clin.</source><year>2011</year><volume>6</volume><fpage>1</fpage><lpage>8</lpage><pub-id pub-id-type="pmid">27156351</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.cpet.2011.02.001</pub-id></element-citation><mixed-citation id="mc-CR9" publication-type="journal">Karakousis, G. C. &amp; Czerniecki, B. J. Diagnosis of melanoma. <italic toggle="yes">PET Clin.</italic><bold>6</bold>, 1&#8211;8 (2011).<pub-id pub-id-type="pmid">27156351</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.cpet.2011.02.001</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Heibel</surname><given-names>HD</given-names></name><name name-style="western"><surname>Hooey</surname><given-names>L</given-names></name><name name-style="western"><surname>Cockerell</surname><given-names>CJ</given-names></name></person-group><article-title>A review of noninvasive techniques for skin cancer detection in dermatology</article-title><source>Am. J. Clin. Dermat.</source><year>2020</year><volume>21</volume><fpage>513</fpage><lpage>524</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s40257-020-00517-z</pub-id><pub-id pub-id-type="pmid">32383142</pub-id></element-citation><mixed-citation id="mc-CR10" publication-type="journal">Heibel, H. D., Hooey, L. &amp; Cockerell, C. J. A review of noninvasive techniques for skin cancer detection in dermatology. <italic toggle="yes">Am. J. Clin. Dermat.</italic><bold>21</bold>, 513&#8211;524 (2020).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s40257-020-00517-z</pub-id><pub-id pub-id-type="pmid">32383142</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Meng</surname><given-names>X</given-names></name><etal/></person-group><article-title>Non-invasive optical methods for melanoma diagnosis</article-title><source>Photodiagn. Photodyn. Ther.</source><year>2021</year><volume>34</volume><fpage>102266</fpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.pdpdt.2021.102266</pub-id><pub-id pub-id-type="pmid">33785441</pub-id></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Meng, X. et al. Non-invasive optical methods for melanoma diagnosis. <italic toggle="yes">Photodiagn. Photodyn. Ther.</italic><bold>34</bold>, 102266 (2021).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.pdpdt.2021.102266</pub-id><pub-id pub-id-type="pmid">33785441</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Leachman, S. A. et al. <italic toggle="yes">Methods of Melanoma Detection. Melanoma</italic> 51&#8211;105 (2016).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/978-3-319-22539-5_3</pub-id><pub-id pub-id-type="pmid">26601859</pub-id></mixed-citation></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Werner</surname><given-names>B</given-names></name></person-group><article-title>Skin biopsy and its histopathologic analysis: Why? what for? how? part i</article-title><source>Anais Bras. Dermatol.</source><year>2009</year><volume>84</volume><fpage>391</fpage><lpage>395</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1590/s0365-05962009000400010</pub-id><pub-id pub-id-type="pmid">19851671</pub-id></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Werner, B. Skin biopsy and its histopathologic analysis: Why? what for? how? part i. <italic toggle="yes">Anais Bras. Dermatol.</italic><bold>84</bold>, 391&#8211;395 (2009).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1590/s0365-05962009000400010</pub-id><pub-id pub-id-type="pmid">19851671</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mogensen</surname><given-names>M</given-names></name><name name-style="western"><surname>Jemec</surname><given-names>GB</given-names></name></person-group><article-title>Diagnosis of nonmelanoma skin cancer/keratinocyte carcinoma: A review of diagnostic accuracy of nonmelanoma skin cancer diagnostic tests and technologies</article-title><source>Dermatol. Surg.</source><year>2007</year><volume>33</volume><fpage>1158</fpage><lpage>1174</lpage><pub-id pub-id-type="pmid">17903149</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1111/j.1524-4725.2007.33251.x</pub-id></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Mogensen, M. &amp; Jemec, G. B. Diagnosis of nonmelanoma skin cancer/keratinocyte carcinoma: A review of diagnostic accuracy of nonmelanoma skin cancer diagnostic tests and technologies. <italic toggle="yes">Dermatol. Surg.</italic><bold>33</bold>, 1158&#8211;1174 (2007).<pub-id pub-id-type="pmid">17903149</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1111/j.1524-4725.2007.33251.x</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Dobre</surname><given-names>E-G</given-names></name><etal/></person-group><article-title>Skin cancer pathobiology at a glance: A focus on imaging techniques and their potential for improved diagnosis and surveillance in clinical cohorts</article-title><source>Int. J. Mol. Sci.</source><year>2023</year><volume>24</volume><fpage>1079</fpage><pub-id pub-id-type="pmid">36674595</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/ijms24021079</pub-id><pub-id pub-id-type="pmcid">PMC9866322</pub-id></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Dobre, E.-G. et al. Skin cancer pathobiology at a glance: A focus on imaging techniques and their potential for improved diagnosis and surveillance in clinical cohorts. <italic toggle="yes">Int. J. Mol. Sci.</italic><bold>24</bold>, 1079 (2023).<pub-id pub-id-type="pmid">36674595</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/ijms24021079</pub-id><pub-id pub-id-type="pmcid">PMC9866322</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Bridge, P., Fielding, A., Rowntree, P. &amp; Pullar, A. <italic toggle="yes">Intraobserver Variability: Should We Worry?</italic> (2016).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.jmir.2016.06.004</pub-id><pub-id pub-id-type="pmid">31047285</pub-id></mixed-citation></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Van Den Einden</surname><given-names>LC</given-names></name><etal/></person-group><article-title>Interobserver variability and the effect of education in the histopathological diagnosis of differentiated vulvar intraepithelial neoplasia</article-title><source>Mod. Pathol.</source><year>2013</year><volume>26</volume><fpage>874</fpage><lpage>880</lpage><pub-id pub-id-type="pmid">23370772</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/modpathol.2012.235</pub-id></element-citation><mixed-citation id="mc-CR17" publication-type="journal">Van Den Einden, L. C. et al. Interobserver variability and the effect of education in the histopathological diagnosis of differentiated vulvar intraepithelial neoplasia. <italic toggle="yes">Mod. Pathol.</italic><bold>26</bold>, 874&#8211;880 (2013).<pub-id pub-id-type="pmid">23370772</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/modpathol.2012.235</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bajaj</surname><given-names>S</given-names></name><etal/></person-group><article-title>The role of color and morphologic characteristics in dermoscopic diagnosis</article-title><source>JAMA Dermatol.</source><year>2016</year><volume>152</volume><fpage>676</fpage><lpage>682</lpage><pub-id pub-id-type="pmid">27007917</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1001/jamadermatol.2016.0270</pub-id><pub-id pub-id-type="pmcid">PMC5473029</pub-id></element-citation><mixed-citation id="mc-CR18" publication-type="journal">Bajaj, S. et al. The role of color and morphologic characteristics in dermoscopic diagnosis. <italic toggle="yes">JAMA Dermatol.</italic><bold>152</bold>, 676&#8211;682 (2016).<pub-id pub-id-type="pmid">27007917</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1001/jamadermatol.2016.0270</pub-id><pub-id pub-id-type="pmcid">PMC5473029</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Malik, S. &amp; Dixit, V.&#160;V. Skin cancer detection: State of art methods and challenges. In <italic toggle="yes">ICCCE 2021: Proceedings of the 4th International Conference on Communications and Cyber Physical Engineering</italic> 729&#8211;736 (Springer, 2022).</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Anand, V., Gupta, S. &amp; Koundal, D. Skin disease diagnosis: challenges and opportunities. In <italic toggle="yes">Proceedings of Second Doctoral Symposium on Computational Intelligence: DoSCI 2021</italic> 449&#8211;459 (Springer, 2022).</mixed-citation></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zarella</surname><given-names>MD</given-names></name><etal/></person-group><article-title>A practical guide to whole slide imaging: A white paper from the digital pathology association</article-title><source>Arch. Pathol. Lab. Med.</source><year>2019</year><volume>143</volume><fpage>222</fpage><lpage>234</lpage><pub-id pub-id-type="pmid">30307746</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.5858/arpa.2018-0343-RA</pub-id></element-citation><mixed-citation id="mc-CR21" publication-type="journal">Zarella, M. D. et al. A practical guide to whole slide imaging: A white paper from the digital pathology association. <italic toggle="yes">Arch. Pathol. Lab. Med.</italic><bold>143</bold>, 222&#8211;234 (2019).<pub-id pub-id-type="pmid">30307746</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.5858/arpa.2018-0343-RA</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rizzo</surname><given-names>PC</given-names></name><etal/></person-group><article-title>Digital pathology world tour</article-title><source>Digit. Health</source><year>2023</year><volume>9</volume><fpage>20552076231194551</fpage><pub-id pub-id-type="pmid">37654717</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1177/20552076231194551</pub-id><pub-id pub-id-type="pmcid">PMC10467307</pub-id></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Rizzo, P. C. et al. Digital pathology world tour. <italic toggle="yes">Digit. Health</italic><bold>9</bold>, 20552076231194550 (2023).<pub-id pub-id-type="pmid">37654717</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1177/20552076231194551</pub-id><pub-id pub-id-type="pmcid">PMC10467307</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rizzo</surname><given-names>PC</given-names></name><etal/></person-group><article-title>Technical and diagnostic issues in whole slide imaging published validation studies</article-title><source>Front. Oncol.</source><year>2022</year><volume>12</volume><fpage>918580</fpage><pub-id pub-id-type="pmid">35785212</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3389/fonc.2022.918580</pub-id><pub-id pub-id-type="pmcid">PMC9246412</pub-id></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Rizzo, P. C. et al. Technical and diagnostic issues in whole slide imaging published validation studies. <italic toggle="yes">Front. Oncol.</italic><bold>12</bold>, 918580 (2022).<pub-id pub-id-type="pmid">35785212</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3389/fonc.2022.918580</pub-id><pub-id pub-id-type="pmcid">PMC9246412</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Marletta</surname><given-names>S</given-names></name><etal/></person-group><article-title>Artificial intelligence-based algorithms for the diagnosis of prostate cancer: A systematic review</article-title><source>Am. J. Clin. Pathol.</source><year>2024</year><volume>161</volume><fpage>526</fpage><lpage>534</lpage><pub-id pub-id-type="pmid">38381582</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/ajcp/aqad182</pub-id></element-citation><mixed-citation id="mc-CR24" publication-type="journal">Marletta, S. et al. Artificial intelligence-based algorithms for the diagnosis of prostate cancer: A systematic review. <italic toggle="yes">Am. J. Clin. Pathol.</italic><bold>161</bold>, 526&#8211;534 (2024).<pub-id pub-id-type="pmid">38381582</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1093/ajcp/aqad182</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Koteluk</surname><given-names>O</given-names></name><name name-style="western"><surname>Wartecki</surname><given-names>A</given-names></name><name name-style="western"><surname>Mazurek</surname><given-names>S</given-names></name><name name-style="western"><surname>Ko&#322;odziejczak</surname><given-names>I</given-names></name><name name-style="western"><surname>Mackiewicz</surname><given-names>A</given-names></name></person-group><article-title>How do machines learn? Artificial intelligence as a new era in medicine</article-title><source>J. Pers. Med.</source><year>2021</year><volume>11</volume><fpage>32</fpage><pub-id pub-id-type="pmid">33430240</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/jpm11010032</pub-id><pub-id pub-id-type="pmcid">PMC7825660</pub-id></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Koteluk, O., Wartecki, A., Mazurek, S., Ko&#322;odziejczak, I. &amp; Mackiewicz, A. How do machines learn? Artificial intelligence as a new era in medicine. <italic toggle="yes">J. Pers. Med.</italic><bold>11</bold>, 32 (2021).<pub-id pub-id-type="pmid">33430240</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/jpm11010032</pub-id><pub-id pub-id-type="pmcid">PMC7825660</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Halabi, D. Enhancing skin cancer detection and classification: Exploring the impact of attention mechanisms in transfer learning models. <italic toggle="yes">Preprints</italic> (2023).</mixed-citation></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kushimo</surname><given-names>OO</given-names></name><name name-style="western"><surname>Salau</surname><given-names>AO</given-names></name><name name-style="western"><surname>Adeleke</surname><given-names>OJ</given-names></name><name name-style="western"><surname>Olaoye</surname><given-names>DS</given-names></name></person-group><article-title>Deep learning model to improve melanoma detection in people of color</article-title><source>Arab. J. Basic Appl. Sci.</source><year>2023</year><volume>30</volume><fpage>92</fpage><lpage>102</lpage></element-citation><mixed-citation id="mc-CR27" publication-type="journal">Kushimo, O. O., Salau, A. O., Adeleke, O. J. &amp; Olaoye, D. S. Deep learning model to improve melanoma detection in people of color. <italic toggle="yes">Arab. J. Basic Appl. Sci.</italic><bold>30</bold>, 92&#8211;102 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Ashfaq, M. &amp; Ahmad, A. Skin cancer classification with convolutional deep neural networks and vision transformers using transfer learning. In <italic toggle="yes">Advances in Deep Generative Models for Medical Artificial Intelligence</italic> 151&#8211;176 (publisherSpringer, 2023).</mixed-citation></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Jones</surname><given-names>O</given-names></name><etal/></person-group><article-title>Artificial intelligence and machine learning algorithms for early detection of skin cancer in community and primary care settings: A systematic review</article-title><source>Lancet Digit. Health</source><year>2022</year><volume>4</volume><fpage>e466</fpage><lpage>e476</lpage><pub-id pub-id-type="pmid">35623799</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/S2589-7500(22)00023-1</pub-id></element-citation><mixed-citation id="mc-CR29" publication-type="journal">Jones, O. et al. Artificial intelligence and machine learning algorithms for early detection of skin cancer in community and primary care settings: A systematic review. <italic toggle="yes">Lancet Digit. Health</italic><bold>4</bold>, e466&#8211;e476 (2022).<pub-id pub-id-type="pmid">35623799</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/S2589-7500(22)00023-1</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Takiddin</surname><given-names>A</given-names></name><name name-style="western"><surname>Schneider</surname><given-names>J</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Abd-Alrazaq</surname><given-names>A</given-names></name><name name-style="western"><surname>Househ</surname><given-names>M</given-names></name></person-group><article-title>Artificial intelligence for skin cancer detection: Scoping review</article-title><source>J. Med. Internet Res.</source><year>2021</year><volume>23</volume><fpage>e22934</fpage><pub-id pub-id-type="pmid">34821566</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.2196/22934</pub-id><pub-id pub-id-type="pmcid">PMC8663507</pub-id></element-citation><mixed-citation id="mc-CR30" publication-type="journal">Takiddin, A., Schneider, J., Yang, Y., Abd-Alrazaq, A. &amp; Househ, M. Artificial intelligence for skin cancer detection: Scoping review. <italic toggle="yes">J. Med. Internet Res.</italic><bold>23</bold>, e22934 (2021).<pub-id pub-id-type="pmid">34821566</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.2196/22934</pub-id><pub-id pub-id-type="pmcid">PMC8663507</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hauser</surname><given-names>K</given-names></name><etal/></person-group><article-title>Explainable artificial intelligence in skin cancer recognition: A systematic review</article-title><source>Eur. J. Cancer</source><year>2022</year><volume>167</volume><fpage>54</fpage><lpage>69</lpage><pub-id pub-id-type="pmid">35390650</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.ejca.2022.02.025</pub-id></element-citation><mixed-citation id="mc-CR31" publication-type="journal">Hauser, K. et al. Explainable artificial intelligence in skin cancer recognition: A systematic review. <italic toggle="yes">Eur. J. Cancer</italic><bold>167</bold>, 54&#8211;69 (2022).<pub-id pub-id-type="pmid">35390650</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.ejca.2022.02.025</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>H&#246;hn</surname><given-names>J</given-names></name><etal/></person-group><article-title>Combining cnn-based histologic whole slide image analysis and patient data to improve skin cancer classification</article-title><source>Eur. J. Cancer</source><year>2021</year><volume>149</volume><fpage>94</fpage><lpage>101</lpage><pub-id pub-id-type="pmid">33838393</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.ejca.2021.02.032</pub-id></element-citation><mixed-citation id="mc-CR32" publication-type="journal">H&#246;hn, J. et al. Combining cnn-based histologic whole slide image analysis and patient data to improve skin cancer classification. <italic toggle="yes">Eur. J. Cancer</italic><bold>149</bold>, 94&#8211;101 (2021).<pub-id pub-id-type="pmid">33838393</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.ejca.2021.02.032</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mampitiya</surname><given-names>LI</given-names></name><name name-style="western"><surname>Rathnayake</surname><given-names>N</given-names></name><name name-style="western"><surname>De Silva</surname><given-names>S</given-names></name></person-group><article-title>Efficient and low-cost skin cancer detection system implementation with a comparative study between traditional and cnn-based models</article-title><source>J. Comput. Cogn. Eng.</source><year>2023</year><volume>2</volume><fpage>226</fpage><lpage>235</lpage></element-citation><mixed-citation id="mc-CR33" publication-type="journal">Mampitiya, L. I., Rathnayake, N. &amp; De Silva, S. Efficient and low-cost skin cancer detection system implementation with a comparative study between traditional and cnn-based models. <italic toggle="yes">J. Comput. Cogn. Eng.</italic><bold>2</bold>, 226&#8211;235 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Shah, A. et al. A comprehensive study on skin cancer detection using artificial neural network (ann) and convolutional neural network (cnn). <italic toggle="yes">Clinical eHealth</italic> (2023).</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Junayed, M.&#160;S., Anjum, N., Noman, A. &amp; Islam, B. A deep cnn model for skin cancer detection and classification. <italic toggle="yes">International Conference in Central Europe on Computer Graphics, Visualization and Computer Vision</italic> (2021).</mixed-citation></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nawaz</surname><given-names>M</given-names></name><etal/></person-group><article-title>Melanoma segmentation: A framework of improved densenet77 and unet convolutional neural network</article-title><source>Int. J. Imaging Syst. Technol.</source><year>2022</year><volume>32</volume><fpage>2137</fpage><lpage>2153</lpage></element-citation><mixed-citation id="mc-CR36" publication-type="journal">Nawaz, M. et al. Melanoma segmentation: A framework of improved densenet77 and unet convolutional neural network. <italic toggle="yes">Int. J. Imaging Syst. Technol.</italic><bold>32</bold>, 2137&#8211;2153 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Miradwal, S., Mohammad, W., Jain, A. &amp; Khilji, F. Lesion segmentation in skin cancer detection using unet architecture. In <italic toggle="yes">Computational Intelligence and Data Analytics: Proceedings of ICCIDA 2022</italic> 329&#8211;340 (publisherSpringer, 2022).</mixed-citation></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>Z</given-names></name><etal/></person-group><article-title>Artificial intelligence in dermatology image analysis: Current developments and future trends</article-title><source>J. Clin. Med.</source><year>2022</year><volume>11</volume><fpage>6826</fpage><pub-id pub-id-type="pmid">36431301</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/jcm11226826</pub-id><pub-id pub-id-type="pmcid">PMC9693628</pub-id></element-citation><mixed-citation id="mc-CR38" publication-type="journal">Li, Z. et al. Artificial intelligence in dermatology image analysis: Current developments and future trends. <italic toggle="yes">J. Clin. Med.</italic><bold>11</bold>, 6826 (2022).<pub-id pub-id-type="pmid">36431301</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/jcm11226826</pub-id><pub-id pub-id-type="pmcid">PMC9693628</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liopyris</surname><given-names>K</given-names></name><name name-style="western"><surname>Gregoriou</surname><given-names>S</given-names></name><name name-style="western"><surname>Dias</surname><given-names>J</given-names></name><name name-style="western"><surname>Stratigos</surname><given-names>AJ</given-names></name></person-group><article-title>Artificial intelligence in dermatology: Challenges and perspectives</article-title><source>Dermatol. Ther.</source><year>2022</year><volume>12</volume><fpage>2637</fpage><lpage>2651</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s13555-022-00833-8</pub-id><pub-id pub-id-type="pmcid">PMC9674813</pub-id><pub-id pub-id-type="pmid">36306100</pub-id></element-citation><mixed-citation id="mc-CR39" publication-type="journal">Liopyris, K., Gregoriou, S., Dias, J. &amp; Stratigos, A. J. Artificial intelligence in dermatology: Challenges and perspectives. <italic toggle="yes">Dermatol. Ther.</italic><bold>12</bold>, 2637&#8211;2651 (2022).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s13555-022-00833-8</pub-id><pub-id pub-id-type="pmcid">PMC9674813</pub-id><pub-id pub-id-type="pmid">36306100</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sarker</surname><given-names>IH</given-names></name></person-group><article-title>Deep learning: A comprehensive overview on techniques, taxonomy, applications and research directions</article-title><source>SN Comput. Sci.</source><year>2021</year><volume>2</volume><fpage>420</fpage><pub-id pub-id-type="pmid">34426802</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s42979-021-00815-1</pub-id><pub-id pub-id-type="pmcid">PMC8372231</pub-id></element-citation><mixed-citation id="mc-CR40" publication-type="journal">Sarker, I. H. Deep learning: A comprehensive overview on techniques, taxonomy, applications and research directions. <italic toggle="yes">SN Comput. Sci.</italic><bold>2</bold>, 420 (2021).<pub-id pub-id-type="pmid">34426802</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s42979-021-00815-1</pub-id><pub-id pub-id-type="pmcid">PMC8372231</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ali</surname><given-names>K</given-names></name><name name-style="western"><surname>Shaikh</surname><given-names>ZA</given-names></name><name name-style="western"><surname>Khan</surname><given-names>AA</given-names></name><name name-style="western"><surname>Laghari</surname><given-names>AA</given-names></name></person-group><article-title>Multiclass skin cancer classification using efficientnets-a first step towards preventing skin cancer</article-title><source>Neurosci. Inform.</source><year>2022</year><volume>2</volume><fpage>100034</fpage></element-citation><mixed-citation id="mc-CR41" publication-type="journal">Ali, K., Shaikh, Z. A., Khan, A. A. &amp; Laghari, A. A. Multiclass skin cancer classification using efficientnets-a first step towards preventing skin cancer. <italic toggle="yes">Neurosci. Inform.</italic><bold>2</bold>, 100034 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shetty</surname><given-names>B</given-names></name><etal/></person-group><article-title>Skin lesion classification of dermoscopic images using machine learning and convolutional neural network</article-title><source>Sci. Rep.</source><year>2022</year><volume>12</volume><fpage>18134</fpage><pub-id pub-id-type="pmid">36307467</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-022-22644-9</pub-id><pub-id pub-id-type="pmcid">PMC9616944</pub-id></element-citation><mixed-citation id="mc-CR42" publication-type="journal">Shetty, B. et al. Skin lesion classification of dermoscopic images using machine learning and convolutional neural network. <italic toggle="yes">Sci. Rep.</italic><bold>12</bold>, 18134 (2022).<pub-id pub-id-type="pmid">36307467</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-022-22644-9</pub-id><pub-id pub-id-type="pmcid">PMC9616944</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><mixed-citation publication-type="other">Adebiyi, A. et al. Accurate skin lesion classification using multimodal learning on the ham10000 dataset. <italic toggle="yes">MedRxiv</italic> 2024&#8211;05 (2024).</mixed-citation></ref><ref id="CR44"><label>44.</label><citation-alternatives><element-citation id="ec-CR44" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xin</surname><given-names>C</given-names></name><etal/></person-group><article-title>An improved transformer network for skin cancer classification</article-title><source>Comput. Biol. Med.</source><year>2022</year><volume>149</volume><fpage>105939</fpage><pub-id pub-id-type="pmid">36037629</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.compbiomed.2022.105939</pub-id></element-citation><mixed-citation id="mc-CR44" publication-type="journal">Xin, C. et al. An improved transformer network for skin cancer classification. <italic toggle="yes">Comput. Biol. Med.</italic><bold>149</bold>, 105939 (2022).<pub-id pub-id-type="pmid">36037629</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.compbiomed.2022.105939</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR45"><label>45.</label><mixed-citation publication-type="other">Mao, J. et al. Medical supervised masked autoencoder: Crafting a better masking strategy and efficient fine-tuning schedule for medical image classification. <italic toggle="yes">Appl. Soft Comput.</italic> 112536 (2024).</mixed-citation></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yacin Sikkandar</surname><given-names>M</given-names></name><etal/></person-group><article-title>Deep learning based an automated skin lesion segmentation and intelligent classification model</article-title><source>J. Ambient Intell. Hum. Comput.</source><year>2021</year><volume>12</volume><fpage>3245</fpage><lpage>3255</lpage></element-citation><mixed-citation id="mc-CR46" publication-type="journal">Yacin Sikkandar, M. et al. Deep learning based an automated skin lesion segmentation and intelligent classification model. <italic toggle="yes">J. Ambient Intell. Hum. Comput.</italic><bold>12</bold>, 3245&#8211;3255 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><mixed-citation publication-type="other">Neha, F. et al. U-net in medical image segmentation: A review of its applications across modalities. <italic toggle="yes">arXiv preprint</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2412.02242">arXiv:2412.02242</ext-link> (2024).</mixed-citation></ref><ref id="CR48"><label>48.</label><citation-alternatives><element-citation id="ec-CR48" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Minaee</surname><given-names>S</given-names></name><etal/></person-group><article-title>Image segmentation using deep learning: A survey</article-title><source>IEEE Trans. Pattern Anal. Mach. Intell.</source><year>2021</year><volume>44</volume><fpage>3523</fpage><lpage>3542</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TPAMI.2021.3059968</pub-id><pub-id pub-id-type="pmid">33596172</pub-id></element-citation><mixed-citation id="mc-CR48" publication-type="journal">Minaee, S. et al. Image segmentation using deep learning: A survey. <italic toggle="yes">IEEE Trans. Pattern Anal. Mach. Intell.</italic><bold>44</bold>, 3523&#8211;3542 (2021).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TPAMI.2021.3059968</pub-id><pub-id pub-id-type="pmid">33596172</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR49"><label>49.</label><citation-alternatives><element-citation id="ec-CR49" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sahli</surname><given-names>H</given-names></name><name name-style="western"><surname>Ben Slama</surname><given-names>A</given-names></name><name name-style="western"><surname>Labidi</surname><given-names>S</given-names></name></person-group><article-title>U-net: A valuable encoder-decoder architecture for liver tumors segmentation in ct images</article-title><source>J. X-ray Sci. Technol.</source><year>2022</year><volume>30</volume><fpage>45</fpage><lpage>56</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.3233/XST-210993</pub-id><pub-id pub-id-type="pmid">34806644</pub-id></element-citation><mixed-citation id="mc-CR49" publication-type="journal">Sahli, H., Ben Slama, A. &amp; Labidi, S. U-net: A valuable encoder-decoder architecture for liver tumors segmentation in ct images. <italic toggle="yes">J. X-ray Sci. Technol.</italic><bold>30</bold>, 45&#8211;56 (2022).<pub-id pub-id-type="doi" assigning-authority="pmc">10.3233/XST-210993</pub-id><pub-id pub-id-type="pmid">34806644</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR50"><label>50.</label><mixed-citation publication-type="other">Azad, R. et al. Medical image segmentation review: The success of u-net. <italic toggle="yes">IEEE Trans. Pattern Anal. Mach. Intell.</italic> (2024).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1109/TPAMI.2024.3435571</pub-id><pub-id pub-id-type="pmid">39167505</pub-id></mixed-citation></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>Y</given-names></name><name name-style="western"><surname>Lin</surname><given-names>J</given-names></name><name name-style="western"><surname>Chen</surname><given-names>S</given-names></name><name name-style="western"><surname>He</surname><given-names>H</given-names></name><name name-style="western"><surname>Cai</surname><given-names>Y</given-names></name></person-group><article-title>Automatic tumor segmentation by means of deep convolutional u-net with pre-trained encoder in pet images</article-title><source>IEEE Access</source><year>2020</year><volume>8</volume><fpage>113636</fpage><lpage>113648</lpage></element-citation><mixed-citation id="mc-CR51" publication-type="journal">Lu, Y., Lin, J., Chen, S., He, H. &amp; Cai, Y. Automatic tumor segmentation by means of deep convolutional u-net with pre-trained encoder in pet images. <italic toggle="yes">IEEE Access</italic><bold>8</bold>, 113636&#8211;113648 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><citation-alternatives><element-citation id="ec-CR52" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bagheri</surname><given-names>F</given-names></name><name name-style="western"><surname>Tarokh</surname><given-names>MJ</given-names></name><name name-style="western"><surname>Ziaratban</surname><given-names>M</given-names></name></person-group><article-title>Skin lesion segmentation from dermoscopic images by using mask r-cnn, retina-deeplab, and graph-based methods</article-title><source>Biomed. Signal Process. Control</source><year>2021</year><volume>67</volume><fpage>102533</fpage></element-citation><mixed-citation id="mc-CR52" publication-type="journal">Bagheri, F., Tarokh, M. J. &amp; Ziaratban, M. Skin lesion segmentation from dermoscopic images by using mask r-cnn, retina-deeplab, and graph-based methods. <italic toggle="yes">Biomed. Signal Process. Control</italic><bold>67</bold>, 102533 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR53"><label>53.</label><citation-alternatives><element-citation id="ec-CR53" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lilhore</surname><given-names>UK</given-names></name><etal/></person-group><article-title>A precise model for skin cancer diagnosis using hybrid u-net and improved mobilenet-v3 with hyperparameters optimization</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><fpage>4299</fpage><pub-id pub-id-type="pmid">38383520</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-024-54212-8</pub-id><pub-id pub-id-type="pmcid">PMC10881962</pub-id></element-citation><mixed-citation id="mc-CR53" publication-type="journal">Lilhore, U. K. et al. A precise model for skin cancer diagnosis using hybrid u-net and improved mobilenet-v3 with hyperparameters optimization. <italic toggle="yes">Sci. Rep.</italic><bold>14</bold>, 4299 (2024).<pub-id pub-id-type="pmid">38383520</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-024-54212-8</pub-id><pub-id pub-id-type="pmcid">PMC10881962</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR54"><label>54.</label><mixed-citation publication-type="other">Saleem, S. &amp; Sharif, M.&#160;I. An integrated deep learning framework leveraging nasnet and vision transformer with mixprocessing for accurate and precise diagnosis of lung diseases. <italic toggle="yes">arXiv preprint</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2502.20570">arXiv:2502.20570</ext-link> (2025).</mixed-citation></ref><ref id="CR55"><label>55.</label><mixed-citation publication-type="other">Hafeez, R. et al. Deep learning in early alzheimers diseases detection: A comprehensive survey of classification, segmentation, and feature extraction methods. <italic toggle="yes">arXiv preprint</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2501.15293">arXiv:2501.15293</ext-link> (2025).</mixed-citation></ref><ref id="CR56"><label>56.</label><mixed-citation publication-type="other">Balaha, H.&#160;M., Ali, K.&#160;M., Mahmoud, A., Ghazal, M. &amp; El-Baz, A. Integrated grading framework for histopathological breast cancer: Multi-level vision transformers, textural features, and fusion probability network. In <italic toggle="yes">International Conference on Pattern Recognition</italic> 76&#8211;91 (Springer, 2024).</mixed-citation></ref><ref id="CR57"><label>57.</label><citation-alternatives><element-citation id="ec-CR57" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shah</surname><given-names>OI</given-names></name><name name-style="western"><surname>Rizvi</surname><given-names>DR</given-names></name><name name-style="western"><surname>Mir</surname><given-names>AN</given-names></name></person-group><article-title>Transformer-based innovations in medical image segmentation: A mini review</article-title><source>SN Comput. Sci.</source><year>2025</year><volume>6</volume><fpage>375</fpage></element-citation><mixed-citation id="mc-CR57" publication-type="journal">Shah, O. I., Rizvi, D. R. &amp; Mir, A. N. Transformer-based innovations in medical image segmentation: A mini review. <italic toggle="yes">SN Comput. Sci.</italic><bold>6</bold>, 375 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR58"><label>58.</label><mixed-citation publication-type="other">Zimmermann, E. et al. Virchow2: Scaling self-supervised mixed magnification models in pathology. <italic toggle="yes">arXiv preprint</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2408.00738">arXiv:2408.00738</ext-link> (2024).</mixed-citation></ref><ref id="CR59"><label>59.</label><mixed-citation publication-type="other">Vorontsov, E. et al. Virchow: A million-slide digital pathology foundation model. <italic toggle="yes">arXiv preprint</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2309.07778">arXiv:2309.07778</ext-link> (2023).</mixed-citation></ref><ref id="CR60"><label>60.</label><mixed-citation publication-type="other">Nussbaum, Z., Morris, J.&#160;X., Duderstadt, B. &amp; Mulyar, A. Nomic embed: Training a reproducible long context text embedder. <italic toggle="yes">arXiv preprint</italic><ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2402.01613">arXiv:2402.01613</ext-link> (2024).</mixed-citation></ref><ref id="CR61"><label>61.</label><citation-alternatives><element-citation id="ec-CR61" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Balaha</surname><given-names>HM</given-names></name><name name-style="western"><surname>Hassan</surname><given-names>AE-S</given-names></name><name name-style="western"><surname>El-Gendy</surname><given-names>EM</given-names></name><name name-style="western"><surname>ZainEldin</surname><given-names>H</given-names></name><name name-style="western"><surname>Saafan</surname><given-names>MM</given-names></name></person-group><article-title>An aseptic approach towards skin lesion localization and grading using deep learning and Harris Hawks optimization</article-title><source>Multimed. Tools Appl.</source><year>2024</year><volume>83</volume><fpage>19787</fpage><lpage>19815</lpage></element-citation><mixed-citation id="mc-CR61" publication-type="journal">Balaha, H. M., Hassan, A.E.-S., El-Gendy, E. M., ZainEldin, H. &amp; Saafan, M. M. An aseptic approach towards skin lesion localization and grading using deep learning and Harris Hawks optimization. <italic toggle="yes">Multimed. Tools Appl.</italic><bold>83</bold>, 19787&#8211;19815 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR62"><label>62.</label><citation-alternatives><element-citation id="ec-CR62" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abd El-Khalek</surname><given-names>AA</given-names></name><etal/></person-group><article-title>A concentrated machine learning-based classification system for age-related macular degeneration (amd) diagnosis using fundus images</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><fpage>2434</fpage><pub-id pub-id-type="pmid">38287062</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-024-52131-2</pub-id><pub-id pub-id-type="pmcid">PMC10825213</pub-id></element-citation><mixed-citation id="mc-CR62" publication-type="journal">Abd El-Khalek, A. A. et al. A concentrated machine learning-based classification system for age-related macular degeneration (amd) diagnosis using fundus images. <italic toggle="yes">Sci. Rep.</italic><bold>14</bold>, 2434 (2024).<pub-id pub-id-type="pmid">38287062</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-024-52131-2</pub-id><pub-id pub-id-type="pmcid">PMC10825213</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR63"><label>63.</label><mixed-citation publication-type="other">Chaturvedi, S.&#160;S., Gupta, K. &amp; Prasad, P.&#160;S. Skin lesion analyser: an efficient seven-way multi-class skin cancer classification using mobilenet. In <italic toggle="yes">Advanced machine learning technologies and applications: proceedings of AMLTA 2020</italic> 165&#8211;176 (Springer, 2021).</mixed-citation></ref><ref id="CR64"><label>64.</label><citation-alternatives><element-citation id="ec-CR64" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Agyenta</surname><given-names>C</given-names></name><name name-style="western"><surname>Akanzawon</surname><given-names>M</given-names></name></person-group><article-title>Skin lesion classification based on convolutional neural network</article-title><source>J. Appl. Sci. Technol. Trends</source><year>2022</year><volume>3</volume><fpage>21</fpage><lpage>26</lpage></element-citation><mixed-citation id="mc-CR64" publication-type="journal">Agyenta, C. &amp; Akanzawon, M. Skin lesion classification based on convolutional neural network. <italic toggle="yes">J. Appl. Sci. Technol. Trends</italic><bold>3</bold>, 21&#8211;26 (2022).</mixed-citation></citation-alternatives></ref></ref-list></back></article>
        
    </metadata>
</record>
    </GetRecord>

</OAI-PMH>