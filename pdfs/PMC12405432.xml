


<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
    <responseDate>2025-09-09T14:05:51Z</responseDate>
    <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:12405432" metadataPrefix="pmc">https://pmc.ncbi.nlm.nih.gov/api/oai/v1/mh/</request>
    
    <GetRecord>
        <record>
    <header>
    <identifier>oai:pubmedcentral.nih.gov:12405432</identifier>
    <datestamp>2025-09-04</datestamp>
    
        
        <setSpec>scirep</setSpec>
        
    
        
        <setSpec>pmc-open</setSpec>
        
    
</header>
    <metadata>
        
        <article xmlns="https://jats.nlm.nih.gov/ns/archiving/1.4/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.4/ https://jats.nlm.nih.gov/archiving/1.4/xsd/JATS-archivearticle1-4.xsd" article-type="research-article" xml:lang="en" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-id journal-id-type="pmc-domain-id">1579</journal-id><journal-id journal-id-type="pmc-domain">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12405432</article-id><article-id pub-id-type="pmcid-ver">PMC12405432.1</article-id><article-id pub-id-type="pmcaid">12405432</article-id><article-id pub-id-type="pmcaiid">12405432</article-id><article-id pub-id-type="pmid">40897806</article-id><article-id pub-id-type="doi">10.1038/s41598-025-18263-9</article-id><article-id pub-id-type="publisher-id">18263</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Object detection model of vehicle-road cooperative autonomous driving based on improved YOLO11 algorithm</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name name-style="western"><surname>Liang</surname><given-names initials="E">Enqiang</given-names></name><address><email>enqiangliang@163.com</email></address><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name name-style="western"><surname>Wei</surname><given-names initials="D">Dongpo</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name name-style="western"><surname>Li</surname><given-names initials="F">Feng</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name name-style="western"><surname>Lv</surname><given-names initials="H">Huimin</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><contrib contrib-type="author" equal-contrib="yes"><name name-style="western"><surname>Li</surname><given-names initials="S">Shengtao</given-names></name><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/02mr3ar13</institution-id><institution-id institution-id-type="GRID">grid.412509.b</institution-id><institution-id institution-id-type="ISNI">0000 0004 1808 3414</institution-id><institution>Department of Mechanical Engineering, </institution><institution>Shandong Huayu University of Technology, </institution></institution-wrap>Dezhou, 253034 Shandong China </aff></contrib-group><pub-date pub-type="epub"><day>2</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type="pmc-issue-id">478255</issue-id><elocation-id>32348</elocation-id><history><date date-type="received"><day>30</day><month>6</month><year>2025</year></date><date date-type="accepted"><day>1</day><month>9</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-04 00:25:59.930"><day>04</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri content-type="pmc-pdf" xlink:href="41598_2025_Article_18263.pdf"/><abstract id="Abs1"><p id="Par5">To address the issues of low detection accuracy, false detection, and missing detection, as well as the challenge of modeling lightweight scenes caused by the overlapping occlusion of roadside targets and distant targets in autonomous driving scenarios, an improved small target detection algorithm for autonomous driving based on YOLO11 is proposed. Firstly, it embedded the Channel Transposed Attention in the C3k2 module, proposed the C3CTA module, and replaced the C3k2 module in the Backbone network to improve the feature extraction ability and strengthen the detection ability in the case of target occlusion. Secondly, the Diffusion Focusing Pyramid Network is introduced to improve the Neck part, enhance the understanding ability of small targets in complex scenes, and effectively solve the problem that it is difficult to extract vehicle target features. Finally, a Lightweight Shared Convolutional Detection Head is introduced to reduce the number of model parameters and achieve lightweight requirements. The experimental results show that the Precision, Recall, mAP@0.5, and mAP@0.5&#8211;95 of the improved algorithm on the world&#8217;s first DAIR-V2X-I dataset reach 85.7%, 79.4%, 85.3%, and 61.3%, which is 4.0% higher than the baseline model. 3.1%, 2.4%, 2.8%. Higher detection accuracy is achieved, which proves the effectiveness of the improvement. Proposed a new solution approach for target detection in complex autonomous driving scenarios.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>YOLO11</kwd><kwd>Autonomous driving</kwd><kwd>Vehicle-road collaboration</kwd><kwd>Feature pyramid</kwd><kwd>Attention mechanism</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Engineering</kwd><kwd>Mathematics and computing</kwd></kwd-group><funding-group><award-group><funding-source><institution>Intelligent Manufacturing Engineering Laboratory - Shandong Province Higher Education Characteristic Laboratory</institution></funding-source><award-id>PT2025KJS002</award-id><award-id>PT2025KJS002</award-id><award-id>PT2025KJS002</award-id><award-id>PT2025KJS002</award-id><award-id>PT2025KJS002</award-id><principal-award-recipient><name name-style="western"><surname>Liang</surname><given-names>Enqiang</given-names></name><name name-style="western"><surname>Wei</surname><given-names>Dongpo</given-names></name><name name-style="western"><surname>Li</surname><given-names>Feng</given-names></name><name name-style="western"><surname>Lv</surname><given-names>Huimin</given-names></name><name name-style="western"><surname>Li</surname><given-names>Shengtao</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par6">Object detection is one of the core algorithms in an automatic driving system. Especially in complex scenes, object occlusion and recognition of distant small targets have always been an important challenge in research and application. The autonomous driving environment is usually full of various dynamic factors, including occlusion, light changes, different weather conditions, and diverse traffic participants, which makes object detection very difficult in practical applications<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Therefore, designing an efficient algorithm that can effectively detect objects in these complex situations is not only the key to the development of the technology, but also the basis for the safe and reliable operation of autonomous driving technology. At present, object detection algorithms can be mainly divided into two categories: traditional object detection methods and object detection methods based on deep learning<sup><xref ref-type="bibr" rid="CR2">2</xref></sup>. Traditional object detection methods rely on manually extracted features and classifiers, and these methods recognize and classify objects through manually designed features. Although these methods can achieve certain results in some simple scenes, they perform poorly in complex and dynamic environments<sup><xref ref-type="bibr" rid="CR3">3</xref></sup>. In contrast, object detection methods based on deep learning have been widely used in recent years and have gradually become the mainstream technology of autonomous driving systems<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Deep learning algorithms can automatically learn complex features through large-scale data training, so as to achieve more efficient and accurate target recognition. Convolutional Neural Network (CNN) performs well in feature extraction and image classification. Deep learning methods not only improve the accuracy of detection, but also better deal with diversity and dynamics in complex environments<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. For example, object detection algorithms based on deep learning, such as Faster R-CNN, YOLO, SSD, etc., can process a large amount of data in a short time, and have strong robustness and adaptability, especially in the face of occlusion, illumination change, and long-distance targets<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. And in this kind of deep learning algorithm, the YOLO series algorithm has gradually become the mainstream algorithm for automatic driving target detection with a high degree of real-time performance, a small amount of calculation, and high detection accuracy<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>.</p><p id="Par7">Av&#351;ar et al.<sup><xref ref-type="bibr" rid="CR8">8</xref></sup> used deep learning technology to detect and track moving vehicles in roundabouts, trying to improve the performance of traffic monitoring systems in complex traffic scenarios. However, although this method improves the detection accuracy to a certain extent, it still faces the contradiction between algorithm accuracy and computational complexity in practical applications. XU et al.<sup><xref ref-type="bibr" rid="CR9">9</xref></sup> introduced a standardized attention module to improve the object detection ability of YOLOv5 in complex traffic environments, especially for the detection of smaller objects. Although this method has made significant progress in improving the accuracy, it still has certain limitations when dealing with difficult small samples and overlapping samples. Ocher et al.<sup><xref ref-type="bibr" rid="CR10">10</xref></sup> proposed the YOLOv8 model. This new version significantly improves the effect of object detection by optimizing the backbone network. Although YOLOv8 performs well in multiple tasks, further optimizing its performance in dynamic and severely occluded traffic environments, and balancing accuracy and efficiency are still the research directions for further development of the algorithm.</p><p id="Par8">Compared with other models, the YOLO model shows better recognition performance and identifies the state of the vehicle target. However, the latest YOLO model and the above studies have encountered some technical challenges and difficulties in the detection task of autonomous driving<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>, which can be briefly summarized as follows:</p><p id="Par9">1) In the traffic scene of vehicle-road cooperative perception, there are long-distance targets and small targets in the side view of the road, and the recognition accuracy is low, and the features is few. The scene information obtained by the sensors in the side view of the road is limited, which also limits their ability to perceive the targets comprehensively and accurately in the complex traffic environment<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>.</p><p id="Par10">2) In the process of vehicle driving, vehicles on both sides of the road surface often overlap and interleave, or are blocked by some large obstacles, which greatly affects the detection accuracy of the algorithm<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>.</p><p id="Par11">3) In autonomous driving scenarios, the computing performance of edge devices is often poor, and the processing speed of some models with large amounts of calculation is slow, which makes it difficult to meet the real-time requirements of traffic scenes<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>.</p><p id="Par12">Although YOLOv5 and YOLOv8 performs well in multiple tasks, further optimizing its performance in dynamic and severely occluded traffic environments, and balancing accuracy and efficiency are still the research directions for further development of the algorithm. Specifically, the challenges of accurately detecting small and occluded targets (addressed by our DFPN and C3CTA modules) and the computational burden on edge devices (addressed by our LSCD head) remain largely unsolved by these existing approaches.</p><p id="Par13">Based on the above analysis, this paper uses YOLO11 to improve and propose a new object detection algorithm, YOLO-FLC. The main contributions include three aspects:</p><p id="Par14">1) A Diffusion Focusing Pyramid Network is proposed, which significantly improves the expression ability of multi-scale features and retains feature details at different scales by introducing a specially designed feature focusing module and feature diffusion mechanism. The FoFe module (Focus Features Module) is used to explicitly enhance the context information transmission of small targets, thereby avoiding the loss of details during the multi-layer transmission process. Using parallel depth separable convolution to capture the context information of different receptive fields, and by integrating through an adaptive gating mechanism, it outperforms the traditional upsampling/downsampling&#8201;+&#8201;concatenation approach. At the same time, frequency domain processing is introduced in feature extraction to enhance the ability to retain high-frequency details (such as edges and textures).The ability to understand small targets in complex scenes was enhanced, and the problem that it was difficult to extract vehicle target features was effectively solved.</p><p id="Par15">2) In order to effectively learn the details in the image, the Channel Transposed Attention is added to the Backbone network, and a new C3CTA module is constructed. The C3CTA module, based on the original C3k2 structure of YOLO11, incorporates a dual-branch attention mechanism: spatial-frequency attention (SFA) and channel transposed attention (CTA). Traditional attention mechanisms (such as SE and CBAM) usually perform channel or spatial modeling through global average pooling or max pooling. However, CTA establishes richer cross-channel dependencies in the channel dimension through transposition operations, avoiding information loss. The SFA branch was the first to introduce frequency-domain projection in object detection. By enhancing the high-frequency components and applying spatial window attention, it significantly improved the model&#8217;s ability to perceive details and occluded objects. C3CTA achieves more comprehensive feature enhancement through parallel processing of spatial-frequency and channel attention, while existing methods mostly adopt serial or single-branch structures.It effectively solves the problem of low detection accuracy and missed detection in the case of overlapping and interleaving targets.</p><p id="Par16">3) In order to solve the problem of slow Detection caused by a large amount of model calculation, we propose a new Lightweight Shared Convolutional Detection (LSCD) Head, which can effectively solve the problem of a large amount of model calculation. The model is lightweight.The LSCD detection head achieves a balance between accuracy and efficiency through weight sharing and scale adaptive layers. Multiple detection heads share the same set of convolutional weights, significantly reducing the number of parameters and computational load. In contrast, traditional detection heads (such as the YOLO series) usually design separate heads for each scale. Explicitly adjusting the scale of the regression branch to improve the positioning accuracy of multi-scale targets, and avoiding the scale inconsistency problem caused by simple fusion in structures such as FPN.</p></sec><sec id="Sec2"><title>Related work</title><p id="Par17">Deep learning object detection models are mainly divided into two categories: one-stage and two-stage. The one-stage detection model uses the regression method to directly input the input image into the convolutional neural network, and then outputs the classification and localization results of the target at the same time. The detection accuracy of this type of model is relatively slightly lower, but the advantage is that the detection speed is faster<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. With the continuous iteration and optimization of the single-stage detection algorithm, its detection accuracy has been significantly improved, which is enough to cope with the target detection tasks with high detection accuracy requirements. Typical representatives of single-stage detection models include YOLO (You Only Look Once) and Single Shot MultiBox Detector (SSD)<sup><xref ref-type="bibr" rid="CR16">16</xref></sup>.</p><p id="Par18">The operation process of the two-stage detection model is different. It first uses a convolutional neural network to obtain candidate regions, and then completes the detection and classification work based on these regions. Compared with the one-stage detection model, the two-stage detection model has higher detection accuracy, but the detection speed is relatively slow. Common two-stage detection models are Regions with CNN features (RCNN), region-based Fast convolutional neural network (Fast RCNN), region-based Faster convolutional neural network (Faster RCNN), Spatial Pyramid Pooling Networks (SPPNet), etc<sup><xref ref-type="bibr" rid="CR17">17</xref></sup>.</p><p id="Par19">Given the advantages of fast detection speed and stronger real-time performance of single-stage detection algorithms, YOLO series models have been widely used in video or real-time image object detection tasks. Therefore, the YOLO11 model is selected in the study carried out for the traffic vehicle detection task.</p><p id="Par20">Although the current object detection algorithms have high accuracy, they still have shortcomings in practical applications. For dense targets, occlusion, complex backgrounds (such as different brightness and angle), and low-resolution images, the stability of model detection is not good. In addition, when the detection program is applied under different camera angles, the image information of distant small targets in the top view is less, and the network model has difficulty extracting features, resulting in low accuracy or missed detection<sup><xref ref-type="bibr" rid="CR18">18</xref></sup>.</p><p id="Par21">In view of the above problems, many scholars have improved the algorithm. For example, Lu et al.<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> proposed an aerial image vehicle detection method based on YOLO deep learning algorithm, processed three public data sets, and constructed an aerial image data set suitable for YOLO training, so that the trained model had a good test effect on small objects, rotating objects and compact objects, but did not improve the algorithm itself. Limited application scope; Zhao et al.<sup><xref ref-type="bibr" rid="CR20">20</xref></sup> proposed the YOLO-Lite algorithm to reduce the computing power overhead of the YOLO network model by changing the input image size, reducing the number of convolution layers, and removing the batch normalization layer, so as to adapt to the detection application without GPU devices and improve the detection speed, but the algorithm effect is not ideal. Fang et al.<sup><xref ref-type="bibr" rid="CR21">21</xref></sup> proposed the Tinier-YOLO algorithm based on Tiny-YOLO-v3, introduced the Fire module and SqueezeNet model to reduce the amount of parameters, added the straight-through layer, and merged the feature map of the previous layer to obtain fine-grained features, so as to improve the detection speed and maintain the accuracy. However, due to the lightweight of the model, the performance is not up to standard. The existing feature pyramid structure effectively integrates multi-scale features, but it still falls short in enhancing and diffusing the features of tiny targets in complex occlusion scenarios. Our designed DFPN employs the feature focusing and feature diffusion mechanisms, aiming to more explicitly strengthen the propagation of context information for small targets.Pan et al.<sup><xref ref-type="bibr" rid="CR22">22</xref></sup> adopted the YOLOv5&#8201;+&#8201;CA attention model to add the CA attention mechanism to the last layer of the backbone network to reduce the interference of irrelevant information and let the algorithm focus on relevant feature information. However, from the results, the detection accuracy of the network model for small targets still needs to be improved.Although attention mechanisms are widely used to enhance feature extraction, most of these mechanisms have trade-offs in terms of computational complexity and the sufficiency of cross-channel interaction of features. Our proposed C3CTA module, through the channel shunting mechanism, aims to achieve more effective channel information integration with lower computational costs.</p><sec id="Sec3"><title>The proposed method</title><sec id="Sec4"><title>YOLO11</title><p id="Par22">YOLO11 inherits and extends the advantages of the YOLO family of models, especially making significant improvements in the architecture design of backbone and neck networks. These improvements greatly improve the feature extraction ability of the model, which improves the accuracy of object detection and performs better in handling complex tasks. Compared to previous versions, YOLO11 implements a more refined and efficient architecture by design, which allows models to not only be processed faster but also maintain the best balance between accuracy and performance. This architectural optimization, especially the tradeoff between computation and performance, makes YOLO11 an extremely useful model in a variety of application scenarios. The structure diagram of the YOLO11 model is shown in Fig.&#160;<xref rid="Fig1" ref-type="fig">1</xref>.</p><p id="Par23">Specifically, YOLO11 achieves higher Mean Average Precision (mAP) on the COCO dataset, which means that it is able to identify more types of objects and make more accurate localization in object detection tasks. Moreover, YOLO11 significantly optimizes the number of parameters. Compared with YOLOv8m, YOLO11 reduces the number of parameters by 22%. This reduction in the number of parameters not only improves computational efficiency but also makes YOLO11 more efficient to run on a variety of devices, especially in resource-constrained environments, without sacrificing model accuracy. YOLO11 is innovative in a number of ways, especially in the core modules of the architecture. Compared with YOLOv8, YOLO11 makes several important optimizations in the network structure. Firstly, YOLO11 replaces the original C2f module with the C3K2 module, which significantly enhances the model&#8217;s ability to extract feature information and improves the accuracy of object detection. Secondly, after the SPPF module (spatial pyramid pooling), YOLO11 adds the C2PSA module to further optimize the effect of multi-scale feature fusion and help the model better deal with objects of different sizes and scales. In addition, YOLO11 introduces the head design concept of YOLOv10 into the detection head of YOLO11, which greatly reduces redundant calculations and improves computational efficiency through the depthwise separable convolution method. Deptwise separable convolution reduces the computational complexity of the model by decomposing the standard convolution operation, allowing YOLO11 to achieve a significant improvement in execution speed without sacrificing detection accuracy. Although YOLO11 can improve the detection accuracy and reduce the number of parameters compared with the previous YOLO model, it also has great challenges in the vehicle detection scene in the complex environment of unmanned driving.</p><p id="Par24">
<fig id="Fig1" position="float" orientation="portrait"><label>Fig. 1</label><caption><p>YOLO11 model diagram.</p></caption><graphic id="d33e327" position="float" orientation="portrait" xlink:href="41598_2025_18263_Fig1_HTML.jpg"/></fig>
</p></sec></sec></sec><sec id="Sec5"><title>YOLO-FLC</title><p id="Par25">In autonomous driving scenarios, object occlusion and small object detection have always been significant challenges in the field of computer vision. In order to solve these problems, this paper makes innovative improvements on the basis of the YOLO11 model. This paper proposes a new network model, YOLO-FLC (YOLO with Feature Diffusion and Lightweight Shared Channel Attention). The core innovations of this model include the following aspects: First, an advanced Channel Transfer Attention mechanism<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> is introduced into the backbone network, and a new C3CTA (Channel Transposed Attention) network module is constructed. This module can adaptively select and transfer important channel information in multi-level feature maps, so as to improve the recognition ability of the model for different targets. Especially in the case of occlusion or overlap between targets, the C3CTA module can help the network to distinguish the target better and significantly improve the detection performance. Secondly, in the Neck part of the Network, the Diffusion Focusing Pyramid Network<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> is proposed. The network enhances the context information of features at different scales by introducing a feature focusing module and a feature diffusion mechanism. The feature focusing module can highlight important local information, while the feature diffusion mechanism enhances the propagation of global information by diffusion. In this way, the feature map of each scale can contain more detailed context information, especially when dealing with small targets, which can effectively improve its detection accuracy<sup><xref ref-type="bibr" rid="CR25">25</xref></sup>. Finally, we use a Lightweight Shared Convolutional Detection Head (LSCD) to replace the original detection head in the model, which can improve the recognition ability and accuracy of the model for different targets while reducing the amount of calculation. To optimize the performance of the detection head, we add a Scale layer to the network. By scaling the feature map, the objects of different scales can be more accurately captured by the detection head, so as to further improve the multi-scale detection ability of the model. With these improvements, the YOLO-FLC model can better cope with complex scenes and environments in autonomous driving tasks, especially when dealing with object occlusion or detecting small objects, and achieves significant performance improvement. The structure of the improved YOLO11 model is shown in Fig.&#160;<xref rid="Fig2" ref-type="fig">2</xref>, which shows how the new network modules are integrated into the overall architecture of the model.</p><p id="Par26">
<fig id="Fig2" position="float" orientation="portrait"><label>Fig. 2</label><caption><p>YOLO-FLC model diagram.</p></caption><graphic id="d33e356" position="float" orientation="portrait" xlink:href="41598_2025_18263_Fig2_HTML.jpg"/></fig>
</p><p id="Par27">C3CTA module design.</p><p id="Par28">The traditional C3k2 module combines the variable convolution kernel and the channel separation strategy, which makes the model have a strong ability in feature extraction and can adapt to targets of different scales and shapes. However, in the face of scenes with complex background information, especially in the case of overlapping and occlusion between vehicle targets, traditional feature extraction methods still have certain limitations, resulting in insufficient detection performance. In order to further improve the feature extraction ability of the Backbone network, the Channel Diverting Attention (CTA) mechanism was introduced into the Backbone network, and a new C3CTA module was constructed. The core idea of the channel shifting attention mechanism is to automatically assign a weight to each channel by modeling the relationship between channels inside the feature map. Specifically, the network learns which feature channels are more important for the current task and enhances the influence of these channels, while suppressing those channels that are irrelevant or less important for the current task. This mechanism can help the network to focus on the key information in the image when dealing with scenes with complex backgrounds and overlapping targets, especially when the vehicle targets are occluded or overlapping each other, and improve the detection performance.By introducing the CTA mechanism, the C3CTA module not only enhances the attention of the network to the useful feature channels, but also effectively suppresses the interference of irrelevant background information, so as to improve the detection ability of the model in complex environments, especially when the background is cluttered and there is overlap between the targets, showing stronger robustness. This improvement greatly enhances the network&#8217;s ability to identify and detect vehicle targets in complex scenes, especially in practical application scenarios such as automatic driving, which can significantly improve the accuracy and stability of target detection. The C3CTA module consists of two main computational branches operating in parallel: first, the SFA branch, which processes features through the space-frequency attention mechanism, and second, the CTA branch, which applies channel transpose attention for cross-channel feature refinement.</p><p id="Par29">Figure&#160;<xref rid="Fig3" ref-type="fig">3</xref> shows the structure diagram of C3CTA as well as the CTA module. The detailed process of the Spatial Frequency Attention (SFA) mechanism is as follows. The SFA mechanism first converts the input features into a frequency domain representation. Given the input feature map <inline-formula id="IEq1"><alternatives><tex-math id="d33e367">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\text{X}\in\:{\text{R}}^{\text{B}\times\:\text{C}\times\:\text{H}\times\:\text{W}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18263_Article_IEq1.gif"/></alternatives></inline-formula>, the frequency projection is computed as follows:<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e373">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{F}_{freq}=\text{F}\text{r}\text{e}\text{q}\text{P}\text{r}\text{o}\text{j}\left(X\right)={\text{Conv}}_{out}\left(\text{Cat}\left(\text{GELU}\left({X}_{1}\right),\text{R}\text{e}\text{s}\text{B}\text{l}\text{o}\text{c}\text{k}\left({X}_{2}\right)\right)\right)+X$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18263_Article_Equ1.gif"/></alternatives></disp-formula></p><p id="Par30">Where <inline-formula id="IEq2"><alternatives><tex-math id="d33e381">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{\text{X}}_{1},{\text{X}}_{2}=\text{S}\text{p}\text{l}\text{i}\text{t}\left({\text{Conv}}_{1}\left(\text{X}\right)\right)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18263_Article_IEq2.gif"/></alternatives></inline-formula> and residual blocks enhance the high-frequency components. The spatial attention mechanism uses a window-based approach to capture local and global dependencies:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="d33e387">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\text{Attn}\left(Q,K,V\right)=\text{S}\text{o}\text{f}\text{t}\text{m}\text{a}\text{x}\left(\frac{Q{K}^{T}}{\sqrt{{d}_{k}}}+\text{R}\text{P}\text{E}\right)V$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18263_Article_Equ2.gif"/></alternatives></disp-formula></p><p id="Par31">Where RPE represents the relative position encoding calculated by dynamic position deviation:<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="d33e395">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\text{RPE=DynamicPosBias}\left(\text{BiasCoords}\right)$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18263_Article_Equ3.gif"/></alternatives></disp-formula></p><p id="Par32">SFA integrates spatial attention, convolutional features, and frequency information through adaptive gating:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="d33e403">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\begin{array}{rr}{C}_{map}&amp;\:=\text{C}\text{h}\text{a}\text{n}\text{n}\text{e}\text{l}\text{P}\text{r}\text{o}\text{j}\left(\text{Conv}\left(V\right)\right)\\\:{S}_{map}&amp;\:=\text{S}\text{p}\text{a}\text{t}\text{i}\text{a}\text{l}\text{P}\text{r}\text{o}\text{j}\left(\text{Attn}\left(Q,K,V\right)\right)\\\:{X}_{out}&amp;\:=\sigma\:\left({C}_{map}\right)\odot\:{X}_{attn}+\sigma\:\left({S}_{map}\right)\odot\:{X}_{conv}+{X}_{freq}\end{array}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18263_Article_Equ4.gif"/></alternatives></disp-formula></p><p id="Par33">The CTA mechanism operates on the channel dimension to capture the relationship between channels:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="d33e411">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\begin{array}{rr}{Q}_{c},{K}_{c},{V}_{c}&amp;\:=\text{L}\text{i}\text{n}\text{e}\text{a}\text{r}{\left(X\right)}^{T}\text{(transposed\:to\:channel\:dimension)}\\\:{\text{Attn}}_{c}&amp;\:=\text{S}\text{o}\text{f}\text{t}\text{m}\text{a}\text{x}\left(\frac{{Q}_{c}{K}_{c}^{T}}{\tau\:}\right){V}_{c}\end{array}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18263_Article_Equ5.gif"/></alternatives></disp-formula></p><p id="Par34">Where <inline-formula id="IEq3"><alternatives><tex-math id="d33e419">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\text{T}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18263_Article_IEq3.gif"/></alternatives></inline-formula> is a learnable temperature parameter that controls attention sharpness. The next step is dual-path feature enhancement. CTA uses dual-path feature processing:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="d33e425">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\begin{array}{rr}{X}_{attn}&amp;\:={\text{Attn}}_{c}+\text{C}\text{h}\text{a}\text{n}\text{n}\text{e}\text{l}\text{P}\text{r}\text{o}\text{j}\left({\text{Attn}}_{c}\right)\\\:{X}_{conv}&amp;\:=\text{D}\text{W}\text{C}\text{o}\text{n}\text{v}\left(V\right)\\\:{X}_{out}&amp;\:=\sigma\:\left(\text{SpatialProj}\left({X}_{conv}\right)\right)\odot\:{X}_{attn}+\sigma\:\left(\text{ChannelMap}\right)\odot\:{X}_{conv}\end{array}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18263_Article_Equ6.gif"/></alternatives></disp-formula></p><p id="Par35">Next is the Dual-frequency Aggregated feedforward Network (DFFN), which enhances the feature representation with frequency-aware processing:<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="d33e434">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\begin{array}{rr}{X}_{ffn}&amp;\:={\text{Linear}}_{1}\left(X\right)\\\:{X}_{1},{X}_{2}&amp;\:=\text{S}\text{p}\text{l}\text{i}\text{t}\left({X}_{ffn}\right)\\\:{X}_{gated}&amp;\:={X}_{1}\odot\:\text{C}\text{o}\text{n}\text{v}\left(\text{LayerNorm}\left({X}_{2}\right)\right)\\\:{X}_{out}&amp;\:={\text{Linear}}_{2}\left({X}_{gated}\right)\end{array}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18263_Article_Equ7.gif"/></alternatives></disp-formula></p><p id="Par36">The full C3CTA forward pass integrates two branches:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="d33e442">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\begin{array}{rr}{X}_{sfa}&amp;\:=X+\text{D}\text{r}\text{o}\text{p}\text{P}\text{a}\text{t}\text{h}\left(\text{SFA}\left(\text{LayerNorm}\left(X\right)\right)\right)\\\:{X}_{sfa}&amp;\:={X}_{sfa}+\text{D}\text{r}\text{o}\text{p}\text{P}\text{a}\text{t}\text{h}\left(\text{DFFN}\left(\text{LayerNorm}\left({X}_{sfa}\right)\right)\right)\\\:{X}_{cta}&amp;\:=X+\text{D}\text{r}\text{o}\text{p}\text{P}\text{a}\text{t}\text{h}\left(\text{CTA}\left(\text{LayerNorm}\left(X\right)\right)\right)\\\:{X}_{cta}&amp;\:={X}_{cta}+\text{D}\text{r}\text{o}\text{p}\text{P}\text{a}\text{t}\text{h}\left(\text{DFFN}\left(\text{LayerNorm}\left({X}_{cta}\right)\right)\right)\end{array}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18263_Article_Equ8.gif"/></alternatives></disp-formula></p><p id="Par37">
<fig id="Fig3" position="float" orientation="portrait"><label>Fig. 3</label><caption><p>C3CTA structure diagram and CTA module.</p></caption><graphic id="d33e456" position="float" orientation="portrait" xlink:href="41598_2025_18263_Fig3_HTML.jpg"/></fig>
</p><p id="Par38">Feature Diffusion Focused Pyramid Network.</p><p id="Par39">In the backbone network, the input image is processed by multiple downsampling and convolution modules, and a multi-scale feature map is generated using a bottom-up approach. In the scene of roadside target detection, due to the variable size and uneven distribution of targets, most of the common targets in the image are small targets. In order to improve the detection accuracy, this study innovated the Neck part of the backbone network, proposed a new feature diffusion-focused pyramid network (DFPN), and added the FoFe (Focus Features) Module to the Neck part. Through the customized feature focusing module and feature diffusion mechanism, our method can introduce richer context information into the feature maps at each scale. This approach ensures that features at different scales not only preserve local details but also effectively capture global information, thus improving the accuracy and robustness of subsequent object detection and classification tasks. In this way, the model can better understand complex scenes and improve the recognition ability of objects at different scales.</p><p id="Par40">The custom feature focusing module is designed to support processing three different scales of input, and an internally integrated inception-style module is used to further optimize feature extraction. The module uses a set of parallel deep convolution operations, which can effectively capture rich information across multiple scales, and then enhance the adaptability of the network to multi-scale objects. Through this structure, the network can not only extract useful information in feature maps with different resolutions, but also combine multi-level context to improve the overall performance of the model.</p><p id="Par41">Under the action of the feature diffusion mechanism, the features with rich context information are further diffused into different detection scales. Through this mechanism, the network is able to transfer important context information across multiple scales, thus ensuring that useful information at other scales can be utilized at each scale. This process of information diffusion can significantly improve the performance of the model in complex scenes, especially in the face of occlusion, overlap, and other challenges, which can effectively improve the accuracy and reliability of object detection.</p><p id="Par42">The FoFe module consists of a small kernel convolution to capture local information, followed by a series of parallel depthwise separable convolutions to capture contextual information across multiple scales. Formally, FoFe modules can be mathematically represented as follows.<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="d33e468">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{L}_{l-1,n}={\text{C}\text{o}\text{n}\text{v}}_{{k}_{s}\times\:{k}_{s}}\left({X}_{l-1,n}^{\left(2\right)}\right),n=0,\dots\:,{N}_{l}-1$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18263_Article_Equ9.gif"/></alternatives></disp-formula><disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="d33e474">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{\text{Z}}_{\text{l}-1,\text{n}}^{\left(\text{m}\right)}={\text{D}\text{W}\text{C}\text{o}\text{n}\text{v}}_{{\text{k}}^{\left(\text{m}\right)}\times\:{\text{k}}^{\left(\text{m}\right)}}\left({\text{L}}_{\text{l}-1,\text{n}}\right),\text{m}=1,\dots\:,4$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18263_Article_Equ10.gif"/></alternatives></disp-formula></p><p id="Par43">Here, <inline-formula id="IEq4"><alternatives><tex-math id="d33e482">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{\text{L}}_{\text{l}-1,\text{n}}\in\:{\text{R}}^{\frac{1}{2}{\text{C}}_{\text{l}}\times\:{\text{H}}_{\text{l}}\times\:{\text{W}}_{\text{l}}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18263_Article_IEq4.gif"/></alternatives></inline-formula> is the local feature extracted by ks&#215;ks convolution, while <inline-formula id="IEq5"><alternatives><tex-math id="d33e488">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{\text{Z}}_{\text{l}-1,\text{n}}^{\left(\text{m}\right)}\in\:{\text{R}}^{\frac{1}{2}{\text{C}}_{\text{l}}\times\:{\text{H}}_{\text{l}}\times\:{\text{W}}_{\text{l}}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18263_Article_IEq5.gif"/></alternatives></inline-formula> is the contextual feature extracted by the mth k(m)&#215;k(m) depthwise separable convolution (DWConv).</p><p id="Par44">We set ks&#8201;=&#8201;3 and k(m) = (m&#8201;+&#8201;1)&#215;2&#8201;+&#8201;1. For <italic toggle="yes">n</italic>&#8201;=&#8201;0, we have <inline-formula id="IEq6"><alternatives><tex-math id="d33e499">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{\text{X}}_{\text{l}-1,\text{n}}^{\left(2\right)}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18263_Article_IEq6.gif"/></alternatives></inline-formula>=<inline-formula id="IEq7"><alternatives><tex-math id="d33e505">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{\text{X}}_{\text{l}-1}^{\left(2\right)}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18263_Article_IEq7.gif"/></alternatives></inline-formula>. Then, local features and context features are fused by 1&#8201;&#215;&#8201;1 convolution to characterize the relationship between different channels:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="d33e511">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{P}_{l-1,n}={\text{C}\text{o}\text{n}\text{v}}_{1\times\:1}\left({L}_{l-1,n}+\sum\:_{m=1}^{4}{\mathbf{Z}}_{l-1,n}^{\left(m\right)}\right)$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18263_Article_Equ11.gif"/></alternatives></disp-formula></p><p id="Par45">Where <inline-formula id="IEq8"><alternatives><tex-math id="d33e519">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:{\text{P}}_{\text{l}-1,\text{n}}\in\:{\text{R}}^{\frac{1}{2}{\text{C}}_{\text{l}}\times\:{\text{H}}_{\text{l}}\times\:{\text{W}}_{\text{l}}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18263_Article_IEq8.gif"/></alternatives></inline-formula>denotes the output feature. The 1&#8201;&#215;&#8201;1 convolution is used as a channel fusion mechanism that aims to integrate features with different receptive field sizes. In this way, our FoFe module is able to effectively capture a wide range of contextual information while keeping the details of local texture features unaffected. The specific structure of FoFe (Focus Features Module) module is shown in Fig.&#160;<xref rid="Fig4" ref-type="fig">4</xref>.</p><p id="Par46">
<fig id="Fig4" position="float" orientation="portrait"><label>Fig. 4</label><caption><p>Structure diagram of FoFe module.</p></caption><graphic id="d33e537" position="float" orientation="portrait" xlink:href="41598_2025_18263_Fig4_HTML.jpg"/></fig>
</p><p id="Par47">Lightweight shared convolutional Detection head.</p><p id="Par48">The Lightweight Shared Convolution Detection Head (LSCD) effectively improves the detection ability and computational efficiency of the model by using the hierarchical shared convolution structure and multi-scale feature fusion method. Specifically, the LSCD module receives feature maps from different scales (e.g., P3, P4, P5), and makes full use of different granularity information by fusing these multi-scale features, so as to obtain richer context information. The advantage of multi-scale feature fusion is that the network can process and synthesize feature maps from different resolutions at the same time, which represent the information of objects at different scales, helping the model to better capture the details and global information of the target, thereby improving the detection accuracy, especially when dealing with targets with multiple scales. In addition, the LSCD module adopts the design of shared convolutional layers, and this strategy not only reduces the number of parameters but also significantly improves the computational efficiency. In traditional detection models, each detection head usually requires an independent convolutional layer, which leads to a significant increase in the number of parameters and the calculation of the model. However, LSCD reduces redundant calculations by sharing the convolution operation, where multiple detection heads share the same convolution weights. This design effectively reduces the computational burden of the model, so that it can maintain low latency and real-time response ability while ensuring efficient calculation, and is suitable for application scenarios that require high-speed reasoning. The structural diagram of the LSCD is shown in Fig.&#160;<xref rid="Fig5" ref-type="fig">5</xref>. In the LSCD module, in order to solve the problem faced by different detection heads when dealing with inconsistent target scales, we introduce a Scale Layer. The function of this scale layer is to perform feature scaling on the output of the regression branch, which in turn adjusts the scale of the object in order to localize objects of different sizes more accurately. Due to the scale difference of objects, traditional detection methods may have differences in dealing with small and large objects, while the scale layer ensures that the model can better deal with multi-scale objects by adaptively adjusting the feature map. A similar mechanism can also be found in the FPN (Feature Pyramid Networks) architecture, where multi-scale feature fusion is used to improve the detection performance of the model for objects of different sizes. FPN combines feature maps of different levels to adapt to objects of different sizes. However, the scale layer in LSCD is not only a simple feature fusion, but also provides more flexibility and controllability on this basis, which can more accurately adjust the relationship between each scale and improve the adaptability to different target sizes. The introduction of a scale layer optimizes the existing multi-scale fusion method. Compared with traditional structures such as FPN, the scale layer provides more fine-grained scale adjustment capabilities for LSCD, which enhances the applicability of shared convolution in multi-scale object detection. More importantly, this design reduces the negative impact on the quality of the feature map, because it makes the objects of different sizes be more accurately expressed and located in a unified feature space by adjusting and enhancing the features of different scales.</p><p id="Par49">In the classification branch, the LSCD model predicts the probability of each target class by using 1&#8201;&#215;&#8201;1 convolutional layers. This design allows the network to make independent predictions about the class of each pixel location while working in concert with other branches, such as the localization branch. To ensure that the model can effectively handle the task of object localization and classification separately, the weights of the convolutional layers of the classification branch and the regression branch are independent. This independence allows the network to focus on different tasks in the learning process and optimize the localization accuracy and classification accuracy, respectively, thus avoiding the mutual interference between the two.</p><p id="Par50">In the output stage, the model combines the results of the classification branch and the regression branch to obtain the actual keypoint coordinates by decoding the predicted keypoint features. This process ensures the improvement of positioning accuracy and makes the final detection results more accurate, especially when dealing with fine-grained targets. It can ensure the accurate positioning of key points. This method effectively improves the overall accuracy of object detection, especially in complex scenes. In addition, the LSCD model adopts the weight design of shared convolutional layers, which significantly reduces the number of parameters and computational complexity of the model. By sharing weights, the model avoids redundant convolution calculations, thereby improving computational efficiency and reducing memory usage.</p><p id="Par51">
<fig id="Fig5" position="float" orientation="portrait"><label>Fig. 5</label><caption><p>Structure diagram of LSCD.</p></caption><graphic id="d33e558" position="float" orientation="portrait" xlink:href="41598_2025_18263_Fig5_HTML.jpg"/></fig>
</p></sec><sec id="Sec6"><title>Experiment and analysis</title><p id="Par52">Datasets and Data processing.</p><p id="Par53">The dataset used in this study is the world&#8217;s first large-scale time-series vehicle-road collaborative autonomous driving dataset, DAIR-V2X-I, released by the Institute of Artificial Intelligence (AIR) of Tsinghua University in February 2022. The dataset covers a variety of scenes in Beijing&#8217;s advanced autonomous driving demonstration zone, including urban roads, highways, and 28 intersections, involving different weather and lighting conditions, such as sunny, rainy, foggy, day, and night scenes. The DAIR-V2X-I dataset consists of image data with a total of ten categories: cars, trucks, vans, buses, pedestrians, cyclists, tricycles, motorcycles, carts, and traffic barrel cones. See Fig.&#160;<xref rid="Fig6" ref-type="fig">6</xref> for an example dataset. The final dataset consists of 7,058 images, which are randomly divided into a training set (5,081 images, approximately 70%), a validation set (565 images, approximately 10%), and a test set (1,412 images, approximately 20%) with a ratio of 7:1:2.</p><p id="Par54">
<fig id="Fig6" position="float" orientation="portrait"><label>Fig. 6</label><caption><p>Sample diagram of the DAIR-V2X-I data set.</p></caption><graphic id="d33e577" position="float" orientation="portrait" xlink:href="41598_2025_18263_Fig6_HTML.jpg"/></fig>
</p><p id="Par55">Experimental environment and evaluation metrics.</p><p id="Par56">The experimental environment is the operating system is Windows11, the processor is Intel i5-12600KF, the memory size is 32GB, and the graphics card is NVIDIA GeForce RTX 3080Ti 12GB graphics card for training. The development tools are VsCode, Python version 3.10.14, Pytorch version 2.2.2, and CUDA version 12.1. The image size during training is 640*640, the initial learning rate is 0.01, the batch is 16, and the epochs are 150.The experimental environment is shown in Table&#160;<xref rid="Tab1" ref-type="table">1</xref>, and the training parameters are presented in Table&#160;<xref rid="Tab2" ref-type="table">2</xref>.</p><p id="Par57">Since in the scenario of road target detection, it is often necessary to have a more lightweight model and a faster detection speed, we used the lightweight version of YOLO11n for all the experimental parts in this paper.</p><p id="Par58">
<table-wrap id="Tab1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Experimental environment.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Operating system</th><th align="left" colspan="1" rowspan="1">processor</th><th align="left" colspan="1" rowspan="1">memory size</th><th align="left" colspan="1" rowspan="1">graphics card</th><th align="left" colspan="1" rowspan="1">development tools</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Windows11</td><td align="left" colspan="1" rowspan="1">i5-12600KF</td><td align="left" colspan="1" rowspan="1">32GB</td><td align="left" colspan="1" rowspan="1">RTX 3080Ti 12GB</td><td align="left" colspan="1" rowspan="1">VsCode</td></tr></tbody></table></table-wrap>
</p><p id="Par59">
<table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Evaluation metrics.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Python version</th><th align="left" colspan="1" rowspan="1">Pytorch version</th><th align="left" colspan="1" rowspan="1">CUDA version</th><th align="left" colspan="1" rowspan="1">image size</th><th align="left" colspan="1" rowspan="1">learning rate</th><th align="left" colspan="1" rowspan="1">batch</th><th align="left" colspan="1" rowspan="1">epochs</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">3.10.14</td><td align="left" colspan="1" rowspan="1">2.2.2</td><td align="left" colspan="1" rowspan="1">12.1</td><td align="left" colspan="1" rowspan="1">640*640</td><td align="left" colspan="1" rowspan="1">0.01</td><td align="left" colspan="1" rowspan="1">16</td><td align="left" colspan="1" rowspan="1">150</td></tr></tbody></table></table-wrap>
</p><p id="Par60">The experiment will evaluate the performance of different models from the following aspects: Precision, Recall, mean Average Precision (mAP), detection speed (FPS), Params, GFLOPs. It is calculated as follows:<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="d33e679">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\text{P}\text{r}\text{e}\text{c}\text{i}\text{s}\text{i}\text{o}\text{n}=\frac{TP}{TP+FP}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18263_Article_Equ12.gif"/></alternatives></disp-formula><disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="d33e685">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:\text{R}\text{e}\text{c}\text{a}\text{l}\text{l}=\frac{\text{T}\text{P}}{\text{T}\text{P}+\text{F}\text{N}}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18263_Article_Equ13.gif"/></alternatives></disp-formula><disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="d33e691">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\:mAP=\frac{1}{m}\sum\:_{i=1}^{m}A{P}_{i}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18263_Article_Equ14.gif"/></alternatives></disp-formula></p><p id="Par61">Where TP (True Positives) represents the number of instances that were correctly predicted as the target, FP (False Positives) represents the number of instances that were incorrectly predicted as the target, and FN (False Negatives) represents the number of target instances that were not correctly predicted. mAP is the most commonly used evaluation metric in object detection tasks. It is the Average AP (Average Precision) over all classes. AP itself is to calculate the average of the precision of a certain category under different recall rates, which is usually calculated under different IoU (Intersection over Union) thresholds, and the IoU thresholds of 0.5 and 0.95 are used for calculation in this experiment.</p></sec><sec id="Sec7"><title>Experiment and result analysis</title><sec id="Sec8"><title>Comparison of ablation experiments</title><p id="Par62">In order to verify the effectiveness of the C3CTA module, DFPN network, and LSCD detection head, it is added one by one to the benchmark model YOLO11, and the parameters, mAP@0.5 and FPS, are considered. The results of the comparative experiment are shown in Table&#160;<xref rid="Tab3" ref-type="table">3</xref>. The ablation experiments clearly demonstrated the positive impact of adding modules on the model&#8217;s performance. Compared to the baseline (Group 1, 83.3% mAP@0.5), adding any single module from DFPN, C3CTA, or LSCD (Groups 2&#8211;4) could improve the accuracy (reaching 84.2%, 83.9%, and 84.1% respectively). The combination of adding modules led to a more significant performance leap: any two-module combination (Groups 5&#8211;7) increased mAP@0.5 to above 84.8%, with the highest reaching 85.2% (DFPN&#8201;+&#8201;C3CTA). Most importantly, integrating all three modules simultaneously (Group 8) achieved the best performance (85.3% mAP@0.5), with an improvement of 2.0% points compared to the baseline. Notably, this highest performance was achieved with a parameter size (2.51&#160;M) that was lower than the baseline (2.61&#160;M) and most other combinations (such as DFPN&#8201;+&#8201;C3CTA at 2.77&#160;M), highlighting the synergistic optimization of model efficiency while significantly improving detection accuracy through module combinations.</p><p id="Par63">
<table-wrap id="Tab3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Ablation results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Experimental group</th><th align="left" colspan="1" rowspan="1">DFPN</th><th align="left" colspan="1" rowspan="1">C3CTA</th><th align="left" colspan="1" rowspan="1">LSCD</th><th align="left" colspan="1" rowspan="1">Parameters</th><th align="left" colspan="1" rowspan="1">mAP@0.5%</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Group 1</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">2.61&#160;M</td><td char="." align="char" colspan="1" rowspan="1">83.3</td></tr><tr><td align="left" colspan="1" rowspan="1">Group 2</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">2.62&#160;M</td><td char="." align="char" colspan="1" rowspan="1">84.2</td></tr><tr><td align="left" colspan="1" rowspan="1">Group 3</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">2.60&#160;M</td><td char="." align="char" colspan="1" rowspan="1">83.9</td></tr><tr><td align="left" colspan="1" rowspan="1">Group 4</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">2.57&#160;M</td><td char="." align="char" colspan="1" rowspan="1">84.1</td></tr><tr><td align="left" colspan="1" rowspan="1">Group 5</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">2.59&#160;M</td><td char="." align="char" colspan="1" rowspan="1">84.8</td></tr><tr><td align="left" colspan="1" rowspan="1">Group 6</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">2.77&#160;M</td><td char="." align="char" colspan="1" rowspan="1">85.2</td></tr><tr><td align="left" colspan="1" rowspan="1">Group 7</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">2.56&#160;M</td><td char="." align="char" colspan="1" rowspan="1">85.0</td></tr><tr><td align="left" colspan="1" rowspan="1">Group 8</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">2.51&#160;M</td><td char="." align="char" colspan="1" rowspan="1">85.3</td></tr></tbody></table></table-wrap>
</p></sec></sec><sec id="Sec9"><title>Comparison of attention experiments</title><p id="Par64">In order to verify the effect of adding the attention mechanism to the C3k2 module, we introduce a variety of attention mechanisms into the YOLO11 basic model, and conduct performance comparison experiments through the mAP@0.5 index. We also conducted a comprehensive comparison using both CTA attention and our improved C3CTA.The results show that the overall performance of the model using the CTA attention mechanism is significantly improved in the detection task, and its mAP@0.5 value is significantly higher than that of other attention mechanisms. Therefore, YOLO11-C3CTA performs the best in all experiments. The detailed experimental results can be seen in Table&#160;<xref rid="Tab4" ref-type="table">4</xref>.</p><p id="Par65">
<table-wrap id="Tab4" position="float" orientation="portrait"><label>Table 4</label><caption><p>Comparison of attention experiment results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Model</th><th align="left" colspan="1" rowspan="1">mAP@0.5%</th><th align="left" colspan="1" rowspan="1"><italic toggle="yes">P</italic>%</th><th align="left" colspan="1" rowspan="1"><italic toggle="yes">R</italic>%</th><th align="left" colspan="1" rowspan="1">mAP@0.5&#8211;95%</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">YOLO11</td><td char="." align="char" colspan="1" rowspan="1">83.3</td><td char="." align="char" colspan="1" rowspan="1">82.4</td><td align="left" colspan="1" rowspan="1">77</td><td char="." align="char" colspan="1" rowspan="1">59.6</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO11-GAM<sup><xref ref-type="bibr" rid="CR26">26</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">83.6</td><td char="." align="char" colspan="1" rowspan="1">82.6</td><td align="left" colspan="1" rowspan="1">77.3</td><td char="." align="char" colspan="1" rowspan="1">59.6</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO11-CBAM<sup><xref ref-type="bibr" rid="CR27">27</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">83.7</td><td char="." align="char" colspan="1" rowspan="1">82.6</td><td align="left" colspan="1" rowspan="1">77.4</td><td char="." align="char" colspan="1" rowspan="1">59.8</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO11-EMA<sup><xref ref-type="bibr" rid="CR28">28</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">82.3</td><td char="." align="char" colspan="1" rowspan="1">81.8</td><td align="left" colspan="1" rowspan="1">76.4</td><td char="." align="char" colspan="1" rowspan="1">58.8</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO11-CGA<sup><xref ref-type="bibr" rid="CR29">29</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">83.7</td><td char="." align="char" colspan="1" rowspan="1">82.6</td><td align="left" colspan="1" rowspan="1">77.6</td><td char="." align="char" colspan="1" rowspan="1">59.6</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO11-CTA</td><td char="." align="char" colspan="1" rowspan="1">83.8</td><td char="." align="char" colspan="1" rowspan="1">82.8</td><td align="left" colspan="1" rowspan="1">77.4</td><td char="." align="char" colspan="1" rowspan="1">59.8</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO11-C3CTA</td><td char="." align="char" colspan="1" rowspan="1">84.5</td><td char="." align="char" colspan="1" rowspan="1">83.8</td><td align="left" colspan="1" rowspan="1">77.9</td><td char="." align="char" colspan="1" rowspan="1">60.2</td></tr></tbody></table></table-wrap>
</p></sec><sec id="Sec10"><title>Lightweight detection head comparative experiment</title><p id="Par66">To further illustrate the advantages of our LSCD in the road detection scenario, we conducted a comparative experiment with other outstanding detection heads.The specific numerical values of the experiment are shown in Table&#160;<xref rid="Tab5" ref-type="table">5</xref>.The experimental results show that among all the comparison models, YOLO11-LSCD achieved the best performance in all the performance indicators. Its mAP@0.5 reached 84.1%, higher than all the other variants. At the same time, it also had the highest precision rate (P: 83.2%) and recall rate (R: 77.8%), demonstrating stronger detection accuracy and robustness. Moreover, in the stricter mAP@0.5&#8211;0.95 indicators, YOLO11-LSCD also led with a score of 60.4%, further verifying its excellent comprehensive detection ability in multi-scale and complex scenarios.</p><p id="Par67">
<table-wrap id="Tab5" position="float" orientation="portrait"><label>Table 5</label><caption><p>Comparison of detection head experiment results.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Model</th><th align="left" colspan="1" rowspan="1">mAP@0.5%</th><th align="left" colspan="1" rowspan="1"><italic toggle="yes">P</italic>%</th><th align="left" colspan="1" rowspan="1"><italic toggle="yes">R</italic>%</th><th align="left" colspan="1" rowspan="1">mAP@0.5&#8211;95%</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">YOLO11</td><td char="." align="char" colspan="1" rowspan="1">83.3</td><td char="." align="char" colspan="1" rowspan="1">82.4</td><td align="left" colspan="1" rowspan="1">77</td><td char="." align="char" colspan="1" rowspan="1">59.6</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO11-BiFPN<sup><xref ref-type="bibr" rid="CR30">30</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">83.7</td><td char="." align="char" colspan="1" rowspan="1">82.2</td><td align="left" colspan="1" rowspan="1">77.4</td><td char="." align="char" colspan="1" rowspan="1">59.7</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO11-PANet<sup><xref ref-type="bibr" rid="CR31">31</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">82.4</td><td char="." align="char" colspan="1" rowspan="1">81.8</td><td align="left" colspan="1" rowspan="1">76.2</td><td char="." align="char" colspan="1" rowspan="1">58.0</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO11-Dyhead<sup><xref ref-type="bibr" rid="CR32">32</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">83.0</td><td char="." align="char" colspan="1" rowspan="1">82.3</td><td align="left" colspan="1" rowspan="1">77.1</td><td char="." align="char" colspan="1" rowspan="1">59.4</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO11-AFPN<sup><xref ref-type="bibr" rid="CR33">33</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">83.5</td><td char="." align="char" colspan="1" rowspan="1">82.4</td><td align="left" colspan="1" rowspan="1">77.6</td><td char="." align="char" colspan="1" rowspan="1">59.5</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO11-LSCD</td><td char="." align="char" colspan="1" rowspan="1">84.1</td><td char="." align="char" colspan="1" rowspan="1">83.2</td><td align="left" colspan="1" rowspan="1">77.8</td><td char="." align="char" colspan="1" rowspan="1">60.4</td></tr></tbody></table></table-wrap>
</p><sec id="Sec11"><title>Comparison of excellent model experiments</title><p id="Par68">In order to prove the advantages of the proposed algorithm, the proposed algorithm is trained and tested with other excellent object detection algorithms. The specific experimental comparison values are shown in Table&#160;<xref rid="Tab6" ref-type="table">6</xref>.Based on the provided experimental data, our model demonstrates significant advantages in multiple core performance indicators, and its overall performance surpasses that of the existing mainstream models. Precision (85.7%) and Recall (79.4%) both ranked first, outperforming Mamba-YOLO-B (85.6%/79.1%) and RT-DETR-r18 (85.5%/78.9%). Both mAP@0.5 (85.3%) and mAP@0.5&#8211;95 (61.3%) reached the highest values, verifying the model&#8217;s superiority in terms of localization accuracy and multi-scale generalization ability. FPS (167.5) far exceeds the computationally-intensive models Mamba-YOLO-B (118) and RT-DETR-r18 (101), approaching the real-time performance level of lightweight YOLOv5n/v8n (173&#8211;177 FPS). The parameter count (2.51&#160;M) is significantly lower than Mamba-YOLO-B (21.79&#160;M) and RT-DETR&#8201;+&#8201;18 (19.88&#160;M), even lower than YOLOv8n (2.71&#160;M), indicating that the model structure is highly streamlined. The computational power (7.6 GFLOPs) is less than that of Mamba-YOLO-B (49.6) and RT-DETR-r18 (57.0), being only 1/6 of them, which meets the low computational power requirements of vehicle-mounted devices. Compared with lightweight models (such as YOLO12n/YOLO13n), our model achieves an improvement of approximately 3% in accuracy metrics (such as mAP@0.5 increasing by 2.1&#8211;2.3%), while maintaining a similar inference speed. Compared with models of the same precision level (such as Mamba-YOLO-B), the inference speed of Ours has increased by 42%, and the parameter quantity has been reduced by 11.5%, achieving a breakthrough of &#8220;high precision&#8201;+&#8201;low complexity&#8221;. The Ours model has achieved triple breakthroughs in terms of accuracy, speed, and lightweighting in the autonomous driving scenario. Its efficient feature extraction and optimized architecture design provide a better solution for real-time road target detection, and are particularly suitable for deployment on vehicle platforms with limited computing power.</p></sec><sec id="Sec12"><title>Visual comparison of detection effect</title><p id="Par69">In order to more intuitively verify the performance of the proposed algorithm in the actual scene, the representative scenes in the DAIR-V2X-I validation set are selected, including the presence of occlusion, dense stacking, low brightness, and complex scenes. For visual comparison with the YOLO11 baseline model, the results are shown in Fig.&#160;<xref rid="Fig7" ref-type="fig">7</xref>. The picture on the left is the YOLO11 model detection, and the picture on the right is the YOLO-FLC model detection, in which the missed detection is circled for easy viewing. From the perspective of the number of detected targets and the accuracy of detection boxes, the proposed algorithm significantly reduces the false detection rate and missed detection rate, especially in the recognition of small targets. This shows that the proposed algorithm has stronger robustness and practical application value when dealing with different scenes, target occlusion and other problems.</p><p id="Par70">
<fig id="Fig7" position="float" orientation="portrait"><label>Fig. 7</label><caption><p>Visual comparison of detection effect.</p></caption><graphic id="d33e1107" position="float" orientation="portrait" xlink:href="41598_2025_18263_Fig7_HTML.jpg"/></fig>
</p><p id="Par71">
<table-wrap id="Tab6" position="float" orientation="portrait"><label>Table 6</label><caption><p>The experimental results were compared with each model.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Model</th><th align="left" colspan="1" rowspan="1">Precision%</th><th align="left" colspan="1" rowspan="1">Recall%</th><th align="left" colspan="1" rowspan="1">mAP@0.5%</th><th align="left" colspan="1" rowspan="1">mAP@0.5&#8211;95%</th><th align="left" colspan="1" rowspan="1">FPS</th><th align="left" colspan="1" rowspan="1">GFLOPs</th><th align="left" colspan="1" rowspan="1">Params</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">YOLOv5n</td><td char="." align="char" colspan="1" rowspan="1">82.3</td><td char="." align="char" colspan="1" rowspan="1">75.2</td><td align="left" colspan="1" rowspan="1">83</td><td char="." align="char" colspan="1" rowspan="1">59.4</td><td align="left" colspan="1" rowspan="1">173.2</td><td align="left" colspan="1" rowspan="1">6.0</td><td align="left" colspan="1" rowspan="1">2.21&#160;M</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLOv8n</td><td char="." align="char" colspan="1" rowspan="1">83.2</td><td char="." align="char" colspan="1" rowspan="1">75.8</td><td align="left" colspan="1" rowspan="1">83.5</td><td char="." align="char" colspan="1" rowspan="1">59.8</td><td align="left" colspan="1" rowspan="1">176.8</td><td align="left" colspan="1" rowspan="1">7.0</td><td align="left" colspan="1" rowspan="1">2.71&#160;M</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO11n(base)</td><td char="." align="char" colspan="1" rowspan="1">82.4</td><td char="." align="char" colspan="1" rowspan="1">77.0</td><td align="left" colspan="1" rowspan="1">83.3</td><td char="." align="char" colspan="1" rowspan="1">59.6</td><td align="left" colspan="1" rowspan="1">172.4</td><td align="left" colspan="1" rowspan="1">6.5</td><td align="left" colspan="1" rowspan="1">2.61&#160;M</td></tr><tr><td align="left" colspan="1" rowspan="1">Mamba-YOLO-B<sup><xref ref-type="bibr" rid="CR34">34</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">85.6</td><td char="." align="char" colspan="1" rowspan="1">79.1</td><td align="left" colspan="1" rowspan="1">85.2</td><td char="." align="char" colspan="1" rowspan="1">61.1</td><td align="left" colspan="1" rowspan="1">118</td><td align="left" colspan="1" rowspan="1">49.6</td><td align="left" colspan="1" rowspan="1">21.79&#160;M</td></tr><tr><td align="left" colspan="1" rowspan="1">RT-DETR-r18</td><td char="." align="char" colspan="1" rowspan="1">85.5</td><td char="." align="char" colspan="1" rowspan="1">78.9</td><td align="left" colspan="1" rowspan="1">85.1</td><td char="." align="char" colspan="1" rowspan="1">59.9</td><td align="left" colspan="1" rowspan="1">101</td><td align="left" colspan="1" rowspan="1">57.0</td><td align="left" colspan="1" rowspan="1">19.88&#160;M</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLOX-Tiny<sup><xref ref-type="bibr" rid="CR35">35</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">83.1</td><td char="." align="char" colspan="1" rowspan="1">77.2</td><td align="left" colspan="1" rowspan="1">84.4</td><td char="." align="char" colspan="1" rowspan="1">58.8</td><td align="left" colspan="1" rowspan="1">133.2</td><td align="left" colspan="1" rowspan="1">7.5</td><td align="left" colspan="1" rowspan="1">5.03&#160;M</td></tr><tr><td align="left" colspan="1" rowspan="1">D-Fine-N<sup><xref ref-type="bibr" rid="CR36">36</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">82.4</td><td char="." align="char" colspan="1" rowspan="1">76.8</td><td align="left" colspan="1" rowspan="1">82.1</td><td char="." align="char" colspan="1" rowspan="1">57.2</td><td align="left" colspan="1" rowspan="1">146.7</td><td align="left" colspan="1" rowspan="1">7.1</td><td align="left" colspan="1" rowspan="1">3.73&#160;M</td></tr><tr><td align="left" colspan="1" rowspan="1">D-Fine-S</td><td char="." align="char" colspan="1" rowspan="1">83.5</td><td char="." align="char" colspan="1" rowspan="1">77.1</td><td align="left" colspan="1" rowspan="1">84.2</td><td char="." align="char" colspan="1" rowspan="1">59.3</td><td align="left" colspan="1" rowspan="1">134.5</td><td align="left" colspan="1" rowspan="1">24.8</td><td align="left" colspan="1" rowspan="1">10.18&#160;M</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO12n</td><td char="." align="char" colspan="1" rowspan="1">82.5</td><td char="." align="char" colspan="1" rowspan="1">76.7</td><td align="left" colspan="1" rowspan="1">83.2</td><td char="." align="char" colspan="1" rowspan="1">59.3</td><td align="left" colspan="1" rowspan="1">168.4</td><td align="left" colspan="1" rowspan="1">6.3</td><td align="left" colspan="1" rowspan="1">2.56&#160;M</td></tr><tr><td align="left" colspan="1" rowspan="1">YOLO13n</td><td char="." align="char" colspan="1" rowspan="1">82.7</td><td char="." align="char" colspan="1" rowspan="1">76.9</td><td align="left" colspan="1" rowspan="1">83.0</td><td char="." align="char" colspan="1" rowspan="1">59.1</td><td align="left" colspan="1" rowspan="1">169.3</td><td align="left" colspan="1" rowspan="1">6.2G</td><td align="left" colspan="1" rowspan="1">2.45&#160;M</td></tr><tr><td align="left" colspan="1" rowspan="1">DEIM-D-Fine-N<sup><xref ref-type="bibr" rid="CR37">37</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">83.4</td><td char="." align="char" colspan="1" rowspan="1">76.3</td><td align="left" colspan="1" rowspan="1">81.8</td><td char="." align="char" colspan="1" rowspan="1">56.8</td><td align="left" colspan="1" rowspan="1">155.3</td><td align="left" colspan="1" rowspan="1">7.2</td><td align="left" colspan="1" rowspan="1">3.69&#160;M</td></tr><tr><td align="left" colspan="1" rowspan="1">FBRT-YOLO-S<sup><xref ref-type="bibr" rid="CR38">38</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">84.8</td><td char="." align="char" colspan="1" rowspan="1">78.7</td><td align="left" colspan="1" rowspan="1">84.3</td><td char="." align="char" colspan="1" rowspan="1">60.6</td><td align="left" colspan="1" rowspan="1">142.6</td><td align="left" colspan="1" rowspan="1">22.9G</td><td align="left" colspan="1" rowspan="1">2.9&#160;M</td></tr><tr><td align="left" colspan="1" rowspan="1">Ours</td><td char="." align="char" colspan="1" rowspan="1">85.7</td><td char="." align="char" colspan="1" rowspan="1">79.4</td><td align="left" colspan="1" rowspan="1">85.3</td><td char="." align="char" colspan="1" rowspan="1">61.3</td><td align="left" colspan="1" rowspan="1">167.5</td><td align="left" colspan="1" rowspan="1">7.6</td><td align="left" colspan="1" rowspan="1">2.51&#160;M</td></tr></tbody></table></table-wrap>
</p></sec></sec><sec id="Sec13"><title>Conclusion</title><p id="Par72">Aiming at the problems of low detection accuracy, missed detection, and false detection caused by overlapping targets on the roadside in autonomous driving scenes, this paper proposes an object detection model based on improved YOLO-FLC. Based on the benchmark model YOLO11, the C3CTA module is embedded in the C3k2 module, and the C3CTA module is constructed and added to the backbone network, which can make more effective use of channel information, strengthen the detection ability of target occlusion, and improve the detection accuracy. The feature diffusion focused Pyramid Network (DFPN) was introduced into the Neck network to strengthen the feature extraction ability for targets and improve the detection level for small targets. The original detection head was replaced by LSCD to reduce the number of parameters. According to the experimental results, the detection accuracy of the YOLO-FLC model proposed in this paper is improved to 85.3%, which is 2.4% higher than that of YOLO11. The model achieves unity in detection accuracy, model size, and detection speed, which can meet the needs of autonomous driving scenarios. In the future, we will further improve the effectiveness of the algorithm in special weather conditions to meet more situations in vehicle driving, so that the model can better adapt to the scene detection task of autonomous driving in complex situations.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn><fn><p>These authors jointly supervised this work: Dongpo Wei, Feng Li, Huimin Lv and Shengtao Li.</p></fn></fn-group><notes notes-type="author-contribution"><title>Author contributions</title><p>Conceptualization, D.W. and F.L.; methodology, H.L.; validation, S.L. and H.L.; writing&#8212;original draft preparation, E.L.; writing&#8212;review and editing, E.L.and D.W.All authors have read and agreed to the published version of the manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>This research was supported by Intelligent Manufacturing Engineering Laboratory - Shandong Province Higher Education Characteristic Laboratory(PT2025KJS002).</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>The data sets analyzed in the study are openly available in[DAIR-V2X-I] at https://air.tsinghua.edu.cn/DAIR-V2X/index.html.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par73">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sundaresan Geetha</surname><given-names>A</given-names></name><etal/></person-group><article-title>Comparative analysis of YOLOv8 and YOLOv10 in vehicle detection: performance metrics and model Efficacy[J]</article-title><source>Vehicles</source><year>2024</year><volume>6</volume><issue>3</issue><fpage>1364</fpage><lpage>1382</lpage></element-citation><mixed-citation id="mc-CR1" publication-type="journal">Sundaresan Geetha, A. et al. Comparative analysis of YOLOv8 and YOLOv10 in vehicle detection: performance metrics and model Efficacy[J]. <italic toggle="yes">Vehicles</italic><bold>6</bold> (3), 1364&#8211;1382 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Singh</surname><given-names>K</given-names></name><name name-style="western"><surname>Parihar</surname><given-names>AS</given-names></name><name name-style="western"><surname>Bff</surname></name></person-group><article-title>Bi-stream feature fusion for object detection in hazy environment</article-title><source>SIViP</source><year>2024</year><volume>18</volume><fpage>3097</fpage><lpage>3107</lpage><pub-id pub-id-type="doi">10.1007/s11760-023-02973-6</pub-id></element-citation><mixed-citation id="mc-CR2" publication-type="journal">Singh, K., Parihar, A. S. &amp; Bff Bi-stream feature fusion for object detection in hazy environment. <italic toggle="yes">SIViP</italic><bold>18</bold>, 3097&#8211;3107. 10.1007/s11760-023-02973-6 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>MAHAUR</surname><given-names>B</given-names></name></person-group><article-title>Small-Object detection based on Yolov5 in autonomous driving Systems[J]</article-title><source>Pattern Recognit. Lett.</source><year>2023</year><volume>168</volume><fpage>115</fpage><lpage>122</lpage></element-citation><mixed-citation id="mc-CR3" publication-type="journal">Mahaur, B.Mishra K K. Small-Object detection based on Yolov5 in autonomous driving Systems[J]. <italic toggle="yes">Pattern Recognit. Lett.</italic><bold>168</bold>, 115&#8211;122 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bakirci</surname><given-names>M</given-names></name></person-group><article-title>Enhancing vehicle detection in intelligent transportation systems via autonomous UAV platform and YOLOv8 integration[J]</article-title><source>Appl. Soft Comput.</source><year>2024</year><volume>164</volume><fpage>112015</fpage></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Bakirci, M. Enhancing vehicle detection in intelligent transportation systems via autonomous UAV platform and YOLOv8 integration[J]. <italic toggle="yes">Appl. Soft Comput.</italic><bold>164</bold>, 112015 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><mixed-citation publication-type="other">WANG C, Y. &amp; BOCHKOVSKIY A, LIAO H Y M. Yolov7: trainable bag-of-freebies sets new state-of-the-art for real-time object detectors[C]. Vancouver: IEEE/CVF Conference on Computer Vision and Pattern Recognition, (2023).</mixed-citation></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>NIE</surname><given-names>H</given-names></name><etal/></person-group><article-title>A lightweight remote sensing small target image detection algorithm based on improved yolov8[J]</article-title><source>Sensors</source><year>2024</year><volume>24</volume><issue>9</issue><fpage>2952</fpage><pub-id pub-id-type="pmid">38733059</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/s24092952</pub-id><pub-id pub-id-type="pmcid">PMC11086322</pub-id></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Nie, H. et al. A lightweight remote sensing small target image detection algorithm based on improved yolov8[J]. <italic toggle="yes">Sensors</italic><bold>24</bold> (9), 2952 (2024).<pub-id pub-id-type="pmid">38733059</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/s24092952</pub-id><pub-id pub-id-type="pmcid">PMC11086322</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>BAO D C</surname></name><name name-style="western"><surname>GAO R J</surname></name><name name-style="western"><surname>Signal</surname></name></person-group><source>Image Video Process.</source><year>2024</year><volume>18</volume><issue>10</issue><fpage>7211</fpage><lpage>7219</lpage></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Bao D C, Gao R J. &amp; Signal Yed-yolo: An Object Detection Algorithm for Automatic Driving[J]. <italic toggle="yes">Image Video Process.</italic>, <bold>18</bold>(10): 7211&#8211;7219. (2024).</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>P</given-names></name><etal/></person-group><article-title>ISOD: improved small object detection based on extended scale feature pyramid network</article-title><source>Vis. Comput.</source><year>2025</year><volume>41</volume><fpage>465</fpage><lpage>479</lpage><pub-id pub-id-type="doi">10.1007/s00371-024-03341-2</pub-id></element-citation><mixed-citation id="mc-CR8" publication-type="journal">Ma, P. et al. ISOD: improved small object detection based on extended scale feature pyramid network. <italic toggle="yes">Vis. Comput.</italic><bold>41</bold>, 465&#8211;479. 10.1007/s00371-024-03341-2 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">CHEN J R, KAO S H, HE, H. et al. Run, Don&#8217;t Walk: Chasing Higher FLOPS for Faster Neural Networks[C]// Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Vancouver, BC, Canada: IEEE, (2023).</mixed-citation></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">HAMZENEJADI, M. H. <italic toggle="yes">MOHSENI H. Fine-Tuned YOLOv5 for Real-Time Vehicle Detection in UAV Imagery: Architectural Improvements and Performance Boost[J]</italic>231 (Expert Systems with Applications, 2023).</mixed-citation></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Singh</surname><given-names>K</given-names></name><name name-style="western"><surname>Anil Singh Parihar</surname></name></person-group><article-title>MRN-LOD: multi-exposure refinement network for low-light object detection</article-title><source>J. Vis. Commun. Image Represent.</source><year>2024</year><volume>99</volume><fpage>104079</fpage></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Singh, K. &amp; Anil Singh Parihar MRN-LOD: multi-exposure refinement network for low-light object detection. <italic toggle="yes">J. Vis. Commun. Image Represent.</italic><bold>99</bold>, 104079 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><mixed-citation publication-type="other">Alif, M. A. R. Yolov11 for vehicle detection: Advancements, performance, and applications in intelligent transportation systems[J]. arxiv preprint arxiv:2410.22898, (2024).</mixed-citation></ref><ref id="CR13"><label>13.</label><mixed-citation publication-type="other">Bakirci, M. &amp; Bayraktar, I. YOLOv9-enabled vehicle detection for urban security and forensics applications[C]//2024 12th International Symposium on Digital Forensics and Security (ISDFS). IEEE, : 1&#8211;6. (2024).</mixed-citation></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhou</surname><given-names>S</given-names></name><etal/></person-group><article-title>Deep learning-based vehicle detection and tracking from roadside lidar data through robust affinity fusion[J]</article-title><source>Expert Syst. Appl.</source><year>2025</year><volume>279</volume><fpage>127338</fpage></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Zhou, S. et al. Deep learning-based vehicle detection and tracking from roadside lidar data through robust affinity fusion[J]. <italic toggle="yes">Expert Syst. Appl.</italic><bold>279</bold>, 127338 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kang</surname><given-names>L</given-names></name><etal/></person-group><article-title>YOLO-FA: Type-1 fuzzy attention based YOLO detector for vehicle detection[J]</article-title><source>Expert Syst. Appl.</source><year>2024</year><volume>237</volume><fpage>121209</fpage></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Kang, L. et al. YOLO-FA: Type-1 fuzzy attention based YOLO detector for vehicle detection[J]. <italic toggle="yes">Expert Syst. Appl.</italic><bold>237</bold>, 121209 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Dai, T. et al. FreqFormer: Frequency-aware Transformer for Lightweight Image Super-resolution. IJCAI. ijcai. org (2024).</mixed-citation></ref><ref id="CR17"><label>17.</label><mixed-citation publication-type="other">Han, K. et al. Ghostnet: More features from cheap operations. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. (2020).</mixed-citation></ref><ref id="CR18"><label>18.</label><mixed-citation publication-type="other">Chen, J. et al. Run, don&#8217;t walk: chasing higher FLOPS for faster neural networks. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. (2023).</mixed-citation></ref><ref id="CR19"><label>19.</label><mixed-citation publication-type="other">Singh, K. &amp; Anil Singh, P. Image Decomposition for Object Detection in Low-light Environment. 2023 International Conference on Advances in Computation, Communication and Information Technology (ICAICCIT). IEEE, (2023).</mixed-citation></ref><ref id="CR20"><label>20.</label><mixed-citation publication-type="other">Hanzla, M., Yusuf, M. O. &amp; Jalal, A. Vehicle surveillance using U-NET segmentation and DeepSORT over aerial images[C]//2024 International Conference on Engineering &amp; Computing Technologies (ICECT). IEEE, : 1&#8211;6. (2024).</mixed-citation></ref><ref id="CR21"><label>21.</label><mixed-citation publication-type="other">H. Sun, G. Yao, S. Zhu, L. Zhang, H. Xu and J. Kong, "SOD-YOLOv10: Small Object Detection in Remote Sensing Images Based on YOLOv10," in IEEE Geoscience and Remote Sensing Letters, vol. 22, pp. 1-5, 2025.</mixed-citation></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zhao</surname><given-names>X</given-names></name><etal/></person-group><article-title>ITD-YOLOv8: an infrared target detection model based on YOLOv8 for unmanned aerial vehicles[J]</article-title><source>Drones</source><year>2024</year><volume>8</volume><issue>4</issue><fpage>161</fpage></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Zhao, X. et al. ITD-YOLOv8: an infrared target detection model based on YOLOv8 for unmanned aerial vehicles[J]. <italic toggle="yes">Drones</italic><bold>8</bold> (4), 161 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>M</given-names></name><name name-style="western"><surname>Fan</surname><given-names>X</given-names></name></person-group><article-title>YOLOv8-lite: A lightweight object detection model for real-time autonomous driving systems[J]</article-title><source>IECE Trans. Emerg. Top. Artif. Intell.</source><year>2024</year><volume>1</volume><issue>1</issue><fpage>1</fpage><lpage>16</lpage></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Yang, M. &amp; Fan, X. YOLOv8-lite: A lightweight object detection model for real-time autonomous driving systems[J]. <italic toggle="yes">IECE Trans. Emerg. Top. Artif. Intell.</italic><bold>1</bold> (1), 1&#8211;16 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>Q</given-names></name><etal/></person-group><article-title>YOLOv8-CB: dense pedestrian detection algorithm based on in-vehicle camera[J]</article-title><source>Electronics</source><year>2024</year><volume>13</volume><issue>1</issue><fpage>236</fpage></element-citation><mixed-citation id="mc-CR24" publication-type="journal">Liu, Q. et al. YOLOv8-CB: dense pedestrian detection algorithm based on in-vehicle camera[J]. <italic toggle="yes">Electronics</italic><bold>13</bold> (1), 236 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sun</surname><given-names>S</given-names></name><etal/></person-group><article-title>Multi-YOLOv8: an infrared moving small object detection model based on YOLOv8 for air vehicle[J]</article-title><source>Neurocomputing</source><year>2024</year><volume>588</volume><fpage>127685</fpage></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Sun, S. et al. Multi-YOLOv8: an infrared moving small object detection model based on YOLOv8 for air vehicle[J]. <italic toggle="yes">Neurocomputing</italic><bold>588</bold>, 127685 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><mixed-citation publication-type="other">Liu, Y., Shao, Z. &amp; Hoffmann, N. Global attention mechanism: retain information to enhance channel-spatial interactions[J]. (2021). arxiv preprint arxiv:2112.05561.</mixed-citation></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Woo, S. et al. Cbam: Convolutional block attention module[C]//Proceedings of the European conference on computer vision (ECCV). : 3&#8211;19. (2018).</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Ouyang, D. et al. Efficient multi-scale attention module with cross-spatial learning[C]//ICASSP 2023&#8211;2023 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, : 1&#8211;5. (2023).</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">Liu, X. et al. Efficientvit: Memory efficient vision transformer with cascaded group attention[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. : 14420&#8211;14430. (2023).</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Tan, M., Pang, R., Le, Q. V. &amp; Efficientdet Scalable and efficient object detection[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. : 10781&#8211;10790. (2020).</mixed-citation></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Liu, S. et al. Path aggregation network for instance segmentation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. : 8759&#8211;8768. (2018).</mixed-citation></ref><ref id="CR32"><label>32.</label><mixed-citation publication-type="other">Dai, X. et al. Dynamic head: Unifying object detection heads with attentions[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. : 7373&#8211;7382. (2021).</mixed-citation></ref><ref id="CR33"><label>33.</label><mixed-citation publication-type="other">Yang, G. et al. AFPN: Asymptotic feature pyramid network for object detection[C]//2023 IEEE International Conference on Systems, Man, and Cybernetics (SMC). IEEE, : 2184&#8211;2189. (2023).</mixed-citation></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Wang, Z. et al. <italic toggle="yes">Mamba YOLO: SSMs-based YOLO for Object detection[J]</italic>240605835 (arxiv, 2024). arxiv e-prints.</mixed-citation></ref><ref id="CR35"><label>35.</label><mixed-citation publication-type="other">Ge, Z. et al. Yolox: Exceeding yolo series in 2021. arxiv preprint arxiv:2107.08430 (2021).</mixed-citation></ref><ref id="CR36"><label>36.</label><mixed-citation publication-type="other">Zheng, Z., Zhao, J., Fan, J. et al. A complex roadside object detection model based on multi-scale feature pyramid network. Sci Rep 15, 15992 (2025). https://doi.org/10.1038/s41598-025-99544-1.<pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-025-99544-1</pub-id><pub-id pub-id-type="pmcid">PMC12062293</pub-id><pub-id pub-id-type="pmid">40341232</pub-id></mixed-citation></ref><ref id="CR37"><label>37.</label><mixed-citation publication-type="other">Huang, S. et al. Deim: Detr with improved matching for fast convergence. Proceedings of the Computer Vision and Pattern Recognition Conference. (2025).</mixed-citation></ref><ref id="CR38"><label>38.</label><mixed-citation publication-type="other">Yao, X. et al. FBRT-YOLO: Faster and Better for Real-Time Aerial Image Detection. Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 39. No. 8. 2025.</mixed-citation></ref></ref-list></back></article>
        
    </metadata>
</record>
    </GetRecord>

</OAI-PMH>