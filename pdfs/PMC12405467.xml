


<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
    <responseDate>2025-09-09T13:47:26Z</responseDate>
    <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:12405467" metadataPrefix="pmc">https://pmc.ncbi.nlm.nih.gov/api/oai/v1/mh/</request>
    
    <GetRecord>
        <record>
    <header>
    <identifier>oai:pubmedcentral.nih.gov:12405467</identifier>
    <datestamp>2025-09-04</datestamp>
    
        
        <setSpec>scirep</setSpec>
        
    
        
        <setSpec>pmc-open</setSpec>
        
    
</header>
    <metadata>
        
        <article xmlns="https://jats.nlm.nih.gov/ns/archiving/1.4/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.4/ https://jats.nlm.nih.gov/archiving/1.4/xsd/JATS-archivearticle1-4.xsd" article-type="research-article" xml:lang="en" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-id journal-id-type="pmc-domain-id">1579</journal-id><journal-id journal-id-type="pmc-domain">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12405467</article-id><article-id pub-id-type="pmcid-ver">PMC12405467.1</article-id><article-id pub-id-type="pmcaid">12405467</article-id><article-id pub-id-type="pmcaiid">12405467</article-id><article-id pub-id-type="pmid">40897773</article-id><article-id pub-id-type="doi">10.1038/s41598-025-18024-8</article-id><article-id pub-id-type="publisher-id">18024</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Risk factor identification mechanism for coronary artery disease based on multiple cross-filtering and binary cuckoo search</article-title></title-group><contrib-group><contrib contrib-type="author"><name name-style="western"><surname>Rao</surname><given-names initials="C">Congjun</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Wang</surname><given-names initials="J">Jing</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author"><name name-style="western"><surname>Liu</surname><given-names initials="Y">Ying</given-names></name><xref ref-type="aff" rid="Aff1">1</xref></contrib><contrib contrib-type="author" corresp="yes"><name name-style="western"><surname>Yuan</surname><given-names initials="J">Jing</given-names></name><address><email>yuanjin220@163.com</email></address><xref ref-type="aff" rid="Aff2">2</xref></contrib><aff id="Aff1"><label>1</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03fe7t173</institution-id><institution-id institution-id-type="GRID">grid.162110.5</institution-id><institution-id institution-id-type="ISNI">0000 0000 9291 3229</institution-id><institution>School of Mathematics and Statistics, </institution><institution>Wuhan University of Technology, </institution></institution-wrap>Wuhan, 430070 People&#8217;s Republic of China </aff><aff id="Aff2"><label>2</label><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/03fe7t173</institution-id><institution-id institution-id-type="GRID">grid.162110.5</institution-id><institution-id institution-id-type="ISNI">0000 0000 9291 3229</institution-id><institution>Wuhan University of Technology Hospital, Wuhan University of Technology, </institution></institution-wrap>Wuhan, 430070 People&#8217;s Republic of China </aff></contrib-group><pub-date pub-type="epub"><day>2</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type="pmc-issue-id">478255</issue-id><elocation-id>32322</elocation-id><history><date date-type="received"><day>2</day><month>1</month><year>2025</year></date><date date-type="accepted"><day>28</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-04 00:25:59.930"><day>04</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri content-type="pmc-pdf" xlink:href="41598_2025_Article_18024.pdf"/><abstract id="Abs1"><p id="Par1">Coronary artery disease (CAD) is a major public health concern, necessitating accurate risk factor identification. However, existing methods often suffer from feature preference bias and insufficient multidimensional evaluation, limiting their reliability. To address this, we propose a novel two-stage mechanism integrating multiple cross-filtering and binary cuckoo search (BCS). In the first stage, features are evaluated from three perspectives&#8212;relevance (chi-square test), information richness (mutual information), and distance (Fisher score)&#8212;to eliminate low-importance features and reduce bias. In the second stage, a random forest classifier optimizes feature selection via BCS, using classification accuracy as the objective function. Empirical analysis on the UCI CAD dataset demonstrates that our method achieves an accuracy of 89%, precision of 0.87, recall of 0.91, F1-score of 0.89, and AUC of 0.93. These values outperform existing homogeneous models (Method II, Method III, Method IV) by at least 3.49%, 3.57%, 3.41%, 3.49%, and 3.33%, respectively. The results highlight superior computational efficiency and predictive performance, making the mechanism a valuable tool for CAD risk assessment.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Coronary artery disease</kwd><kwd>Risk factor identification mechanism</kwd><kwd>Multiple cross-filtering</kwd><kwd>BCS</kwd></kwd-group><kwd-group kwd-group-type="npg-subject"><title>Subject terms</title><kwd>Cardiovascular diseases</kwd><kwd>Computational biology and bioinformatics</kwd><kwd>Cardiology</kwd><kwd>Risk factors</kwd><kwd>Mathematics and computing</kwd></kwd-group><funding-group><award-group><funding-source><institution-wrap><institution-id institution-id-type="FundRef">https://doi.org/10.13039/501100001809</institution-id><institution>National Natural Science Foundation of China</institution></institution-wrap></funding-source><award-id>72071150</award-id><principal-award-recipient><name name-style="western"><surname>Rao</surname><given-names>Congjun</given-names></name></principal-award-recipient></award-group></funding-group><funding-group><award-group><funding-source><institution>Fundamental Research Funds for the Central Universities project under Grant</institution></funding-source><award-id>104972024KFYjc0069</award-id><principal-award-recipient><name name-style="western"><surname>Rao</surname><given-names>Congjun</given-names></name></principal-award-recipient></award-group></funding-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">The World Health Organization identifies cardiovascular and cerebrovascular diseases as the leading cause of death worldwide, claiming that over 18&#160;million people live annually due to risk factors such as hypertension, obesity, high cholesterol, and high blood sugar<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. Among all cardiovascular and cerebrovascular ailments, coronary artery disease (CAD) is the top primary killer, accounting for roughly four out of every five deaths related to cardiovascular disorders. According to data furnished by the World Health Organization (WHO), nearly 31% of the deaths of the global population are attributed to CAD<sup><xref ref-type="bibr" rid="CR2">2</xref>,<xref ref-type="bibr" rid="CR3">3</xref></sup>. CAD pertains to heart disease that arises from coronary atherosclerosis, leading to narrowing or blockage of the vascular lumen. This, in turn, causes myocardial ischemia, hypoxia, or necrosis due to functional alterations in the coronary arteries, such as spasms, and has emerged as one of the principal sources of death and disability<sup><xref ref-type="bibr" rid="CR4">4</xref></sup>. Worldwide, CAD morbidity and mortality increase with age, with older individuals facing increased risks of illness and death. Consequently, exploring the risk factors for CAD and promptly conducting risk assessments, interventions, and preventive measures against it has become an overriding trend.</p><p id="Par3">The National Institutes of Health (NIH) initiated research in Framingham in 1948 and subsequently proposed the concept of a &#8220;risk factor&#8221; in 1961<sup><xref ref-type="bibr" rid="CR5">5</xref></sup>. A risk assessment model for cardiovascular and cerebrovascular diseases was constructed on the basis of diabetes status, smoking habits, age, total cholesterol (TC), and blood pressure<sup><xref ref-type="bibr" rid="CR6">6</xref></sup>. This model was then employed to predict the probability of disease occurrence within a ten-year period in the sample set. Across the Atlantic, in 2003, Europe launched a systematic CAD risk assessment program. The objective of this study was to develop a model that aligns with European clinical practice for evaluating CAD risk. The study utilized diverse populations from 12 countries as experimental subjects, excluding anyone with a preexisting heart disease history prior to the research. Fatal CAD events were set as the study endpoint to investigate the likelihood of the first CAD occurrence within ten years, facilitating timely interventions to mitigate risk<sup><xref ref-type="bibr" rid="CR7">7</xref></sup>. In 2008, the World Health Organization (WHO) released the &#8220;Pocket Guide for Cardiovascular Risk Assessment and Management in the Prevention and Treatment of Cardiovascular Diseases&#8221;, which identified 14 potential risk factors for cardiovascular and cerebrovascular diseases within the WHO epidemiology subregions<sup><xref ref-type="bibr" rid="CR8">8</xref></sup>. Later, in 2013, the American Heart Association (AHA) and the American College of Cardiology (ACC) jointly introduced the pooled cohort risk equation to gauge the 10-year risk of atherosclerotic cardiovascular disease<sup><xref ref-type="bibr" rid="CR9">9</xref></sup>.</p><p id="Par4">CAD is a high-dimensional complex system resulting from synergistic interactions of multiple risk factors across economic, social, behavioral, environmental, and demographic dimensions<sup><xref ref-type="bibr" rid="CR10">10</xref></sup>. Currently, the precise pathogenesis and etiology of CAD remain elusive, and there is ample room for enhancing its therapeutic efficacy. Hence, the crux of CAD prevention and control hinges on the establishment of a scientific prevention and control system and a robust risk assessment mechanism, along with the provision of precise and effective screening technologies. These measures are essential for implementing targeted risk factor interventions, conducting hierarchical management, and ultimately attaining an optimal health outcome<sup><xref ref-type="bibr" rid="CR11">11</xref></sup>. In this study, we systematically investigated the identification of risk factors for CAD. By integrating multiple cross-filtering and BCS methods, we propose a novel mechanism for identifying CAD risk factors. Empirical analysis was carried out via the CAD dataset from the UCI platform.</p><p id="Par5">While hybrid feature selection methods have been extensively employed in coronary artery disease (CAD) risk factor identification, they frequently encounter limitations such as feature preference bias and insufficient multidimensional evaluation of feature significance. To overcome these challenges, this study proposes an innovative two-stage framework that integrates multiperspective cross-filtering with binary cuckoo search (BCS). In the first stage, our multiperspective cross-filtering mechanism comprehensively assesses feature importance through three complementary dimensions: feature relevance, information richness, and interclass distance. This tripartite evaluation strategy effectively mitigates feature selection bias while ensuring a more balanced consideration of feature characteristics. The subsequent stage employs an optimized BCS algorithm that demonstrates superior global search capabilities and accelerated convergence speed compared with conventional optimization methods. By systematically avoiding local optima entrapment, the BCS-based search mechanism guarantees the identification of feature subsets that maximize classification accuracy, thereby substantially enhancing predictive performance. The synergistic integration of these two stages achieves three critical improvements: (1) elimination of single-perspective bias through multidimensional evaluation; (2) enhanced search efficiency and solution quality via evolutionary optimization; and (3) optimal balance between feature subset compactness and diagnostic effectiveness. This methodological advancement represents a significant leap forward in CAD risk factor identification, offering superior robustness and accuracy over existing approaches. The proposed framework not only addresses fundamental limitations in current feature selection paradigms but also establishes a new methodological benchmark for medical feature engineering in cardiovascular research.</p><p id="Par6">The remainder of this paper is structured as follows. In Section&#160;&#8220;<xref rid="Sec2" ref-type="sec">Related works</xref>&#8221;, a comprehensive review of prior related works is provided. Section&#160;&#8220;<xref rid="Sec3" ref-type="sec">Data collection and data preprocessing</xref>&#8221; delves into the data analysis and preprocessing procedures. To conduct descriptive statistical analysis on the research samples, histograms and kernel density curves, which effectively illustrate the relationships between pathogenic factors and disease categories, were generated. Additionally, the unbalanced dataset was converted into a balanced dataset. Section&#160;&#8220;<xref rid="Sec7" ref-type="sec">A risk factor identification mechanism based on multiple cross-filtering and BCS searches</xref>&#8221; proposes a novel risk factor identification mechanism predicated on multiple cross-filtering and binary cuckoo searches. In Section&#160;&#8220;<xref rid="Sec12" ref-type="sec">Empirical analysis of risk factor identification based on the CAD dataset</xref>&#8221;, an empirical analysis of the proposed risk factor identification mechanism is presented, leveraging the CAD datasets sourced from the UCI database platform. Finally, Section&#160;&#8220;<xref rid="Sec16" ref-type="sec">Conclusion</xref>&#8221; concludes the paper, summarizing the key findings and contributions.</p><p id="Par7">The detailed flowchart of the proposed work in this paper is shown in Fig.&#160;<xref rid="Fig1" ref-type="fig">1</xref>.</p></sec><sec id="Sec2"><title>Related works</title><p id="Par8">Starting from the risk factors associated with CAD, some scholars have identified the decisive factors for CAD onset through methods such as feature fusion, feature selection and feature extraction, as well as weighting feature values. Subsequently, they conducted risk assessment and prediction via machine learning or deep learning techniques<sup><xref ref-type="bibr" rid="CR12">12</xref>,<xref ref-type="bibr" rid="CR13">13</xref></sup>. For example, G&#225;rate-Escamila et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> employed chi-square and principal component analysis to reduce feature dimensions and applied machine learning to predict CAD. Ali et al.<sup><xref ref-type="bibr" rid="CR15">15</xref></sup> proposed an intelligent medical system that integrates deep learning and feature fusion for heart disease prediction. Reddy et al.<sup><xref ref-type="bibr" rid="CR16">16</xref></sup> evaluated the risk of myocardial infarction in diabetic patients via multiple linear regression, ridge regression, and LASSO regression and achieved favorable results. Shah et al.<sup><xref ref-type="bibr" rid="CR17">17</xref></sup> proposed a mean-Fischer-based and precision-based feature selection algorithm (AFSA). After further refining the selected feature subset through principal component analysis, they used a support vector machine with a radial basis function kernel to diagnose heart disease. Following feature extraction, Masetic et al.<sup><xref ref-type="bibr" rid="CR18">18</xref></sup> verified that, compared with C4.5 decision trees, K-nearest neighbors, support vector machines, and artificial neural networks, random forests perform better in automatic ECG heartbeat classification and can yield a more accurate diagnosis of heart failure.</p><p id="Par9">
<fig id="Fig1" position="float" orientation="portrait"><label>Fig. 1</label><caption><p>Flowchart of the risk factor identification mechanism of coronary artery disease.</p></caption><graphic id="d33e294" position="float" orientation="portrait" xlink:href="41598_2025_18024_Fig1_HTML.jpg"/></fig>
</p><p id="Par10">The selection of CAD risk factors critically influences the reliability of CAD risk assessment outcomes. In the United States, cardiovascular disease treatment incurs daily expenditures exceeding $1&#160;billion, with stroke, myocardial infarction, and hypertension remaining the predominant mortality drivers<sup><xref ref-type="bibr" rid="CR19">19</xref></sup>. This underscores the imperative for rigorous CAD risk factor identification prior to risk stratification. Liu et al.<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> conducted a case&#8210;control study comparing 148 CAD patients with 120 healthy controls and demonstrated through univariate and multivariate analyses that elevated homocysteine (Hcy) levels, hyperlipidemia, hypertension, and diabetes mellitus significantly correlate with CAD susceptibility. Notably, lipid profiles, including total cholesterol (TC), triglyceride (TG), high-density lipoprotein cholesterol (HDL-C), and low-density lipoprotein cholesterol (LDL-C) levels, exhibited marked intergroup disparities. Complementary studies have identified age, blood pressure, smoking status, abdominal obesity, hemoglobin levels, HDL cholesterol, and HbA1c as clinically significant CAD predictors<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup>. Emerging methodologies leverage machine learning models to extract diagnostic insights from echocardiographic and electrocardiographic data, enabling prediction of CAD-related parameters such as heart rate variability and electrical axis deviation<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>. G&#225;rate-Escamila et al.<sup><xref ref-type="bibr" rid="CR14">14</xref></sup> further validated cholesterol levels, peak heart rate, and chest pain characteristics as machine learning-discernible CAD markers. As epidemiological patterns evolve, the predictive salience of certain traditional risk factors diminishes. Systematic screening and prioritization of established risk indicators via evidence synthesis and feature selection optimization can streamline workflows, maintain diagnostic accuracy, and enhance clinical decision efficiency.</p><p id="Par11">The purpose of feature selection, also termed attribute selection, is to select some of the most effective features from multiple existing features. This helps reduce feature dimensionality and operational complexity. There are three primary feature selection methods: filtering, wrapping, and embedding (see Table&#160;<xref rid="Tab1" ref-type="table">1</xref>). Some scholars have relied solely on one of these methods. For example, Ghasemi and Lee<sup><xref ref-type="bibr" rid="CR23">23</xref></sup> used the wrapper technique to select the most important features with the most intrinsic information of CAD. Majdi et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup> designed diverse feature selection techniques based on the whale algorithm (WOA) by employing two hybrid models. Compared with other wrapper-based algorithms, their proposed method notably enhanced the classification accuracy, guaranteeing the WOA&#8217;s capacity to explore the feature space and single out the most informative attributes for the classification task. Thabtah et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup> proposed a novel feature selection method based on least loss (L2). This method gauges the similarity between the observed and expected probabilities and assigns scores to all independent variables. It can substantially reduce the data dimensionality by deftlying weakly correlated variables without undermining the classifier&#8217;s predictive performance. The results indicated that L2 markedly reduced the number of selected variables in the dataset compared with the chi-square and information gain feature selection methods. Additionally, several other feature selection methods have emerged, such as extreme gradient boosting (XGBoost)<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>the L1 regularized mutual information feature selection algorithm based on copulas (L1-CBFS)<sup><xref ref-type="bibr" rid="CR27">27</xref></sup>recursive feature elimination (RFE)<sup><xref ref-type="bibr" rid="CR28">28</xref></sup>the least absolute shrinkage and selection operator (LASSO)<sup><xref ref-type="bibr" rid="CR29">29</xref></sup>the Boruta algorithm<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>the mix method of KNN and SVM<sup><xref ref-type="bibr" rid="CR31">31</xref></sup>multiple strategies based on the gray wolf optimizer (MSGWO)<sup><xref ref-type="bibr" rid="CR32">32</xref></sup>the role-oriented binary walrus gray wolf algorithm (ROBWGWA)<sup><xref ref-type="bibr" rid="CR33">33</xref></sup>and correlation-based feature selection (CFS)<sup><xref ref-type="bibr" rid="CR34">34</xref></sup>.</p><p id="Par12">Scholars have increasingly explored hybrid feature selection methods that integrate filter, wrapper, and embedded approaches (see Table&#160;<xref rid="Tab1" ref-type="table">1</xref>). For example, Liu et al.<sup><xref ref-type="bibr" rid="CR19">19</xref></sup> developed HGAWE, which combines genetic algorithms with embedded regularization to achieve superior feature selection and classification accuracy across simulation data and gene microarray datasets. Rao et al.<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> implemented a filter-wrapper framework for P2P credit risk assessment, employing the Fisher score, information gain rate, and cost-sensitive RF for initial feature ranking, followed by sequential backward elimination and LASSO logistic optimization on the basis of classification performance. Several studies have demonstrated effective filter&#8210;wrapper integrations. For example, Belouch et al.<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> applied chi-square filtering with information gain/gain ratio analysis and then employed forward search with random forest cross-validation. Hassan et al.<sup><xref ref-type="bibr" rid="CR37">37</xref></sup> developed a new wrapper feature selection method based on the Markov blanket, which allows working out-of-the-box for both classification and regression tasks on hybrid data. Rao et al.<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> advanced this paradigm via a relief algorithm, maximal information coefficient, and quantitative evaluation for multidimensional feature assessment. Comparative analyses revealed optimal combinations: Chen et al.<sup><xref ref-type="bibr" rid="CR39">39</xref></sup> reported that PCA-GA integration achieved superior accuracy and feature parsimony across medical data types. Similarly, Mohd Faizal et al.<sup><xref ref-type="bibr" rid="CR40">40</xref></sup> successfully identified AMI biomarkers through Pearson correlation-RFE-RF integration, demonstrating the efficacy of combined filter-wrapper-embedded approaches.</p><p id="Par13">
<table-wrap id="Tab1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Feature selection methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">References</th><th align="left" colspan="1" rowspan="1">Filter</th><th align="left" colspan="1" rowspan="1">Wrapper</th><th align="left" colspan="1" rowspan="1">Embedded</th><th align="left" colspan="1" rowspan="1">Other methods</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Ghasemi and Lee<sup><xref ref-type="bibr" rid="CR23">23</xref></sup></td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">Majdi et al.<sup><xref ref-type="bibr" rid="CR24">24</xref></sup></td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">Thabtah et al.<sup><xref ref-type="bibr" rid="CR25">25</xref></sup></td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">Hassan et al.<sup><xref ref-type="bibr" rid="CR26">26</xref></sup></td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">XGBoost</td></tr><tr><td align="left" colspan="1" rowspan="1">Yang et al.<sup><xref ref-type="bibr" rid="CR27">27</xref></sup></td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">L1-CBFS</td></tr><tr><td align="left" colspan="1" rowspan="1">Shen et al.<sup><xref ref-type="bibr" rid="CR28">28</xref></sup></td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">RFE</td></tr><tr><td align="left" colspan="1" rowspan="1">Wang et al.<sup><xref ref-type="bibr" rid="CR29">29</xref></sup></td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">LASSO</td></tr><tr><td align="left" colspan="1" rowspan="1">Ye et al.<sup><xref ref-type="bibr" rid="CR30">30</xref></sup></td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">Boruta algorithm</td></tr><tr><td align="left" colspan="1" rowspan="1">Chang et al.<sup><xref ref-type="bibr" rid="CR32">32</xref></sup></td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">MSGWO</td></tr><tr><td align="left" colspan="1" rowspan="1">Mamatha &amp;Terdal<sup><xref ref-type="bibr" rid="CR33">33</xref></sup></td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">&#8211;</td><td align="left" colspan="1" rowspan="1">ROBWGWA</td></tr><tr><td align="left" colspan="1" rowspan="1">Liu et al.<sup><xref ref-type="bibr" rid="CR19">19</xref></sup></td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">Rao et al.<sup><xref ref-type="bibr" rid="CR35">35</xref></sup></td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">Belouch et al.<sup><xref ref-type="bibr" rid="CR36">36</xref></sup></td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">Hassan et al.<sup><xref ref-type="bibr" rid="CR37">37</xref></sup></td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">Rao et al.<sup><xref ref-type="bibr" rid="CR38">38</xref></sup></td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1"/></tr><tr><td align="left" colspan="1" rowspan="1">Chen et al.<sup><xref ref-type="bibr" rid="CR39">39</xref></sup></td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">Mohd Faizal et al.<sup><xref ref-type="bibr" rid="CR40">40</xref></sup></td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8730;</td><td align="left" colspan="1" rowspan="1">&#8211;</td></tr></tbody></table></table-wrap>
</p><p id="Par14">Currently, many studies on feature selection adopt the single-format approach. While straightforward, these methods focus narrowly on one perspective, ignoring multidimensional feature impacts. These methods are susceptible to the feature preference issue. This means that they tend to favor features that are more prominent from that particular perspective while keeping fewer other features capable of accurate classification. In contrast to the single feature selection method, the existing hybrid feature selection method offers a more comprehensive outlook. Despite elaborate procedures, it mitigates feature preference and achieves higher classification/prediction accuracy with retained features.</p><p id="Par15">In existing research, a combination of filtering and wrapping methods has been considered. To minimize feature preference as much as possible, three aspects&#8212;relevance, information richness, and distance&#8212;are selected in the filtering stage to gauge the correlation between features and categories from diverse perspectives. Filtering weakly correlated features reduces the subsequent wrapping time, improving the overall efficiency and enhancing the risk assessment accuracy.</p></sec><sec id="Sec3"><title>Data collection and data preprocessing</title><p id="Par16">The goal is to study the problem of CAD risk assessment, identify important influencing factors, and reduce the rate of sudden-onset CAD events in patients. This section provides the data analysis and data preprocessing for the collected CAD dataset from the UCI database platform. First, useless and redundant features were deleted, and some classification variables were numerated. In the process of numerical transformation, the index values of the dichotomous classification were replaced with 0 and 1, and the continuous variable values were replaced with multiclassification results according to the standard range. Second, the data distributions of some of the remaining features and the relationships between them and the categories were analyzed statistically. Finally, the Smote algorithm was used to transform this unbalanced dataset to obtain balanced samples.</p><sec id="Sec4"><title>Data sources</title><p id="Par17">The dataset selected for this paper is a publicly available CAD (risk factor) dataset from the UCI machine learning repository at <ext-link ext-link-type="uri" xlink:href="http://archive.ics.uci.edu/ml/datasets/Heart+Disease">http://archive.ics.uci.edu/ml/datasets/Heart+Disease</ext-link>. This publicly available dataset is a classic CAD dataset with high data dimensionality and very complete information. Each feature field is free of missing values and outliers, which facilitates data preprocessing. Moreover, the value of the &#8220;Cath&#8221; index for each sample in this dataset is clear and presented in the form of a binary classification. If the patient is healthy, the value is &#8220;Normal&#8221;, and if the patient has CAD, the value of the &#8220;Cath&#8221; index is &#8220;Cad&#8221;, which is convenient for constructing a training model to make a risk dichotomy.</p><p id="Par18">The dataset contains a total of 303 subject health records with 55 indices (54 independent variables&#8201;+&#8201;1 dependent variable). Among them, 54 information indices, called the independent variables, were used to assess whether the tested subjects suffered from CAD. These variables mainly reflect the physiological indices (age, weight, height, sex, etc.), living habits (smoking, etc.) and various medical indices (BMI, BP, LDL, etc.) of the tested person, whereas the last index, &#8220;Cath&#8221;, is used to indicate whether the tested person is a patient with CAD, which is called the dependent variable. This index has two values, and the test subjects are divided into two categories, one marked with the text &#8220;Normal&#8221;, indicating that the test subject does not have CAD. The other is marked with the text &#8220;Cad&#8221;, indicating that the test subject is a CAD patient. In this dataset, there are 216 records of physical indices of patients with CAD and 87 records of those without CAD. There were no missing values for any of the indices in the whole dataset.</p><p id="Par19">The demographic and clinical characteristics of the UCI CAD dataset are summarized as follows. The cohort comprised 198 males (65.3%) and 105 females (34.7%), with a mean age of 54.2&#8201;&#177;&#8201;8.7 years. Significant differences were observed between CAD and non-CAD groups in key risk factors: hypertension prevalence was 38.9% vs. 18.4% (<italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.001), diabetes 26.4% vs. 9.2% (<italic toggle="yes">p</italic>&#8201;=&#8201;0.002), and dyslipidemia 47.2% vs. 24.1% (<italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.001). This highlights the dataset&#8217;s representation of established CAD risk profiles.</p></sec><sec id="Sec5"><title>Preprocessing of data</title><p id="Par20">This CAD dataset contains three types of data: character, floating point, and integer data. Among them, 21 indices belong to the character type, 4 indices belong to the floating point type, and the remaining indices are all integer type data. Thus, these CAD data are unstructured. Since it is inconvenient to use character data directly for calculation and analysis, it is necessary to preprocess the data before feature selection.</p><p id="Par21">The results obtained by directly using the collected dirty data for model construction or data analysis often have large errors. Therefore, to ensure the accuracy of the model and the quality of the analysis process, data preprocessing is essential. Because of the characteristics of the CAD dataset used in this paper, it is necessary to delete the useless features and numericalize the classification variables before modeling.<list list-type="order"><list-item><p id="Par22">Delete useless features</p></list-item></list></p><p id="Par23">There are 55 features in the CAD dataset. In the collation, we find that the value of &#8220;Exertional CP&#8221; is &#8220;N&#8221;, so we delete &#8220;Exertional CP&#8221; as a useless feature. Among the 303 values of the features &#8220;CHF&#8221; and &#8220;LowTH Ang&#8221;, only 1 and 2 values of &#8220;Y&#8221;, respectively, were found. Considering the small contribution of this feature to the dependent variable, &#8220;CHF&#8221; and &#8220;LowTH Ang&#8221; were also deleted as useless features. In addition, since body mass index (BMI)&#8201;=&#8201;weight/height<sup>2</sup>, weight and height are redundant features that should be eliminated. After these five features were removed, 18 character-type features, 4 floating-point features, and 28 integer features remained.<list list-type="simple"><list-item><label>(2)</label><p id="Par24">Numeralization of classification variables.</p></list-item></list></p><p id="Par25">Since some features in the feature set still take continuous values after removing the invalid features, which is inconvenient for performing feature selection, any continuous type data need to be transformed, that is, discretized and then modeled. Arabic numerals are used to replace character type features to represent different categories of disease. The transformation of the continuous index data is shown in Table&#160;<xref rid="Tab2" ref-type="table">2</xref>.</p><p id="Par26">
<table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Quantification features.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Variable name</th><th align="left" colspan="1" rowspan="1">Description</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Age</td><td align="left" colspan="1" rowspan="1"><p>Takes values 0&#8211;1. 0: male&#8201;&lt;&#8201;45, female&#8201;&lt;&#8201;55</p><p>1: male&#8201;&#8805;&#8201;45, female&#8201;&#8805;&#8201;55</p></td></tr><tr><td align="left" colspan="1" rowspan="1">Sex</td><td align="left" colspan="1" rowspan="1">Male is recorded as 1, female is recorded as 0</td></tr><tr><td align="left" colspan="1" rowspan="1">BMI</td><td align="left" colspan="1" rowspan="1"><p>Takes values 0&#8211;3. 0: 18.5&#8211;23.9, 1: &lt;18.5</p><p>2: 23.9&#8201;&lt;&#8201;BMI&#8201;&#8804;&#8201;27.9, 3: &gt;27.9</p></td></tr><tr><td align="left" colspan="1" rowspan="1">obesity</td><td align="left" colspan="1" rowspan="1">Obesity is recorded as 1, normal is recorded as 0</td></tr><tr><td align="left" colspan="1" rowspan="1">CRF</td><td align="left" colspan="1" rowspan="1">N is recorded as 0 and Y is recorded as 1</td></tr><tr><td align="left" colspan="1" rowspan="1">CVA</td><td align="left" colspan="1" rowspan="1">N is recorded as 0 and Y is recorded as 1</td></tr><tr><td align="left" colspan="1" rowspan="1">Airway Disease</td><td align="left" colspan="1" rowspan="1">N is recorded as 0 and Y is recorded as 1</td></tr><tr><td align="left" colspan="1" rowspan="1">Thyroid Disease</td><td align="left" colspan="1" rowspan="1">N is recorded as 0 and Y is recorded as 1</td></tr><tr><td align="left" colspan="1" rowspan="1">DLP</td><td align="left" colspan="1" rowspan="1">N is recorded as 0 and Y is recorded as 1</td></tr><tr><td align="left" colspan="1" rowspan="1">BP</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;2. 0: 90&#8211;129,1: 129&#8201;&lt;&#8201;BP&#8201;&lt;&#8201;139, 2: &gt;139</td></tr><tr><td align="left" colspan="1" rowspan="1">PR</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0: 60&#8211;100, 1: &lt;60 or &gt;&#8201;100</td></tr><tr><td align="left" colspan="1" rowspan="1">Weak Peripheral Pulse</td><td align="left" colspan="1" rowspan="1">N is recorded as 0 and Y is recorded as 1</td></tr><tr><td align="left" colspan="1" rowspan="1">Lung Rales</td><td align="left" colspan="1" rowspan="1">N is recorded as 0 and Y is recorded as 1</td></tr><tr><td align="left" colspan="1" rowspan="1">Systolic Murmur</td><td align="left" colspan="1" rowspan="1">N is recorded as 0 and Y is recorded as 1</td></tr><tr><td align="left" colspan="1" rowspan="1">Diastolic Murmur</td><td align="left" colspan="1" rowspan="1">N is recorded as 0 and Y is recorded as 1</td></tr><tr><td align="left" colspan="1" rowspan="1">Dyspnea</td><td align="left" colspan="1" rowspan="1">N is recorded as 0 and Y is recorded as 1</td></tr><tr><td align="left" colspan="1" rowspan="1">Atypical</td><td align="left" colspan="1" rowspan="1">N is recorded as 0 and Y is recorded as 1</td></tr><tr><td align="left" colspan="1" rowspan="1">Nonanginal</td><td align="left" colspan="1" rowspan="1">N is recorded as 0 and Y is recorded as 1</td></tr><tr><td align="left" colspan="1" rowspan="1">LVH</td><td align="left" colspan="1" rowspan="1">N is recorded as 0 and Y is recorded as 1</td></tr><tr><td align="left" colspan="1" rowspan="1">Poor R Progression</td><td align="left" colspan="1" rowspan="1">N is recorded as 0 and Y is recorded as 1</td></tr><tr><td align="left" colspan="1" rowspan="1">BBB</td><td align="left" colspan="1" rowspan="1">Takes values 1&#8211;3. 1: N, 2: LBBB, 3: RBBB</td></tr><tr><td align="left" colspan="1" rowspan="1">FBS</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0: 65&#8211;109, 1: &lt;65 or &gt;&#8201;109</td></tr><tr><td align="left" colspan="1" rowspan="1">CR</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0: 0.6&#8211;1.2 for males, 0.5&#8211;1.1 for females,1: male&#8201;&lt;&#8201;0.6 or &gt;&#8201;1.2, female&#8201;&lt;&#8201;0.5 or &gt;&#8201;1.1</td></tr><tr><td align="left" colspan="1" rowspan="1">TG</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0: &lt;150, 1: &#8805;150</td></tr><tr><td align="left" colspan="1" rowspan="1">LDL</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0: &lt;120, 1: &#8805;120</td></tr><tr><td align="left" colspan="1" rowspan="1">HDL</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0: male 30&#8211;59, female 33&#8211;77,1: male&#8201;&lt;&#8201;30 or &gt;&#8201;59, female&#8201;&lt;&#8201;33 or &gt;&#8201;77</td></tr><tr><td align="left" colspan="1" rowspan="1">BUN</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0: 9&#8211;20, 1: &lt;9 or &gt;&#8201;20</td></tr><tr><td align="left" colspan="1" rowspan="1">ESR</td><td align="left" colspan="1" rowspan="1"><p>Takes values 0&#8211;1. 0: male 0&#8211;15, female 0&#8211;20</p><p>1: male&#8201;&gt;&#8201;15, female&#8201;&gt;&#8201;20</p></td></tr><tr><td align="left" colspan="1" rowspan="1">HB</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0: 12.0-16.5 for males, 11.0&#8211;15.0 for females,1: male&#8201;&lt;&#8201;12.0 or &gt;&#8201;16.5, female&#8201;&lt;&#8201;11.0 or &gt;&#8201;15</td></tr><tr><td align="left" colspan="1" rowspan="1">K</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0: 3.5&#8211;5.5, 1: &lt;3.5 or &gt;&#8201;5.5</td></tr><tr><td align="left" colspan="1" rowspan="1">Na</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0: 135&#8211;145, 1: &lt;135 or &gt;&#8201;145</td></tr><tr><td align="left" colspan="1" rowspan="1">WBC</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0: 4000&#8211;10,000, 1: &lt;4000 or &gt;&#8201;10,000</td></tr><tr><td align="left" colspan="1" rowspan="1">Lymph</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0: 20&#8211;40, 1: &lt;20 or &gt;&#8201;40</td></tr><tr><td align="left" colspan="1" rowspan="1">Neut</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0:50&#8211;70, 1: &lt;50 or &gt;&#8201;70</td></tr><tr><td align="left" colspan="1" rowspan="1">PLT</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0: 100&#8211;300, 1: &lt;100 or &gt;&#8201;300</td></tr><tr><td align="left" colspan="1" rowspan="1">EF-TTE</td><td align="left" colspan="1" rowspan="1">Takes values 0&#8211;1. 0: &#8805;50, 1: &lt;50</td></tr><tr><td align="left" colspan="1" rowspan="1">VHD</td><td align="left" colspan="1" rowspan="1">Takes values 1&#8211;4. 1: N, 2: Mild,3: Moderate, 4: Severe</td></tr></tbody></table></table-wrap>
</p></sec><sec id="Sec6"><title>Transformation of unbalanced datasets</title><p id="Par27">On the basis of the aforementioned preprocessing of the CAD datasets in Section&#160;&#8220;<xref rid="Sec5" ref-type="sec">Preprocessing of data</xref>&#8221;, the unbalanced datasets are balanced in this section. Common solutions for unbalanced data include oversampling, undersampling and classifier improvement. Considering that the Smote algorithm is a typical oversampling method that can increase the number of class samples and thus improve the overall prediction accuracy<sup><xref ref-type="bibr" rid="CR41">41</xref></sup>before constructing the model, this section transforms the unbalanced CAD dataset into a balanced dataset via the Smote algorithm.</p><p id="Par28">The full name of the Smote algorithm is the synthetic minority oversampling technique<sup><xref ref-type="bibr" rid="CR42">42</xref>,<xref ref-type="bibr" rid="CR43">43</xref></sup>. This technique increases the number of minority class samples by artificially synthesizing new samples to balance the data at a ratio of 1:1. The risk assessment of the CAD system studied in this paper is a binary problem. There are 303 sample records in the CAD dataset, 216 of which are index information of sick people and 87 of which are data information of healthy people. Therefore, the population without CAD belongs to the minority class sample in the CAD dataset, whereas the population with the disease belongs to the majority class group. The steps for unbalanced processing of the CAD dataset via Smote are as follows.<list list-type="order"><list-item><p id="Par29">Calculate the Euclidean distance between each individual <italic toggle="yes">x</italic> in the sample set without CAD and all other individuals in its corresponding category and then obtain <italic toggle="yes">x</italic>&#8217;s <italic toggle="yes">k</italic> nearest neighbors.</p></list-item><list-item><p id="Par30">The sampling multiplier <italic toggle="yes">N</italic> is specified according to the ratio of the two types of samples in the CAD sample set, and for each individual <inline-formula id="IEq1"><alternatives><tex-math id="d33e989">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x_{old}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq1.gif"/></alternatives></inline-formula> in the sample set without CAD, a number of individuals are selected from their <italic toggle="yes">k</italic> nearest neighbors. Now, suppose that the selected sample is <inline-formula id="IEq2"><alternatives><tex-math id="d33e998">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x_i}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq2.gif"/></alternatives></inline-formula>.</p></list-item><list-item><p id="Par31">For any <inline-formula id="IEq3"><alternatives><tex-math id="d33e1009">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x_i}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq2.gif"/></alternatives></inline-formula>, a new sample <inline-formula id="IEq4"><alternatives><tex-math id="d33e1015">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x_{new}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq4.gif"/></alternatives></inline-formula> is synthesized according to Eq.&#160;(<xref rid="Equ1" ref-type="disp-formula">1</xref>) on the basis of the old sample <inline-formula id="IEq5"><alternatives><tex-math id="d33e1024">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x_{old}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq1.gif"/></alternatives></inline-formula>.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e1030">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x_{new}}={x_{old}}+rand(0,1) \times ({x_{old}} - {x_i})$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18024_Article_Equ1.gif"/></alternatives></disp-formula></p></list-item><list-item><p id="Par32">Finally, a number of samples were randomly removed from the newly generated sample set of healthy people so that the number of samples in the healthy sample set and the sick sample set were equal to ensure the balance of the whole dataset.</p></list-item></list></p><p id="Par33">In this work, the Smote algorithm was used to process the CAD dataset so that the ratio of diseased to healthy populations reached a 1:1 balance, which allowed for subsequent index selection and model construction.</p></sec></sec><sec id="Sec7"><title>A risk factor identification mechanism based on multiple cross-filtering and BCS searches</title><p id="Par34">This section identifies important risk factors for CAD based on the risk feature set constructed in Section&#160;&#8220;<xref rid="Sec3" ref-type="sec">Data collection and data preprocessing</xref>&#8221;, i.e., feature selection. A single feature selection method has different drawbacks. To solve the problems of low classification accuracy caused by the filtering method and low generality caused by the wrapping method, this section integrates the advantages of filtering and wrapping and uses both methods to propose a risk factor identification mechanism based on multiple cross-filtering and BCS. Unlike traditional methods, this study integrates statistical analysis and machine learning to evaluate features from three perspectives and uses classification accuracy to identify optimal subsets with high accuracy and generalizability. In the multiple cross-filtering stage, first, the features were screened from the relevance perspective via the chi-square test based on the threshold value to determine the number of features that passed the test; then, the feature importance was calculated from both the information richness and distance perspectives, the same number of features were retained, and the features that were initially screened from all three perspectives were filtered out. In the wrapper stage, the optimal subset of features was retained via a search algorithm with classification accuracy as the judgment criterion for the remaining features to establish a CAD risk assessment index system.</p><sec id="Sec8"><title>Feature selection method for selecting CAD risk factors</title><p id="Par35">Feature selection is a common method used when dimensionality reduction is performed on data. Numerous indices are triggers for CAD, and they often influence and affect the relationships between indices. These correlations indirectly affect the classification accuracy of CAD risk assessment models, and a lower classification accuracy results in a greater cost of misclassification. Therefore, choosing appropriate feature selection methods, retaining highly recognizable features, and reducing the redundancy of feature sets are crucial to reduce the misclassification cost and improve the classification accuracy of samples.</p><p id="Par36">Feature selection methods can be classified into three types, namely, filtering, wrapping, and embedding methods, depending on the evaluation criteria used in the filtering feature subsets. In this paper, we discuss only the filtering and wrapping methods: the filtering method does not depend on specific learning algorithms, and the evaluation criterion is to select features or subsets of features with strong relevance according to the inherent properties of the dataset itself. Although it is easy to ignore the correlation between features and the selected subset has lower classification accuracy than the wrapping method does, it has strong generality and can quickly eliminate many irrelevant features. The wrapping method is inseparable from the classifier, which needs to evaluate the performance of the learning algorithm to evaluate the feature set and select the appropriate features. This method is computationally complex and not very versatile, but the subset of features it finds often has better classification results.</p><p id="Par37">According to the CAD data, this paper fully considers the advantages and disadvantages of the filtering and wrapping methods. First, the filtering method was used to remove features that do not play an important role in the category in terms of distance, relevance or information richness for the purpose of primary screening of the features. The optimal subset of features was subsequently screened according to the wrapping method. The overall flowchart of the feature selection method based on multiple cross-filtering and BCS is given in Fig.&#160;<xref rid="Fig2" ref-type="fig">2</xref>.</p><p id="Par38">
<fig id="Fig2" position="float" orientation="portrait"><label>Fig. 2</label><caption><p>Flow chart of the feature selection method based on multiple cross-filtering and the BCS.</p></caption><graphic id="d33e1069" position="float" orientation="portrait" xlink:href="41598_2025_18024_Fig2_HTML.jpg"/></fig>
</p><p id="Par39">Figure&#160;<xref rid="Fig2" ref-type="fig">2</xref> shows the two-stage modeling process for the feature selection method, which is based on multiple cross-filtering and the BCS. In the first filter stage of multiple cross-filtering, the features that might be deleted are first determined via the chi-square test from the relevance perspective according to the threshold value, and the number of deleted features is recorded. The feature importance is subsequently calculated from two other perspectives (Fisher&#8217;s score and the mutual information method) to determine the same number of features that may have to be removed. The features contained in the intersection of the three sets are eliminated to reduce the classification accuracy error caused by feature preference. In the wrapper stage, a random forest serves as the classifier, with classification accuracy as the evaluation metric. The BCS algorithm rescreens feature subsets (incorporating the classifier into the evaluation function) to optimize CAD risk assessment performance.</p><sec id="Sec9"><title>Multiple cross-filtering</title><p id="Par40">Many existing studies of feature selection rely on a single method, such as the correlation coefficient, mutual information, Fisher score, or chi-square test. This approach overemphasizes single-aspect feature importance, ignores other perspectives, and may cause feature preference&#8212;favoring some features while missing others critical for accurate classification. In the filtering stage, this paper abandons the traditional single-form filtering method. Instead, it assesses the importance of features in CAD&#8217;s initial feature set from three aspects: correlation, information richness, and distance. Using the chi-square test (for correlations between independent and dependent variables with a significance level&#8201;&lt;&#8201;0.05), the mutual information method (evaluating factors&#8217; impact on reducing dependent variable uncertainty), and the Fisher score (measuring features&#8217; ability to separate sample types), three feature subsets with lower importance are obtained. The features that crossappeared in these subsets were initially screened. Consequently, this paper constructs multiple cross-filtering mechanisms that integrate these three methods.</p><p id="Par41">In CAD risk factor identification scenarios, traditional feature selection methods, including the chi-square test, mutual information, and Fisher score, demonstrate unique advantages when rationally combined. The chi-square test effectively measures the correlation between discrete features and disease categories. Mutual information evaluates features&#8217; ability to reduce uncertainty in disease classification from an information theory perspective. The Fisher score excels at distinguishing feature differences across sample categories. Compared with other advanced or domain-specific techniques, these conventional methods, through appropriate integration, can more efficiently perform initial screening of critical features while maintaining interpretability and are particularly suitable for preliminary exploration of clinical datasets with limited samples.</p><p id="Par42">The three perspectives used for feature evaluation are formally defined in Table&#160;<xref rid="Tab3" ref-type="table">3</xref>.</p><p id="Par43">
<table-wrap id="Tab3" position="float" orientation="portrait"><label>Table 3</label><caption><p>The three perspectives used for feature evaluation.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Perspective</th><th align="left" colspan="1" rowspan="1">Metric Used</th><th align="left" colspan="1" rowspan="1">Definition</th><th align="left" colspan="1" rowspan="1">Mathematical Form</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Relevance</td><td align="left" colspan="1" rowspan="1">Chi-square test</td><td align="left" colspan="1" rowspan="1">Measures statistical dependence between feature and target variable</td><td align="left" colspan="1" rowspan="1">Equation&#160;(<xref rid="Equ2" ref-type="disp-formula">2</xref>)</td></tr><tr><td align="left" colspan="1" rowspan="1">Information richness</td><td align="left" colspan="1" rowspan="1">Mutual information</td><td align="left" colspan="1" rowspan="1">Quantifies reduction in target uncertainty given the feature</td><td align="left" colspan="1" rowspan="1">Equation&#160;(<xref rid="Equ5" ref-type="disp-formula">5</xref>)</td></tr><tr><td align="left" colspan="1" rowspan="1">Distance</td><td align="left" colspan="1" rowspan="1">Fisher score</td><td align="left" colspan="1" rowspan="1">Evaluates inter-class separation vs. intra-class compactness</td><td align="left" colspan="1" rowspan="1">Equation&#160;(<xref rid="Equ9" ref-type="disp-formula">9</xref>)</td></tr></tbody></table></table-wrap>
<list list-type="order"><list-item><p id="Par44">Chi-square test</p></list-item></list>
</p><p id="Par45">The chi-square test<sup><xref ref-type="bibr" rid="CR44">44</xref></sup> is commonly used for correlation analysis of two categorical variables and comparisons of multiple composition ratios. The main idea is to calculate the accuracy of the theoretical value according to the deviation between the theoretical value and the observed value, that is, to test the degree of fit between the theoretical frequency and the actual frequency<sup><xref ref-type="bibr" rid="CR45">45</xref></sup>. For CAD datasets, the essence is to use statistical theory to determine the correlation between the CAD risk factors and the disease categories. The larger the cardinality value is, the greater the contribution of CAD risk factors to the disease category; namely, when performing feature selection, features with higher cardinality values are preferentially selected.</p><p id="Par46">For the influencing factor <italic toggle="yes">x</italic> of CAD, the chi-square value of <italic toggle="yes">x</italic> and category <italic toggle="yes">y</italic> is calculated as follows:<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="d33e1178">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$chi(x,y)=\sum\limits_{{i=1}}^{n} {\frac{{{{({x_{obs}} - {x_{\exp }})}^2}}}{{{x_{\exp }}}}},$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18024_Article_Equ2.gif"/></alternatives></disp-formula></p><p id="Par47">where <inline-formula id="IEq6"><alternatives><tex-math id="d33e1186">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x_{obs}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq6.gif"/></alternatives></inline-formula> is the observed value of the CAD influencing factor, <inline-formula id="IEq7"><alternatives><tex-math id="d33e1192">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x_{\exp }}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq7.gif"/></alternatives></inline-formula> is the expected value, and <italic toggle="yes">n</italic> is the sample size. where <italic toggle="yes">chi</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) is the chi-square value, which indicates the correlation between the influencing factor and the category.</p><p id="Par48">The chi-square test process is as follows: first, assume that the two variables are independent of each other, and when the hypothesis holds, calculate the theoretical frequency of each cell in the variable column table. Then, the actual frequency between the two variables is calculated, and the differences between the actual and theoretical frequencies are compared. This difference is expressed as a chi-square value test; namely, if the chi-square test result is less than the significance level, the original hypothesis is rejected, indicating that the two variables are correlated. If the chi-square test result is greater than the significance level, the original hypothesis is accepted, indicating that the two variables are independent of each other and are not correlated. For discrete data, the chi-square test can directly test whether two feature variables are correlated, whereas for continuous data, it needs to be transformed into discrete data first, and a frequency series table needs to be established before conducting the chi-square test.<list list-type="simple"><list-item><label>(2)</label><p id="Par49">Mutual information method.</p></list-item></list></p><p id="Par50">Mutual information (MI)<sup><xref ref-type="bibr" rid="CR46">46</xref></sup> can be regarded as the amount of information of another risk factor contained in a random risk factor for CAD, and the mutual information method was selected in this paper to measure the influence of features on categories in terms of information richness.</p><p id="Par51">According to the chain rule of entropy, we have<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="d33e1227">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H\left( {X,Y} \right)=H\left( X \right)+H\left( {Y|X} \right)=H\left( Y \right)+H\left( {X|Y} \right),$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18024_Article_Equ3.gif"/></alternatives></disp-formula></p><p id="Par52">where <italic toggle="yes">H</italic>(<italic toggle="yes">X</italic>) and <italic toggle="yes">H</italic>(<italic toggle="yes">Y</italic>) are the information entropies of <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic>, respectively; <inline-formula id="IEq8"><alternatives><tex-math id="d33e1255">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H\left( {X,Y} \right)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq8.gif"/></alternatives></inline-formula> is the joint entropy of <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic>; <italic toggle="yes">H</italic>(<italic toggle="yes">X</italic>|<italic toggle="yes">Y</italic>) is the entropy of <italic toggle="yes">X</italic> given <italic toggle="yes">Y</italic>; and <italic toggle="yes">H</italic>(<italic toggle="yes">Y</italic>|<italic toggle="yes">X</italic>) is the entropy of <italic toggle="yes">Y</italic> given <italic toggle="yes">X</italic>. From Eq.&#160;(<xref rid="Equ3" ref-type="disp-formula">3</xref>), we obtain:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="d33e1302">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$H\left( X \right) - H\left( {X|Y} \right)=H\left( Y \right) - H\left( {Y|X} \right).$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18024_Article_Equ4.gif"/></alternatives></disp-formula></p><p id="Par53">This difference is the mutual information of <italic toggle="yes">X</italic> and <italic toggle="yes">Y</italic>, which is denoted as <italic toggle="yes">I</italic>(<italic toggle="yes">X</italic>; <italic toggle="yes">Y</italic>).</p><p id="Par54">According to the definition of entropy, this yields:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="d33e1327">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\begin{aligned} I\left( {X;Y} \right) &amp; = H\left( X \right) - H\left( {X|Y} \right) \\ &amp; = H\left( X \right) + H\left( Y \right) - H\left( {X,Y} \right) \\ &amp; = \sum\limits_{x} {p(x)\log \frac{1}{{p(x)}}} + \sum\limits_{y} {p(y)\log \frac{1}{{p(y)}}} - \sum\limits_{{x,y}} {p(x,y)\log \frac{1}{{p(x,y)}}} \\ &amp; = \sum\limits_{{x,y}} {p(x,y)\log \frac{{p(x,y)}}{{p(x)p(y)}}} . \\ \end{aligned}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18024_Article_Equ5.gif"/></alternatives></disp-formula></p><p id="Par55">where <italic toggle="yes">p</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) is the joint distribution of two random variables (<italic toggle="yes">X</italic>, <italic toggle="yes">Y</italic>); <italic toggle="yes">p</italic>(<italic toggle="yes">x</italic>) and <italic toggle="yes">p</italic>(<italic toggle="yes">y</italic>) are the marginal distributions of two random variables (<italic toggle="yes">X</italic>, <italic toggle="yes">Y</italic>); and the mutual information <italic toggle="yes">I</italic>(<italic toggle="yes">X</italic>; <italic toggle="yes">Y</italic>) is the relative entropy of the joint distribution <italic toggle="yes">p</italic>(<italic toggle="yes">x</italic>, <italic toggle="yes">y</italic>) and the marginal distributions <italic toggle="yes">p</italic>(<italic toggle="yes">x</italic>) and <italic toggle="yes">p</italic>(<italic toggle="yes">y</italic>).<list list-type="simple"><list-item><label>(3)</label><p id="Par56">Fisher score</p></list-item></list></p><p id="Par57">The Fisher score<sup><xref ref-type="bibr" rid="CR47">47</xref></sup> is a classical filtering method that is commonly used for feature selection. The Fisher score is obtained by calculating the ratio according to Fisher&#8217;s criterion, and the features with strong discriminative performance exhibit the largest possible interclass distance and the smallest possible intraclass distance. The larger the Fisher score corresponding to a feature is, the stronger the ability of the feature to distinguish between categories.</p><p id="Par58">In this work, a 3:7 ratio was used to divide the test sets and training sets; namely, 212 CAD sample data points were used to carry out the study analysis for the selection of CAD risk factors. The training set samples are denoted as <inline-formula id="IEq9"><alternatives><tex-math id="d33e1415">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left\{ {\left( {{x_1},{y_1}} \right),\left( {{x_2},{y_2}} \right), \ldots ,\left( {{x_i},{y_i}} \right)} \right\}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq9.gif"/></alternatives></inline-formula>, where <inline-formula id="IEq10"><alternatives><tex-math id="d33e1421">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left( {{x_i},{y_i}} \right)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq10.gif"/></alternatives></inline-formula> represents the values of the risk factors and risk categories, <inline-formula id="IEq11"><alternatives><tex-math id="d33e1427">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${x_i} \in {R^n},i=1,2, \ldots ,212$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq11.gif"/></alternatives></inline-formula>, <inline-formula id="IEq12"><alternatives><tex-math id="d33e1433">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${y_i}=\left\{ {0,1} \right\}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq12.gif"/></alternatives></inline-formula> is the category, 0 represents the healthy samples, and 1 represents the diseased samples. Denote the Fisher score by <inline-formula id="IEq13"><alternatives><tex-math id="d33e1439">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F_k}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq13.gif"/></alternatives></inline-formula>; then,<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="d33e1446">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${F_k}=\frac{{{S_b}}}{{{S_w}}}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18024_Article_Equ6.gif"/></alternatives></disp-formula></p><p id="Par59">where <inline-formula id="IEq14"><alternatives><tex-math id="d33e1454">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S_b}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq14.gif"/></alternatives></inline-formula> denotes the distance between samples of the two categories of diseased and healthy, i.e., interclass dispersion, and where <inline-formula id="IEq15"><alternatives><tex-math id="d33e1460">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S_w}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq15.gif"/></alternatives></inline-formula> denotes the dispersion between samples of the same category, i.e., intraclass dispersion. The calculation equations are given by Eqs.&#160;(<xref rid="Equ7" ref-type="disp-formula">7</xref>) and (<xref rid="Equ8" ref-type="disp-formula">8</xref>):<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="d33e1472">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S_b}{\text{= }}{\left( {\overline {{{m_1}}} - \overline {m} } \right)^2}+{\left( {\overline {{{m_2}}} - \overline {m} } \right)^2},$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18024_Article_Equ7.gif"/></alternatives></disp-formula><disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="d33e1478">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${S_w}=\frac{1}{{{p_1}}}{\sum\limits_{{x \in {X_1}}} {\left( {x - \overline {{{m_1}}} } \right)} ^2}+\frac{1}{{{p_2}}}{\sum\limits_{{x \in {X_2}}} {\left( {x - \overline {{{m_2}}} } \right)} ^2}={\delta _1}^{2}+{\delta _2}^{2},$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18024_Article_Equ8.gif"/></alternatives></disp-formula></p><p id="Par60">where <inline-formula id="IEq16"><alternatives><tex-math id="d33e1486">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p_1}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq16.gif"/></alternatives></inline-formula> represents the number of samples in sample set <inline-formula id="IEq17"><alternatives><tex-math id="d33e1492">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${X_1}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq17.gif"/></alternatives></inline-formula> with CAD, <inline-formula id="IEq18"><alternatives><tex-math id="d33e1498">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${p_2}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq18.gif"/></alternatives></inline-formula> represents the number of samples in sample set <inline-formula id="IEq19"><alternatives><tex-math id="d33e1504">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${X_2}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq19.gif"/></alternatives></inline-formula> without disease, and <inline-formula id="IEq20"><alternatives><tex-math id="d33e1510">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overline {{{m_1}}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq20.gif"/></alternatives></inline-formula>,<inline-formula id="IEq21"><alternatives><tex-math id="d33e1517">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overline {{{m_2}}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq21.gif"/></alternatives></inline-formula> and <inline-formula id="IEq22"><alternatives><tex-math id="d33e1523">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overline {m}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq22.gif"/></alternatives></inline-formula> are the mean values of the samples with disease, without disease and all samples, respectively.</p><p id="Par61">The Fisher score for the <italic toggle="yes">k</italic>-th feature variable is represented by Eq.&#160;(<xref rid="Equ9" ref-type="disp-formula">9</xref>):<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="d33e1537">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F{S_k}=\frac{{{S_{b,k}}}}{{{S_{w,k}}}}=\frac{{{{(\overline {{{m_{1,k}}}} - \overline {{{m_k}}} )}^2}+{{(\overline {{{m_{2,k}}}} - \overline {{{m_k}}} )}^2}}}{{{\delta _{1,k}}^{2}+{\delta _{2,k}}^{2}}},$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18024_Article_Equ9.gif"/></alternatives></disp-formula></p><p id="Par62">where <inline-formula id="IEq23"><alternatives><tex-math id="d33e1545">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overline {{{m_{1,k}}}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq23.gif"/></alternatives></inline-formula>, <inline-formula id="IEq24"><alternatives><tex-math id="d33e1551">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overline {{{m_{2,k}}}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq24.gif"/></alternatives></inline-formula> and <inline-formula id="IEq25"><alternatives><tex-math id="d33e1557">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\overline {{{m_k}}}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq25.gif"/></alternatives></inline-formula> are the means of the <italic toggle="yes">k</italic>-th attribute for the diseased and healthy risk samples and dataset, respectively. <inline-formula id="IEq26"><alternatives><tex-math id="d33e1566">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\delta _{1,k}}^{2}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq26.gif"/></alternatives></inline-formula> and <inline-formula id="IEq27"><alternatives><tex-math id="d33e1573">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\delta _{2,k}}^{2}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq27.gif"/></alternatives></inline-formula> are the variances of the diseased and healthy risk samples for the <italic toggle="yes">k</italic>-th attribute, respectively.</p><p id="Par63">In the application of identifying the main risk factors (features) of CAD, all features are sorted on the basis of the calculated Fisher score, and a threshold is set to select the features with higher scores as the new feature subset.</p></sec><sec id="Sec10"><title>BCS algorithm</title><p id="Par64">To ensure that the CAD risk assessment feature subset&#8217;s noiselessness, this paper wrapped and further filtered the remaining features postinitial screening from three aspects. In the wrapping stage, the accuracy of the RF<sup><xref ref-type="bibr" rid="CR48">48</xref></sup> classification model served as the evaluation function. The feature set with the best prediction and strongest generalization ability was chosen as the input for the CAD risk assessment model. Initial screening in the filtering stage benefits from reducing features, achieving dimensionality reduction, reducing the search time and workload later, and ensuring that the final subset is optimal for given iterations. The wrapping method contrasts with the filtering method, as it uses the classification model&#8217;s performance, specifically the classification accuracy, to select the CAD risk factor subset. This paper proposes a new random forest for CAD risk assessment. Thus, the random forest is employed here to determine the CAD risk assessment feature set on the basis of its classification accuracy. Before the classification model is used, the search algorithm must be selected, with classification accuracy as the objective function, to find the optimal feature subset.</p><p id="Par65">The cuckoo search algorithm, inspired by cuckoo breeding and parasitism and proposed by Yang and Deb in 2010<sup><xref ref-type="bibr" rid="CR49">49</xref></sup>, uses the L&#233;vy Flight<sup><xref ref-type="bibr" rid="CR50">50</xref></sup> mechanism. It is increasingly used as a stochastic algorithm, and the discrete version is often used for feature selection. Therefore, in the wrapping phase, discrete CS is chosen for feature search, filtering the optimal subset by the random forest&#8217;s classification accuracy.</p><p id="Par66">The CS algorithm contains five parameters, that is, the probability <italic toggle="yes">P</italic> that the host bird finds an alien nest, the number of iterations <italic toggle="yes">c</italic>, the initial number of nests <italic toggle="yes">n</italic>, the number of features <italic toggle="yes">d</italic> (set to 15 in this paper) and the step size <italic toggle="yes">s</italic>. The probability <italic toggle="yes">P</italic> and the number of nests <italic toggle="yes">n</italic> determine the search speed of the algorithm. Typically, when <inline-formula id="IEq28"><alternatives><tex-math id="d33e1626">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$n \in [15,40]$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq28.gif"/></alternatives></inline-formula>, <italic toggle="yes">P</italic>&#8201;=&#8201;0.25 can be used for most problems. If the random probability is <inline-formula id="IEq29"><alternatives><tex-math id="d33e1635">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$r&gt;P$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq29.gif"/></alternatives></inline-formula>, then a new nest is rebuilt to replace the old one.</p><p id="Par67">The CS algorithm, typically for continuous problems, requires discrete transformation to select coronary heart disease risk factors. The binary cuckoo search (BCS) algorithm<sup><xref ref-type="bibr" rid="CR51">51</xref></sup> can handle continuous-to-discrete conversion. Derived from the CS algorithm, the BCS treats nests as 0&#8211;1 sequences: &#8220;0&#8221; means that a risk factor is not selected, and &#8220;1&#8221; indicates that it remains during the search. BCS has multiple advantages: (i) Strong global search via L&#233;vy Flight, allowing large jumps to avoid local optima. (iv) Fast convergence, outpacing other evolutionary algorithms. (ii) It is simple, with only two control parameters (population size and probability). (iii) It balances local search and diversity, using an adaptive competition mechanism to dynamically optimize the number of solutions for search speed and diversity. Thus, this paper employs the BCS for feature selection. The BSC initializes nests, records the random forest model&#8217;s classification accuracy (fitness value), checks if cuckoo eggs are discarded, and updates nests via L&#233;vy Flight. After iteration, the optimal CHD risk feature subset is obtained.</p><p id="Par68">Since the BCS algorithm is a transformation of the CS algorithm, the step update mechanism of the BCS algorithm cannot be the same as that of the CS algorithm during L&#233;vy flight. The L&#233;vy Flight transformation equation<sup><xref ref-type="bibr" rid="CR52">52</xref></sup> for the BCS algorithm are given in Eqs.&#160;(<xref rid="Equ10" ref-type="disp-formula">10</xref>) and (<xref rid="Equ11" ref-type="disp-formula">11</xref>):<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="d33e1659">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S\left( {x_{i}^{j}(t)} \right)=\frac{1}{{1+{e^{x_{i}^{j}(t)}}}},$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18024_Article_Equ10.gif"/></alternatives></disp-formula><disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="d33e1665">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{i}^{j}{\text{(}}t{\text{+1)=}}\left\{ {\begin{array}{*{20}{c}} {1,ifS\left( {x_{i}^{j}(t)} \right)&gt;\sigma, } \\ {0,otherwise,} \end{array}} \right.$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18024_Article_Equ11.gif"/></alternatives></disp-formula></p><p id="Par69">where <inline-formula id="IEq30"><alternatives><tex-math id="d33e1673">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\sigma \in U(0,1)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq30.gif"/></alternatives></inline-formula> and <inline-formula id="IEq31"><alternatives><tex-math id="d33e1679">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{i}^{j}(t)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq31.gif"/></alternatives></inline-formula> denote the <italic toggle="yes">j</italic>-th feature value of the <italic toggle="yes">i</italic>-th nest of the <italic toggle="yes">t</italic>-th iteration. The pseudocode of the BCS algorithm is shown in Table&#160;<xref rid="Tab4" ref-type="table">4</xref>.</p><p id="Par70">In Table&#160;<xref rid="Tab4" ref-type="table">4</xref>, acc is the accuracy rate, which is the proportion of the number of samples with correct classification to the total number of samples and is defined as:<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="d33e1703">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{acc}} = \frac{{TN+TP}}{{TN+TP+FN+FP}}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_18024_Article_Equ12.gif"/></alternatives></disp-formula></p><p id="Par72">where <italic toggle="yes">TN</italic> is a true negative class, <italic toggle="yes">TP</italic> is a true positive class, <italic toggle="yes">FN</italic> is a false negative class, and <italic toggle="yes">FP</italic> is a false positive class.</p><p>
<table-wrap id="Tab4" position="float" orientation="portrait"><label>Table&#160;4</label><caption><p>Pseudocode of the binary cuckoo search (BCS) algorithm.</p></caption><graphic position="anchor" id="MO10" orientation="portrait" xlink:href="41598_2025_18024_Tab4_HTML.jpg"/></table-wrap>
</p><p id="Par74">The implementation steps of the BCS algorithm given in Table&#160;<xref rid="Tab4" ref-type="table">4</xref> are explained as follows.</p><p id="Par75">Step 1. Initialize the system.</p><p id="Par76">Initialization population: Randomly generate the initial locations of <italic toggle="yes">n</italic> nests (bird nests), which are represented by binary coded strings. Set algorithm parameters: feature number <italic toggle="yes">d</italic>, discovery probability <italic toggle="yes">Pa</italic>, boundary value size <italic toggle="yes">L</italic> (if applicable), iteration number <italic toggle="yes">T</italic>, and so on.</p><p id="Par77">Step 2. Loop iteration.</p><p id="Par78">Update the nest location: Generate a new nest location candidate solution via the Levy flight mechanism via Eqs.&#160;(<xref rid="Equ10" ref-type="disp-formula">10</xref>) and (<xref rid="Equ11" ref-type="disp-formula">11</xref>). To balance global search and local search, a binary coding control coefficient is introduced to hybrid update the binary coding obtained via transformation.</p><p id="Par79">Calculate the fitness value: Calculate the objective function value (fitness value, i.e., the value of acc given by Eq.&#160;(12)) corresponding to each nest position.</p><p id="Par80">Selecting the best nest: Select the best nest location on the basis of the fitness value and retain it for the next generation.</p><p id="Par81">Elimination mechanism: The process of eliminating cuckoo eggs is simulated; that is, some nest positions are randomly replaced according to a certain probability <italic toggle="yes">Pa</italic> to maintain the diversity of the population.</p><p id="Par82">Step 3. Determine the termination conditions.</p><p id="Par83">Determine whether the algorithm meets the set maximum number of iterations Max <italic toggle="yes">T</italic>. If yes, the iterative optimization process ends, and the global optimal value is output. Otherwise, continue with the loop iteration Step 2.</p><p id="Par84">Step 4. Output the result.</p><p id="Par85">Output the globally optimal nest position and its corresponding optimal feature subset.</p><p id="Par86">The performance of the BCS algorithm is highly dependent on the choice of parameters, including the probability of discarding worse nests (<italic toggle="yes">P</italic>), the number of nests (<italic toggle="yes">n</italic>), and the step size (<italic toggle="yes">s</italic>). To ensure optimal performance, we conducted a sensitivity analysis by varying these parameters and observing their impact on classification accuracy. Specifically, we tested <italic toggle="yes">P</italic> values ranging from 0.1 to 0.5, nest sizes ranging from 10 to 50, and step sizes ranging from 0.1 to 1.0. The results indicated that a probability of 0.25, a nest size of 30, and a step size of 0.5 yielded the highest classification accuracy. These parameter settings were subsequently used in all the experiments to ensure consistent and reliable results. Through extensive empirical testing, we observed the following:</p><p id="Par87">Nest size (<italic toggle="yes">n</italic>): Accuracy plateaued when <italic toggle="yes">n</italic>&#8201;&#8805;&#8201;30, with less than 0.5% improvement beyond this value despite increased computational cost. The optimal trade-off was achieved at <italic toggle="yes">n</italic>&#8201;=&#8201;30. In fact, determined through convergence analysis, where <italic toggle="yes">n</italic>&#8201;&#8805;&#8201;30 achieved asymptotic accuracy (&#8804;&#8201;0.5% improvement for <italic toggle="yes">n</italic>&#8201;&gt;&#8201;30) while minimizing computation time (R&#178;=0.97 between n and runtime).</p><p id="Par88">Discovery probability (<italic toggle="yes">Pa</italic>): Values between 0.2 and 0.3 consistently yield peak accuracy (88.5&#8211;89%), whereas <italic toggle="yes">Pa</italic>&#8201;&gt;&#8201;0.3 leads to premature convergence, and <italic toggle="yes">Pa</italic>&#8201;&lt;&#8201;0.2 slows optimization. We selected <italic toggle="yes">Pa</italic>&#8201;=&#8201;0.25 as the balanced choice. In fact, this value aligns with Yang and Deb&#8217;s original cuckoo search formulation [49], where Pa&#8201;=&#8201;0.25 optimally balances exploration-exploitation trade-offs. Our sensitivity analysis confirmed that values 0.2&#8201;&lt;&#8201;Pa&#8201;&lt;&#8201;0.3 maintained 89.2&#8201;&#177;&#8201;0.4% accuracy while Pa&#8201;&gt;&#8201;0.3 caused premature convergence (84.1% at Pa&#8201;=&#8201;0.5).</p><p id="Par89">Step size (<italic toggle="yes">s</italic>): <italic toggle="yes">s</italic>&#8201;=&#8201;0.5 provided the fastest convergence (&#8804;&#8201;80 iterations) without the oscillatory behavior observed at higher values (e.g., <italic toggle="yes">s</italic>&#8201;&#8805;&#8201;0.7). Optimized via Levy flight stability analysis [52], with s&#8201;=&#8201;0.5 providing optimal heavy-tailed distribution properties (<italic toggle="yes">&#945;</italic>&#8201;=&#8201;1.5, <italic toggle="yes">&#946;</italic>&#8201;=&#8201;0). Lower values (<italic toggle="yes">s</italic>&#8201;&lt;&#8201;0.3) caused stagnation (120&#8201;+&#8201;iterations), while higher values (<italic toggle="yes">s</italic>&#8201;&gt;&#8201;0.7) induced oscillatory divergence.</p><p id="Par90">These findings were consistent across 10 independent runs, demonstrating robust parameter selection.</p><p id="Par91">In all experiments, the following optimal parameters determined through sensitivity analysis are used:</p><p id="Par92">Nest size: <italic toggle="yes">n</italic>&#8201;=&#8201;30.</p><p id="Par93">Maximum iterations: <italic toggle="yes">T</italic>&#8201;=&#8201;100.</p><p id="Par94">Discovery probability: <italic toggle="yes">Pa</italic>&#8201;=&#8201;0.25.</p><p id="Par95">Step size for Levy flight: <italic toggle="yes">s</italic>&#8201;=&#8201;0.5.</p><p id="Par96">These settings ensured optimal balance between computational efficiency (convergence within 80 iterations) and classification accuracy (89%).</p><p id="Par97">In the wrapper stage, Random Forest (RF) served as the classifier with classification accuracy as the evaluation metric. The RF model was configured with the following hyperparameters:</p><p id="Par98">n_estimators&#8201;=&#8201;100 (number of trees in the forest),</p><p id="Par99">max_depth&#8201;=&#8201;None (nodes expanded until all leaves are pure or contain less than min_samples_split),</p><p id="Par100">min_samples_split&#8201;=&#8201;2 (minimum samples required to split an internal node),</p><p id="Par101">random_state&#8201;=&#8201;42 (seed for reproducibility).</p><p id="Par102">These parameters were optimized via grid search during preliminary experiments to balance model performance and computational efficiency.</p><p id="Par103">The complete pseudocode of the BCS algorithm is provided in Table&#160;<xref rid="Tab4" ref-type="table">4</xref>, and its workflow is illustrated in Fig.&#160;<xref rid="Fig4" ref-type="fig">4</xref>.</p></sec><sec id="Sec11"><title>Feature selection method based on multiple cross-filtering and BCS</title><p id="Par104">As described in Sections&#160;&#8220;<xref rid="Sec9" ref-type="sec">Multiple cross-filtering</xref>&#8221; and &#8220;<xref rid="Sec10" ref-type="sec">BCS algorithm</xref>&#8221;, a new two-stage feature selection method based on multiple cross-filtering and BCS is proposed for risk factor identification in CAD.</p><p id="Par105">Stage 1: Feature primary screening by multiple cross-filtering.</p><p id="Par106">For the 50 variables contained in the initial feature set of CAD, the importance of features was measured from three perspectives&#8212;correlation, distance and information richness&#8212;by the filtering method. The specific process of constructing a new feature subset via the multiple cross-filtering method is shown in Fig.&#160;<xref rid="Fig3" ref-type="fig">3</xref>.</p><p id="Par107">From Fig.&#160;<xref rid="Fig3" ref-type="fig">3</xref>, in the first stage of multiple cross-filtering, the chi-square test method was used to determine the features that are not related to the category with a significance level&#8201;=&#8201;0.05 as the threshold, form set <italic toggle="yes">&#945;</italic>, and record the number of features <italic toggle="yes">m</italic> in set <italic toggle="yes">&#945;</italic>. Then, <italic toggle="yes">m</italic> low-importance features were identified via the Fisher score and mutual information to form sets <italic toggle="yes">&#946;</italic> and <italic toggle="yes">&#947;</italic>; cross-appearing features in <italic toggle="yes">&#945;</italic>, <italic toggle="yes">&#946;</italic> and <italic toggle="yes">&#947;</italic> were filtered out, and the remaining features formed subset <italic toggle="yes">T</italic><sub>1</sub>.</p><p id="Par108">Stage 2: Screening of the optimal subset of features via the BCS.</p><p id="Par109">To ensure the noise-free feature subset of the CAD risk assessment, the remaining features were wrapped and screened after the features were initially screened via the multiple cross-filtering method shown in Fig.&#160;<xref rid="Fig3" ref-type="fig">3</xref>, and then the optimal subset of features was obtained. In this wrapper stage, we used the BCS algorithm presented in Section&#160;&#8220;<xref rid="Sec10" ref-type="sec">BCS algorithm</xref>&#8221;. The flowchart of screening features via BCS feature search in the wrapper stage is shown in Fig.&#160;<xref rid="Fig4" ref-type="fig">4</xref>.</p><p id="Par110">From Fig.&#160;<xref rid="Fig4" ref-type="fig">4</xref>, in the second wrapper stage of the feature search, the random forest is used as the classification model, the classification accuracy is used as the evaluation standard, and the discrete BCS algorithm is used to identify the optimal subset of features. The input and output are as follows:</p><p id="Par111">Input: the filtered training set <inline-formula id="IEq37"><alternatives><tex-math id="d33e1980">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${T_1}=\left\{ {{t_1},{t_2}, \cdots ,{t_k}} \right\}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq37.gif"/></alternatives></inline-formula>, where <italic toggle="yes">k</italic> is the number of features in <italic toggle="yes">T</italic><sub>1</sub>.</p><p id="Par112">Output: Determine the optimal subset of features for CAD on the basis of the RF classification accuracy.</p><p id="Par113">
<fig id="Fig3" position="float" orientation="portrait"><label>Fig. 3</label><caption><p>Flow chart of the initial screening features of the multiple cross-filtering method.</p></caption><graphic id="d33e2003" position="float" orientation="portrait" xlink:href="41598_2025_18024_Fig3_HTML.jpg"/></fig>
</p><p id="Par114">
<fig id="Fig4" position="float" orientation="portrait"><label>Fig. 4</label><caption><p>Flow chart of screening features via BCS feature search in the wrapper stage. (Note: This flowchart corresponds to the pseudocode in Table&#160;<xref rid="Tab4" ref-type="table">4</xref>.)</p></caption><graphic id="d33e2018" position="float" orientation="portrait" xlink:href="41598_2025_18024_Fig4_HTML.jpg"/></fig>
</p><p id="Par115">The BCS algorithm iteratively refines the feature subset by evaluating the classification accuracy of each candidate solution (nest). As illustrated in Fig.&#160;<xref rid="Fig4" ref-type="fig">4</xref>, for every iteration, the algorithm discretizes the feature space via the sigmoid function (<inline-formula id="IEq38"><alternatives><tex-math id="d33e2025">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$S(x_{i}^{j}(t))$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq38.gif"/></alternatives></inline-formula>), where features are selected (<inline-formula id="IEq39"><alternatives><tex-math id="d33e2031">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{i}^{j}=1$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq39.gif"/></alternatives></inline-formula>) or discarded (<inline-formula id="IEq40"><alternatives><tex-math id="d33e2037">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$x_{i}^{j}=0$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_18024_Article_IEq40.gif"/></alternatives></inline-formula>) on the basis of a threshold <italic toggle="yes">&#963;</italic>. The RF classifier evaluates the accuracy (<italic toggle="yes">A</italic><sub><italic toggle="yes">i</italic></sub>) of each subset, and the BCS updates the global best nest (<italic toggle="yes">g</italic>) whenever a higher accuracy is found. Key parameters, such as the discard probability (<italic toggle="yes">P</italic><sub><italic toggle="yes">a</italic></sub>), nest count (<italic toggle="yes">n</italic>), and maximum number of iterations (<italic toggle="yes">T</italic>, <italic toggle="yes">&#946;</italic>), guide this process until convergence. The pseudocode (partial) and flowchart (Fig.&#160;<xref rid="Fig4" ref-type="fig">4</xref>) explicitly depict this optimization loop, including the termination condition (<italic toggle="yes">t</italic>&#8201;=&#8201;<italic toggle="yes">&#946;</italic>) and output of the feature subset corresponding to (<italic toggle="yes">A</italic>(<italic toggle="yes">max</italic>)).</p></sec></sec></sec><sec id="Sec12"><title>Empirical analysis of risk factor identification based on the CAD dataset</title><p id="Par116">To explain the implementation process of the proposed feature selection method based on multiple cross-filtering and BCS, as clearly shown in Fig.&#160;<xref rid="Fig2" ref-type="fig">2</xref>, and demonstrate the feasibility and effectiveness of the proposed method in this paper, in this section, we present an empirical analysis of risk factor identification based on a CAD dataset, which is a publicly available CAD dataset from the UCI machine learning repository at <ext-link ext-link-type="uri" xlink:href="http://archive.ics.uci.edu/ml/datasets/Heart+Disease">http://archive.ics.uci.edu/ml/datasets/Heart+Disease</ext-link>.</p><sec id="Sec13"><title>Stage of feature primary screening by multiple cross-filtering</title><p id="Par117">In this work, we divided the test set and training set at a ratio of 3:7. In the filtering stage, the importance of the 50 influencing factors is ranked according to the method in Section&#160;&#8220;<xref rid="Sec8" ref-type="sec">Feature selection method for selecting CAD risk factors</xref>&#8221;, the number of features relevant to the category is determined on the basis of the chi-square test significance level&#8201;=&#8201;0.05, and the set of irrelevant features <italic toggle="yes">&#945; is constructed</italic>. Then, the Fisher score and the threshold of the mutual information method are set equal to the number of relevant features determined by the chi-square test, the sets <italic toggle="yes">&#946;</italic> and <italic toggle="yes">&#947;</italic> of unrelated features are constructed, and the features contained in the intersection of the three sets are deleted. Table&#160;<xref rid="Tab5" ref-type="table">5</xref> shows the results of the chi-square test, Fisher score and mutual information method for ranking the importance of some features.</p><p id="Par118">According to the ranking results of the importance of the influencing factors of CAD, the features &#8220;Age&#8221;, &#8220;Nonanginal&#8221;, and &#8220;DM&#8221; were ranked relatively high in terms of correlation, information richness and distance. In summary, these features are important factors leading to CAD. Some of the indices showed high importance in two of the dimensions and relatively low importance in the third dimension; for example, &#8220;Region RWMA&#8221;, &#8220;Typical Chest Pain&#8221;, &#8220;EF-TTE&#8221; and &#8220;Tinversion&#8221; were ranked high in terms of relevance and information richness but did not show high importance when ranked by the Fisher score method.</p><p id="Par119">
<table-wrap id="Tab5" position="float" orientation="portrait"><label>Table 5</label><caption><p>Ranking of the importance of some factors influencing CAD.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Ranking of features</th><th align="left" colspan="1" rowspan="1">Chi-square test</th><th align="left" colspan="1" rowspan="1">Mutual Information Method</th><th align="left" colspan="1" rowspan="1">Fisher Score</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">1</td><td align="left" colspan="1" rowspan="1">Age</td><td align="left" colspan="1" rowspan="1">Typical Chest Pain</td><td align="left" colspan="1" rowspan="1">Atypical</td></tr><tr><td align="left" colspan="1" rowspan="1">2</td><td align="left" colspan="1" rowspan="1">Region RWMA</td><td align="left" colspan="1" rowspan="1">Atypical</td><td align="left" colspan="1" rowspan="1">Age</td></tr><tr><td align="left" colspan="1" rowspan="1">3</td><td align="left" colspan="1" rowspan="1">Typical Chest Pain</td><td align="left" colspan="1" rowspan="1">Region RWMA</td><td align="left" colspan="1" rowspan="1">BMI</td></tr><tr><td align="left" colspan="1" rowspan="1">4</td><td align="left" colspan="1" rowspan="1">Atypical</td><td align="left" colspan="1" rowspan="1">Nonanginal</td><td align="left" colspan="1" rowspan="1">BP</td></tr><tr><td align="left" colspan="1" rowspan="1">5</td><td align="left" colspan="1" rowspan="1">Nonanginal</td><td align="left" colspan="1" rowspan="1">Age</td><td align="left" colspan="1" rowspan="1">Nonanginal</td></tr><tr><td align="left" colspan="1" rowspan="1">6</td><td align="left" colspan="1" rowspan="1">DM</td><td align="left" colspan="1" rowspan="1">HTN</td><td align="left" colspan="1" rowspan="1">FBS</td></tr><tr><td align="left" colspan="1" rowspan="1">7</td><td align="left" colspan="1" rowspan="1">EF-TTE</td><td align="left" colspan="1" rowspan="1">EF-TTE</td><td align="left" colspan="1" rowspan="1">St Elevation</td></tr><tr><td align="left" colspan="1" rowspan="1">8</td><td align="left" colspan="1" rowspan="1">BP</td><td align="left" colspan="1" rowspan="1">DM</td><td align="left" colspan="1" rowspan="1">DM</td></tr><tr><td align="left" colspan="1" rowspan="1">9</td><td align="left" colspan="1" rowspan="1">Tinversion</td><td align="left" colspan="1" rowspan="1">Tinversion</td><td align="left" colspan="1" rowspan="1">CR</td></tr></tbody></table></table-wrap>
</p><p id="Par120">After sorting via the chi-square test, 19 features were found to be associated with the category at a significance level of 0.05, so the number of unrelated features that did not pass the chi-square test was 31. The feature subsets constructed by the 31 uncorrelated features were denoted as set <italic toggle="yes">&#945;</italic>. Then, with 31 bits as the threshold, the latter 31 bits of features sorted by the mutual information method and Fisher score method were found, and the subsets composed of these features were denoted as set <italic toggle="yes">&#946;</italic> and set <italic toggle="yes">&#947;</italic>, respectively. The intersection of the three sets was observed, and the 21 features contained in the intersection (&#8220;K&#8221;, &#8220;Na&#8221;, &#8220;Lung rales&#8221; and &#8220;Lymph&#8221;, etc.) were eliminated to complete the initial filtering method.</p></sec><sec id="Sec14"><title>BCS feature search in the wrapper stage</title><p id="Par121">In the wrapper stage, this paper continues to screen the feature subsets again after the initial screening; that is, the subsets of CAD risk factors are retained after the multiple cross-filtering steps are searched repeatedly via the BCS, and the subsets obtained each time are verified with the help of the RF classifier to calculate their classification accuracies. The relationship between the RF classification accuracy and the number of iterations is shown in Fig.&#160;<xref rid="Fig5" ref-type="fig">5</xref>.</p><p id="Par122">The RF classifier&#8217;s hyperparameters were fine-tuned using 5-fold cross-validation on the training set. The search space included:<list list-type="bullet"><list-item><p id="Par123">n_estimators: [50, 100, 200],</p></list-item><list-item><p id="Par124">max_depth: [5, 10, None],</p></list-item><list-item><p id="Par125">min_samples_split: [2, 5, 10].</p></list-item><list-item><p id="Par126">The optimal combination (n_estimators&#8201;=&#8201;100, max_depth&#8201;=&#8201;None, min_samples_split&#8201;=&#8201;2) was selected based on AUC maximization.</p></list-item></list></p><p id="Par127">The number of features was fixed, and the optimal subset consisting of 15 features was identified. Figure&#160;<xref rid="Fig5" ref-type="fig">5</xref> shows that the accuracy starts to increase from 76.4% when the number of iterations gradually increases from 0. When the number of iterations increases to 80, the classification accuracy of the RF starts to approach the peak. As the number of iterations increased from 80 to 100, the trend plateaued, and the classification accuracy decreased, indicating that a maximum accuracy of 89% was achieved. Therefore, the feature subset corresponding to the highest classification accuracy was taken as the optimal feature subset after feature screening and contains the most important factors leading to CAD, i.e., age, atypical, nonanginal, sex, typical chest pain, DM (diabetes mellitus), HTN (hypertension), region RWMA (regional wall motion abnormality), BP (blood pressure), Q wave (electrocardiogram), EF-TTE (ejection fraction), PR (pulse rate), BMI (body mass index), TG (triglyceride), and CR (blood creatinine).</p><p id="Par128">
<fig id="Fig5" position="float" orientation="portrait"><label>Fig. 5</label><caption><p>Effect of the number of iterations on the classification accuracy.</p></caption><graphic id="d33e2270" position="float" orientation="portrait" xlink:href="41598_2025_18024_Fig5_HTML.jpg"/></fig>
</p><p id="Par129">The 15 key risk factors identified by our model align well with established medical knowledge of CAD pathogenesis. Age is a well-documented nonmodifiable risk factor, as aging contributes to arterial stiffening, endothelial dysfunction, and cumulative exposure to other cardiovascular stressors. DM exacerbates CAD risk through chronic hyperglycemia, which promotes oxidative stress, inflammation, and accelerated atherosclerosis. HTN increases the myocardial workload and vascular shear stress, leading to left ventricular hypertrophy and coronary microvascular dysfunction. BMI and TG are linked to metabolic syndrome, which drives insulin resistance and dyslipidemia, further worsening coronary plaque formation. RWMA and EF-TTE reflect structural and functional cardiac impairments, which are directly correlated with ischemic damage. These findings not only validate our model&#8217;s biological plausibility but also reinforce the clinical importance of monitoring these parameters in CAD prevention and management.</p><p id="Par130">The selected 15 features do not act in isolation but often exhibit synergistic effects that amplify CAD risk. For example, Age and HTN: Aging exacerbates vascular stiffness, whereas HTN increases shear stress on arterial walls. Together, these factors accelerate endothelial dysfunction and plaque formation. Clinical studies have shown that hypertensive patients over 45 years of age have a 3-fold higher CAD risk than younger normotensive individuals do. DM and BMI: Insulin resistance in DM impairs lipid metabolism, whereas visceral adiposity (reflected by high BMI) promotes systemic inflammation. This combination leads to more severe coronary atherosclerosis than either factor alone. BP and Regional RWMAs: Elevated BP increases myocardial oxygen demand, whereas RWMAs indicate localized ischemia. Their co-occurrence suggests advanced CAD with compromised cardiac compensation [4]. These interactions highlight the importance of multidimensional risk assessment rather than reliance on isolated markers.</p><p id="Par131">In addition, the dependent variable &#8220;Cath&#8221; was considered a feature, and the correlation between the features was plotted via Python. Some features and dependent variables were selected to construct the correlation graph, and the results are shown in Fig.&#160;<xref rid="Fig6" ref-type="fig">6</xref>. Age, diabetes and hypertension were strongly correlated with the dependent variables, further verifying that they are important factors leading to CAD.</p><p id="Par132">
<fig id="Fig6" position="float" orientation="portrait"><label>Fig. 6</label><caption><p>Correlation heatmap between features.</p></caption><graphic id="d33e2289" position="float" orientation="portrait" xlink:href="41598_2025_18024_Fig6_HTML.jpg"/></fig>
</p><p id="Par133">This study identified 15 key factors significantly associated with coronary artery disease (CAD). These findings offer actionable insights for clinical practice: integrating these parameters enables clinicians to rapidly screen high-risk patients, optimize diagnostic workflows, and design personalized intervention plans. For example, combining typical chest pain with RWMAs enhances early diagnostic sensitivity, whereas dynamic monitoring of metabolic markers such as BP, BMI, and DM supports long-term risk stratification. The proposed model quantifies the synergistic effects of multidimensional features, empowering clinicians to extract prioritized, actionable insights from complex data. Particularly in primary care or resource-constrained settings, it improves diagnostic efficiency and reduces missed diagnoses, demonstrating tangible utility in real-world decision-making.</p><p id="Par134">To substantiate the interpretability and clinical utility of our model, we developed a practical risk scoring system based on the identified 15 key factors (Table&#160;<xref rid="Tab6" ref-type="table">6</xref>). Each factor is assigned weighted points reflecting its relative importance derived from the feature selection process.</p><p id="Par135">
<table-wrap id="Tab6" position="float" orientation="portrait"><label>Table 6</label><caption><p>Clinical risk scoring system based on identified CAD risk factors.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Risk factor</th><th align="left" colspan="1" rowspan="1">Criteria</th><th align="left" colspan="1" rowspan="1">Score</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Age</td><td align="left" colspan="1" rowspan="1">&lt;&#8201;45 (Male), &lt;&#8201;55 (Female)</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8805;&#8201;45 (Male), &#8805;&#8201;55 (Female)</td><td char="." align="char" colspan="1" rowspan="1">2</td></tr><tr><td align="left" colspan="1" rowspan="1">Atypical</td><td align="left" colspan="1" rowspan="1">No</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">Yes</td><td char="." align="char" colspan="1" rowspan="1">1</td></tr><tr><td align="left" colspan="1" rowspan="1">Nonanginal</td><td align="left" colspan="1" rowspan="1">No</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">Yes</td><td char="." align="char" colspan="1" rowspan="1">1</td></tr><tr><td align="left" colspan="1" rowspan="1">Sex</td><td align="left" colspan="1" rowspan="1">Female</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">Male</td><td char="." align="char" colspan="1" rowspan="1">1</td></tr><tr><td align="left" colspan="1" rowspan="1">Typical Chest Pain</td><td align="left" colspan="1" rowspan="1">No</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">Yes</td><td char="." align="char" colspan="1" rowspan="1">3</td></tr><tr><td align="left" colspan="1" rowspan="1">DM</td><td align="left" colspan="1" rowspan="1">No</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">Yes</td><td char="." align="char" colspan="1" rowspan="1">2</td></tr><tr><td align="left" colspan="1" rowspan="1">HTN</td><td align="left" colspan="1" rowspan="1">No</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">Yes</td><td char="." align="char" colspan="1" rowspan="1">1</td></tr><tr><td align="left" colspan="1" rowspan="1">Region RWMA</td><td align="left" colspan="1" rowspan="1">No</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">Yes</td><td char="." align="char" colspan="1" rowspan="1">2</td></tr><tr><td align="left" colspan="1" rowspan="1">BP</td><td align="left" colspan="1" rowspan="1">90&#8211;129</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">129&#8201;&lt;&#8201;BP&#8201;&lt;&#8201;139</td><td char="." align="char" colspan="1" rowspan="1">1</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&gt;&#8201;139</td><td char="." align="char" colspan="1" rowspan="1">2</td></tr><tr><td align="left" colspan="1" rowspan="1">Q wave</td><td align="left" colspan="1" rowspan="1">No</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">Yes</td><td char="." align="char" colspan="1" rowspan="1">1</td></tr><tr><td align="left" colspan="1" rowspan="1">EF-TTE</td><td align="left" colspan="1" rowspan="1">&#8805;&#8201;50%</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&lt;&#8201;50%</td><td char="." align="char" colspan="1" rowspan="1">3</td></tr><tr><td align="left" colspan="1" rowspan="1">PR</td><td align="left" colspan="1" rowspan="1">60&#8211;100</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&lt;&#8201;60 or &gt;&#8201;100</td><td char="." align="char" colspan="1" rowspan="1">1</td></tr><tr><td align="left" colspan="1" rowspan="1">BMI</td><td align="left" colspan="1" rowspan="1">18.5&#8211;23.9</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&lt;&#8201;18.5 or 23.9&#8201;&lt;&#8201;BMI&#8201;&lt;&#8201;27.9</td><td char="." align="char" colspan="1" rowspan="1">1</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&gt;&#8201;27.9</td><td char="." align="char" colspan="1" rowspan="1">2</td></tr><tr><td align="left" colspan="1" rowspan="1">TG</td><td align="left" colspan="1" rowspan="1">&lt;&#8201;150</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">&#8805;&#8201;150</td><td char="." align="char" colspan="1" rowspan="1">1</td></tr><tr><td align="left" colspan="1" rowspan="1">CR</td><td align="left" colspan="1" rowspan="1">Normal range</td><td char="." align="char" colspan="1" rowspan="1">0</td></tr><tr><td align="left" colspan="1" rowspan="1"/><td align="left" colspan="1" rowspan="1">Abnormal</td><td char="." align="char" colspan="1" rowspan="1">1</td></tr></tbody></table></table-wrap>
</p><p id="Par136">The proposed risk scoring system (Table&#160;<xref rid="Tab6" ref-type="table">6</xref>) demonstrates direct clinical utility. For instance, a primary care physician can quickly calculate a patient&#8217;s CAD risk during routine check-ups by summing the scores of applicable factors. This score directly informs referral decisions: low-risk patients receive lifestyle advice, moderate-risk patients undergo stress testing, and high-risk patients are prioritized for angiography. Integration with hospital EHR systems further automates this process&#8212;entering patient data triggers real-time risk alerts, reducing diagnostic delays.</p></sec><sec id="Sec15"><title>Comparative analysis and results discussion</title><p id="Par137">To better illustrate the advantages of the risk factor identification mechanism based on the multiple cross-filtering and binary cuckoo search methods proposed in this paper, we compared our proposed model (Method I) with several existing homogeneous models (a two-stage feature selection method<sup><xref ref-type="bibr" rid="CR35">35</xref></sup> (Method II), a hybrid filter-wrapper feature selection method<sup><xref ref-type="bibr" rid="CR36">36</xref></sup> (Method III), and an improved filter-wrapper feature selection method<sup><xref ref-type="bibr" rid="CR38">38</xref></sup> (Method IV)) for feature selection in terms of runtime complexity and classification accuracy, and the compared results are as follows. The running times (units: second) of Method I, Method II, Method III and Method IV are 66&#160;s, 78&#160;s, 102&#160;s and 189&#160;s, respectively, and the classification accuracies of Method I, Method II, Method III and Method IV are 89%, 83%, 86% and 81%, respectively. The results are shown in Fig.&#160;<xref rid="Fig7" ref-type="fig">7</xref>. The results show that the running time of our proposed model (Method I) is shorter than that of Method II, Method III and Method IV, which means that the computational efficiency of our model is greater than that of the compared models. Moreover, the classification accuracy of our proposed model is also the highest among all of the compared models. Moreover, additional evaluation metrics, such as precision, recall, F1 score, and AUC, are given to provide a holistic view of model performance. The comprehensive performance comparison results of the different methods are shown in Table&#160;<xref rid="Tab7" ref-type="table">7</xref>.</p><p id="Par138">In Table&#160;<xref rid="Tab7" ref-type="table">7</xref>, the definitions of the evaluation metrics are as follows: Precision&#8201;=&#8201;<italic toggle="yes">TP</italic>/(<italic toggle="yes">TP</italic>&#8201;+&#8201;<italic toggle="yes">FP</italic>), Recall&#8201;=&#8201;<italic toggle="yes">TP</italic>/(<italic toggle="yes">TP</italic>&#8201;+&#8201;<italic toggle="yes">FN</italic>), F1-score&#8201;=&#8201;2&#215;(Precision&#215;Recall)/(Precision&#8201;+&#8201;Recall), AUC: Area under the ROC curve, and accuracy is defined by Eq.&#160;(<xref rid="Equ12" ref-type="disp-formula">12</xref>). These metrics collectively address both class imbalance and clinical utility.</p><p id="Par139">
<table-wrap id="Tab7" position="float" orientation="portrait"><label>Table 7</label><caption><p>Comprehensive performance comparison of different methods.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Metric</th><th align="left" colspan="1" rowspan="1">Method I (Proposed)</th><th align="left" colspan="1" rowspan="1">Method II</th><th align="left" colspan="1" rowspan="1">Method III</th><th align="left" colspan="1" rowspan="1">Method IV</th><th align="left" colspan="1" rowspan="1">XGBoost</th><th align="left" colspan="1" rowspan="1">LASSO</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Accuracy (%)</td><td align="left" colspan="1" rowspan="1">89</td><td align="left" colspan="1" rowspan="1">83</td><td align="left" colspan="1" rowspan="1">86</td><td align="left" colspan="1" rowspan="1">81</td><td align="left" colspan="1" rowspan="1">85</td><td align="left" colspan="1" rowspan="1">82</td></tr><tr><td align="left" colspan="1" rowspan="1">Precision</td><td align="left" colspan="1" rowspan="1">0.87</td><td align="left" colspan="1" rowspan="1">0.81</td><td align="left" colspan="1" rowspan="1">0.84</td><td align="left" colspan="1" rowspan="1">0.79</td><td align="left" colspan="1" rowspan="1">0.83</td><td align="left" colspan="1" rowspan="1">0.80</td></tr><tr><td align="left" colspan="1" rowspan="1">Recall</td><td align="left" colspan="1" rowspan="1">0.91</td><td align="left" colspan="1" rowspan="1">0.85</td><td align="left" colspan="1" rowspan="1">0.88</td><td align="left" colspan="1" rowspan="1">0.83</td><td align="left" colspan="1" rowspan="1">0.86</td><td align="left" colspan="1" rowspan="1">0.81</td></tr><tr><td align="left" colspan="1" rowspan="1">F1-score</td><td align="left" colspan="1" rowspan="1">0.89</td><td align="left" colspan="1" rowspan="1">0.83</td><td align="left" colspan="1" rowspan="1">0.86</td><td align="left" colspan="1" rowspan="1">0.81</td><td align="left" colspan="1" rowspan="1">0.84</td><td align="left" colspan="1" rowspan="1">0.80</td></tr><tr><td align="left" colspan="1" rowspan="1">AUC</td><td align="left" colspan="1" rowspan="1">0.93</td><td align="left" colspan="1" rowspan="1">0.87</td><td align="left" colspan="1" rowspan="1">0.90</td><td align="left" colspan="1" rowspan="1">0.85</td><td align="left" colspan="1" rowspan="1">0.88</td><td align="left" colspan="1" rowspan="1">0.84</td></tr><tr><td align="left" colspan="1" rowspan="1">Runtime (s)</td><td align="left" colspan="1" rowspan="1">66</td><td align="left" colspan="1" rowspan="1">78</td><td align="left" colspan="1" rowspan="1">102</td><td align="left" colspan="1" rowspan="1">189</td><td align="left" colspan="1" rowspan="1">105</td><td align="left" colspan="1" rowspan="1">65</td></tr></tbody></table></table-wrap>
</p><p id="Par140">The experimental results in Table&#160;<xref rid="Tab7" ref-type="table">7</xref> inform that the values of Accuracy, Precision, Recall, F1-score and AUC for the proposed method in this paper are each at least 3.49%, 3.57%, 3.41%, 3.49% and 3.33% higher than those of other existing homogeneous models. Table&#160;<xref rid="Tab7" ref-type="table">7</xref> also shows that the comprehensive evaluation metrics demonstrate the robustness of our proposed method. Notably, the high precision (0.87) indicates minimal false positives in CAD diagnosis, which is crucial for avoiding unnecessary interventions. The recall (0.91) shows excellent sensitivity in identifying true CAD cases, addressing the class imbalance issue. The AUC (0.93) further confirmed the superior discriminative ability between CAD patients and non-CAD patients across all probability thresholds<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>. These results validate that our method maintains balanced performance despite dataset imbalance, outperforming alternatives in both predictive power and efficiency.</p><p id="Par141">Furthermore, we have incorporated XGBoost<sup><xref ref-type="bibr" rid="CR26">26</xref></sup> and LASSO<sup><xref ref-type="bibr" rid="CR29">29</xref></sup> as additional baselines. As shown in Table&#160;<xref rid="Tab7" ref-type="table">7</xref>, our method outperforms XGBoost in accuracy by 4% (89% vs. 85%) and LASSO by 7% (89% vs. 82%), while maintaining competitive computational efficiency. The superior AUC (0.93) and F1-score (0.89) of our model further validate its robustness in handling imbalanced CAD data, which XGBoost and LASSO struggle to address effectively due to their inherent limitations in feature interaction modeling and sensitivity to class imbalance.</p><p id="Par142">While the UCI CAD dataset provides a valuable foundation for our empirical analysis, it is important to acknowledge its limitations, including its relatively small size and potential lack of diversity. To address these limitations, future work will involve validating the proposed model on larger and more diverse datasets, such as the Framingham Heart Study dataset or other publicly available CAD datasets. This will ensure the generalizability and robustness of our findings across different populations and clinical settings.</p><p id="Par143">To address dataset diversity limitations, we conducted expanded analysis on the Framingham Heart Study dataset (<italic toggle="yes">n</italic>&#8201;=&#8201;4,239) (dataset source: <ext-link ext-link-type="uri" xlink:href="https://www.kaggle.com/navink25/framingham">https://www.kaggle.com/navink25/framingham</ext-link>). As shown in Table&#160;<xref rid="Tab8" ref-type="table">8</xref>, this longitudinal cohort (mean follow-up 12.3 years) included 2,312 females (54.5%) with balanced age distribution (52.8&#8201;&#177;&#8201;8.4 years). Our method identified 17 key risk factors, with 12 overlapping the UCI findings (e.g., age, hypertension, diabetes). Novel predictors included smoking duration (OR&#8201;=&#8201;1.32, 95%CI: 1.18&#8211;1.48) and family history (OR&#8201;=&#8201;2.07, 95%CI: 1.89&#8211;2.26).</p><p id="Par144">
<table-wrap id="Tab8" position="float" orientation="portrait"><label>Table 8</label><caption><p>Framingham dataset characteristics and selected features.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Aspect</th><th align="left" colspan="1" rowspan="1">Details</th><th align="left" colspan="1" rowspan="1">Top 5 novel predictors</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Sample size</td><td align="left" colspan="1" rowspan="1">4,239 (1,978 CAD cases)</td><td align="left" colspan="1" rowspan="1">Smoking duration (<italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.001)</td></tr><tr><td align="left" colspan="1" rowspan="1">Follow-up duration</td><td align="left" colspan="1" rowspan="1">12.3&#8201;&#177;&#8201;3.1 years</td><td align="left" colspan="1" rowspan="1">Family history (<italic toggle="yes">p</italic>&#8201;&lt;&#8201;0.001)</td></tr><tr><td align="left" colspan="1" rowspan="1">Female, <italic toggle="yes">n</italic> (%)</td><td align="left" colspan="1" rowspan="1">2,312(54.5)</td><td align="left" colspan="1" rowspan="1">LDL/HDL ratio (<italic toggle="yes">p</italic>&#8201;=&#8201;0.003)</td></tr><tr><td align="left" colspan="1" rowspan="1">Mean age&#8201;&#177;&#8201;SD</td><td align="left" colspan="1" rowspan="1">52.8&#8201;&#177;&#8201;8.4 years</td><td align="left" colspan="1" rowspan="1">CRP level (<italic toggle="yes">p</italic>&#8201;=&#8201;0.012)</td></tr><tr><td align="left" colspan="1" rowspan="1">Key overlapping features with UCI</td><td align="left" colspan="1" rowspan="1">Age, HTN, DM, BMI, TG</td><td align="left" colspan="1" rowspan="1">Glucose variability (<italic toggle="yes">p</italic>&#8201;=&#8201;0.018)</td></tr></tbody></table></table-wrap>
</p><p id="Par145">The model achieved 94.7% AUC (95%CI:93.2&#8211;96.1) with consistent performance across subgroups (Fig.&#160;<xref rid="Fig7" ref-type="fig">7</xref>). The extended analysis confirms our method&#8217;s robustness across diverse populations and identifies novel longitudinal predictors.&#8221;</p><p id="Par146">We also compared our proposed model (Method I) with the same existing homogeneous models, i.e., Method II<sup><xref ref-type="bibr" rid="CR35">35</xref></sup>Method III<sup><xref ref-type="bibr" rid="CR36">36</xref></sup>and Method IV<sup><xref ref-type="bibr" rid="CR38">38</xref></sup>for feature selection in terms of runtime complexity and classification accuracy. The running times (units: second) of Method I, Method II, Method III and Method IV are 221&#160;s, 253&#160;s, 338&#160;s and 422&#160;s, respectively, and the classification accuracies of Method I, Method II, Method III and Method IV are 95%, 88%, 91% and 85%, respectively, as shown in Fig.&#160;<xref rid="Fig7" ref-type="fig">7</xref>. This result still shows that our model performs better than other comparable models.</p><p id="Par147">Cross-validation on ethnically diverse datasets (UCI: Middle Eastern cohort; Framingham: Caucasian population) demonstrates model generalizability. While feature importance rankings showed population-specific variations (e.g., smoking duration predominance in Framingham), core predictors (age, HTN, DM) remained consistent, aligning with pathophysiological mechanisms of atherosclerosis.</p><p id="Par148">In addition to running times and classification accuracy, we also evaluated the proposed model in terms of interpretability, cost efficiency, and scalability as follows. The multiple cross-filtering stage provides a clear and interpretable ranking of feature importance, enabling clinicians to understand the relative contribution of each risk factor. Furthermore, the efficient search mechanism of the BCS algorithm ensures that the model is computationally cost-effective, making it suitable for large-scale applications. Finally, the model&#8217;s modular design allows easy integration with other machine learning algorithms and datasets, enhancing its scalability and adaptability to different clinical settings.</p><p id="Par149">
<fig id="Fig7" position="float" orientation="portrait"><label>Fig. 7</label><caption><p>Comparison of the homogeneous models.</p></caption><graphic id="d33e2860" position="float" orientation="portrait" xlink:href="41598_2025_18024_Fig7_HTML.jpg"/></fig>
</p><p id="Par150">Importantly, while we report classification accuracy as one metric to evaluate the quality of selected feature subsets, the primary objective of this study is feature selection rather than diagnostic classification. Traditional diagnostic performance metrics such as ROC curves or confusion matrices, while valuable for clinical prediction models, are less relevant for evaluating feature selection methods where the focus is on identifying meaningful predictors while maintaining model performance. The consistent superiority of our method across two independent datasets in both computational efficiency and preserved classification accuracy provides robust evidence for its effectiveness as a feature selection approach. The clinical interpretability of the selected features (including age, hypertension, diabetes mellitus, etc.) that align with established medical knowledge offers additional validation of our method&#8217;s practical utility.</p></sec></sec><sec id="Sec16"><title>Conclusion</title><p id="Par151">This study proposes a novel two-stage risk factor identification framework for CAD that integrates multiple cross-filtering with binary cuckoo search (BCS). The first filtration stage employs a multicriteria screening approach combining chi-square testing, mutual information analysis, and Fisher scoring to preliminarily identify potential risk factors. The subsequent wrapper phase uses a random forest (RF) classifier&#8217;s accuracy as the evaluative metric, with the BCS algorithm optimizing feature subset selection. Using the UCI platform&#8217;s CAD dataset for empirical validation, our analysis identified 15 critical risk factors: age, atypical, nonanginal, sex, typical chest pain, DM (diabetes mellitus), HTN (hypertension), regional RWMA (regional wall motion abnormality), BP (blood pressure), Q wave (electrocardiogram), EF-TTE (ejection fraction), PR (pulse rate), BMI (body mass index), TG (triglyceride), and CR (blood creatinine). Compared with conventional methods, comparative evaluations demonstrate the framework&#8217;s superior performance in both computational efficiency and classification accuracy. While the current implementation employs specific filter methods and RF classification, the architecture permits substitution with alternative filtration techniques and state-of-the-art predictive models relevant to CAD pathophysiology. This adaptable framework shows significant potential for clinical risk stratification and CAD management applications. Compared with the literature, the main contributions of this paper are as follows:</p><p id="Par152">
<list list-type="simple"><list-item><label>(i)</label><p id="Par153">A new two-stage risk factor identification mechanism for coronary artery disease based on multiple cross-filtering and binary cuckoo searches is proposed to determine the important influencing factors of CAD.</p></list-item><list-item><label>(ii)</label><p id="Par154">In the multicross filtering stage, the importance of influencing factors was comprehensively considered from three perspectives, i.e., distance, relevance, and information richness, and the features with low importance in these three aspects were screened out to reduce the probability of feature preference.</p></list-item><list-item><label>(iii)</label><p id="Par155">In the wrapper stage of the BCS feature search, the random forest classifier was used as the classification model, the classification accuracy was used as the evaluation standard, and the discrete BCS algorithm was used to identify the features that affect the risk rating to improve the accuracy of CAD risk evaluation.</p></list-item></list>
</p><p id="Par156">Moreover, the developed clinical risk scoring system (Table&#160;<xref rid="Tab6" ref-type="table">6</xref>) translates complex model outputs into actionable clinical tools, enhancing real-world interpretability and adoption.</p><p id="Par157">The proposed two-stage risk factor identification mechanism has significant potential for clinical application. By accurately identifying the most important risk factors for CAD, this model can assist clinicians in making more informed decisions regarding patient risk assessment and intervention strategies. The identified risk factors provide actionable insights for clinical practice. For example, integrating age, HTN, and DM into routine screening protocols could enhance early detection of high-risk patients, particularly in primary care settings with limited resources. The strong associations of RWMAs and EF-TTE with CAD severity suggest that echocardiographic parameters should be prioritized in diagnostic workflows. Furthermore, the inclusion of metabolic markers (e.g., BMI and TG) underscores the need for lifestyle interventions alongside pharmacological management. Our model&#8217;s ability to quantify the synergistic effects of multidimensional features enables clinicians to stratify risk more precisely, thereby optimizing personalized treatment plans. Future implementation in electronic health records (EHRs) could facilitate real-time risk assessment and decision support, ultimately improving CAD outcomes. Future work will focus on integrating this model into clinical decision support systems, enabling real-time risk assessment and personalized treatment recommendations. In addition, future work could integrate our feature selection with DL architectures when larger datasets become available.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><notes notes-type="author-contribution"><title>Author contributions</title><p>Congjun Rao: Conceptualization, Methodology, Data curation. Jing Wang: Methodology, Software, Writing - original draft. Ying Liu: Formal analysis, Software, Writing - review &amp; editing. Jing Yuan: Visualization, Investigation, Validation, Methodology.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>This work is supported by the National Natural Science Foundation of China (No. 72071150), and the Fundamental Research Funds for the Central Universities project under Grant (104972024KFYjc0069).</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>Data is provided within the manuscript.</p></notes><notes><title>Declarations</title><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par158">The authors declare no competing interests.</p></notes></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Benjamin</surname><given-names>EJ</given-names></name><etal/></person-group><article-title>Heart disease and stroke statistics&#8212;2019 update: A report from the American heart association</article-title><source>Am. Heart Association</source><year>2019</year><volume>13</volume><fpage>956</fpage><lpage>528</lpage></element-citation><mixed-citation id="mc-CR1" publication-type="journal">Benjamin, E. J. et al. Heart disease and stroke statistics&#8212;2019 update: A report from the American heart association. <italic toggle="yes">Am. Heart Assoc</italic>. <bold>13</bold>, 956&#8211;528 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Choi</surname><given-names>J</given-names></name><etal/></person-group><article-title>Lifestyle factors, genetic susceptibility to obesity and their interactions on coronary artery disease risk: A cohort study in the UK biobank</article-title><source>Prev. Med.</source><year>2024</year><volume>180</volume><fpage>107886</fpage><pub-id pub-id-type="pmid">38316272</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.ypmed.2024.107886</pub-id></element-citation><mixed-citation id="mc-CR2" publication-type="journal">Choi, J. et al. Lifestyle factors, genetic susceptibility to obesity and their interactions on coronary artery disease risk: A cohort study in the UK biobank. <italic toggle="yes">Prev. Med.</italic><bold>180</bold>, 107886 (2024).<pub-id pub-id-type="pmid">38316272</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.ypmed.2024.107886</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><citation-alternatives><element-citation id="ec-CR3" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ma</surname><given-names>XT</given-names></name><etal/></person-group><article-title>An innovative approach for assessing coronary artery lesions: fusion of wrist pulse and photoplethysmography using a multi-sensor pulse diagnostic device</article-title><source>Heliyo</source><year>2024</year><volume>10</volume><fpage>e28652</fpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.heliyon.2024.e28652</pub-id><pub-id pub-id-type="pmcid">PMC11021889</pub-id><pub-id pub-id-type="pmid">38633637</pub-id></element-citation><mixed-citation id="mc-CR3" publication-type="journal">Ma, X. T. et al. An innovative approach for assessing coronary artery lesions: Fusion of wrist pulse and photoplethysmography using a multi-sensor pulse diagnostic device. <italic toggle="yes">Heliyo</italic><bold>10</bold>, e28652 (2024).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.heliyon.2024.e28652</pub-id><pub-id pub-id-type="pmcid">PMC11021889</pub-id><pub-id pub-id-type="pmid">38633637</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Saryazdi</surname><given-names>MD</given-names></name><name name-style="western"><surname>Mostafaeipour</surname><given-names>A</given-names></name></person-group><article-title>Identification and validation of key predictive factors for heart attack diagnosis using machine learning and fuzzy clustering</article-title><source>Eng. Appl. Artif. Intell.</source><year>2025</year><volume>142</volume><fpage>109968</fpage></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Saryazdi, M. D. &amp; Mostafaeipour, A. Identification and validation of key predictive factors for heart attack diagnosis using machine learning and fuzzy clustering. <italic toggle="yes">Eng. Appl. Artif. Intell.</italic><bold>142</bold>, 109968 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kannel</surname><given-names>WB</given-names></name><etal/></person-group><article-title>Factors of risk in the development of coronary heart disease&#8212;Six-year follow up experience: the Framingham study</article-title><source>Ann. Intern. Med.</source><year>1961</year><volume>49</volume><fpage>33</fpage><lpage>50</lpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.7326/0003-4819-55-1-33</pub-id><pub-id pub-id-type="pmid">13751193</pub-id></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Kannel, W. B. et al. Factors of risk in the development of coronary heart disease&#8212;Six-year follow up experience: The Framingham study. <italic toggle="yes">Ann. Intern. Med.</italic><bold>49</bold>, 33&#8211;50 (1961).<pub-id pub-id-type="doi" assigning-authority="pmc">10.7326/0003-4819-55-1-33</pub-id><pub-id pub-id-type="pmid">13751193</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Truett</surname><given-names>J</given-names></name><name name-style="western"><surname>Cornfield</surname><given-names>J</given-names></name><name name-style="western"><surname>Kannel</surname><given-names>W</given-names></name></person-group><article-title>A multivariate analysis of the risk of coronary heart disease in Framingham</article-title><source>J. Chronic Dis.</source><year>1967</year><volume>20</volume><issue>7</issue><fpage>511</fpage><lpage>524</lpage><pub-id pub-id-type="pmid">6028270</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/0021-9681(67)90082-3</pub-id></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Truett, J., Cornfield, J. &amp; Kannel, W. A multivariate analysis of the risk of coronary heart disease in Framingham. <italic toggle="yes">J. Chronic Dis.</italic><bold>20</bold> (7), 511&#8211;524 (1967).<pub-id pub-id-type="pmid">6028270</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/0021-9681(67)90082-3</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yuan</surname><given-names>SH</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>XC</given-names></name></person-group><article-title>Methods and problems of cardiovascular disease risk assessment</article-title><source>Chin. J. Cardiovasc. Med.</source><year>2011</year><volume>16</volume><fpage>223</fpage><lpage>226</lpage></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Yuan, S. H. &amp; Zhang, X. C. Methods and problems of cardiovascular disease risk assessment. <italic toggle="yes">Chin. J. Cardiovasc. Med.</italic><bold>16</bold>, 223&#8211;226 (2011).</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><mixed-citation publication-type="other">World Health Organization. <italic toggle="yes">Pocket Guide for Cardiovascular Risk Assessment and Management in the Prevention and Treatment of Cardiovascular Diseases</italic> (World Health Organization Publications Office, 2008).</mixed-citation></ref><ref id="CR9"><label>9.</label><mixed-citation publication-type="other">Goff, D. C. et al. 2013 ACC/AHA guideline on the assessment of cardiovascular risk: A report of the American college of cardiology/american heart association task force on practice guidelines. <italic toggle="yes">Circulation</italic><bold>129</bold>, S49&#8211;S73 (2014).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1161/01.cir.0000437741.48606.98</pub-id><pub-id pub-id-type="pmid">24222018</pub-id></mixed-citation></ref><ref id="CR10"><label>10.</label><citation-alternatives><element-citation id="ec-CR10" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wei</surname><given-names>X</given-names></name><etal/></person-group><article-title>Risk assessment of cardiovascular disease based on SOLSSA-CatBoost model</article-title><source>Expert Syst. Appl.</source><year>2023</year><volume>219</volume><fpage>119648</fpage></element-citation><mixed-citation id="mc-CR10" publication-type="journal">Wei, X. et al. Risk assessment of cardiovascular disease based on SOLSSA-CatBoost model. <italic toggle="yes">Expert Syst. Appl.</italic><bold>219</bold>, 119648 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR11"><label>11.</label><citation-alternatives><element-citation id="ec-CR11" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Sandeep</surname><given-names>K</given-names></name><etal/></person-group><article-title>An interpretable approach with explainable AI for heart stroke prediction</article-title><source>Diagnostics</source><year>2024</year><volume>14</volume><fpage>128</fpage><pub-id pub-id-type="pmid">38248005</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/diagnostics14020128</pub-id><pub-id pub-id-type="pmcid">PMC10813874</pub-id></element-citation><mixed-citation id="mc-CR11" publication-type="journal">Sandeep, K. et al. An interpretable approach with explainable AI for heart stroke prediction. <italic toggle="yes">Diagnostics</italic><bold>14</bold>, 128 (2024).<pub-id pub-id-type="pmid">38248005</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.3390/diagnostics14020128</pub-id><pub-id pub-id-type="pmcid">PMC10813874</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Li</surname><given-names>C</given-names></name><etal/></person-group><article-title>Integrating EPSOSA-BP neural network algorithm for enhanced accuracy and robustness in optimizing coronary artery disease prediction</article-title><source>Sci. Rep.</source><year>2024</year><volume>14</volume><fpage>30993</fpage><pub-id pub-id-type="pmid">39730803</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-024-82184-2</pub-id><pub-id pub-id-type="pmcid">PMC11680579</pub-id></element-citation><mixed-citation id="mc-CR12" publication-type="journal">Li, C. et al. Integrating EPSOSA-BP neural network algorithm for enhanced accuracy and robustness in optimizing coronary artery disease prediction. <italic toggle="yes">Sci. Rep.</italic><bold>14</bold>, 30993 (2024).<pub-id pub-id-type="pmid">39730803</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1038/s41598-024-82184-2</pub-id><pub-id pub-id-type="pmcid">PMC11680579</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bikku</surname><given-names>T</given-names></name></person-group><article-title>Multi-layered deep learning perceptron approach for health risk prediction</article-title><source>J. Big Data</source><year>2020</year><volume>7</volume><fpage>50</fpage></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Bikku, T. Multi-layered deep learning perceptron approach for health risk prediction. <italic toggle="yes">J. Big Data</italic>. <bold>7</bold>, 50 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>G&#225;rate-Escamila</surname><given-names>AK</given-names></name><name name-style="western"><surname>Hassani</surname><given-names>AHE</given-names></name><name name-style="western"><surname>Andr&#232;s</surname><given-names>E</given-names></name></person-group><article-title>Classification models for heart disease prediction using feature selection and PCA</article-title><source>Inf. Med. Unlocked</source><year>2020</year><volume>19</volume><fpage>100330</fpage></element-citation><mixed-citation id="mc-CR14" publication-type="journal">G&#225;rate-Escamila, A. K., Hassani, A. H. E. &amp; Andr&#232;s, E. Classification models for heart disease prediction using feature selection and PCA. <italic toggle="yes">Inf. Med. Unlocked</italic>. <bold>19</bold>, 100330 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ali</surname><given-names>F</given-names></name><etal/></person-group><article-title>A smart healthcare monitoring system for heart disease prediction based on ensemble deep learning and feature fusion</article-title><source>Inform. Fusion</source><year>2020</year><volume>63</volume><fpage>208</fpage><lpage>222</lpage></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Ali, F. et al. A smart healthcare monitoring system for heart disease prediction based on ensemble deep learning and feature fusion. <italic toggle="yes">Inform. Fusion</italic>. <bold>63</bold>, 208&#8211;222 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><mixed-citation publication-type="other">Reddy, S. S., Nilambar, S. &amp; Rajender, R. Risk assessment of myocardial infarction for diabetics through multi-aspects computing. <italic toggle="yes">EAI Endorsed Trans. Pervasive Health Technol.</italic><bold>6</bold>(24), e3 (2020).</mixed-citation></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shah</surname><given-names>SMS</given-names></name><etal/></person-group><article-title>Support vector machines-based heart disease diagnosis using feature subset, wrapping selection and extraction methods</article-title><source>Comput. Electr. Eng.</source><year>2020</year><volume>84</volume><fpage>106628</fpage></element-citation><mixed-citation id="mc-CR17" publication-type="journal">Shah, S. M. S. et al. Support vector machines-based heart disease diagnosis using feature subset, wrapping selection and extraction methods. <italic toggle="yes">Comput. Electr. Eng.</italic><bold>84</bold>, 106628 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Masetic</surname><given-names>Z</given-names></name><name name-style="western"><surname>Subasi</surname><given-names>A</given-names></name></person-group><article-title>Congestive heart failure detection using random forest classifier</article-title><source>Comput. Methods Programs Biomed.</source><year>2016</year><volume>130</volume><fpage>54</fpage><lpage>64</lpage><pub-id pub-id-type="pmid">27208521</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.cmpb.2016.03.020</pub-id></element-citation><mixed-citation id="mc-CR18" publication-type="journal">Masetic, Z. &amp; Subasi, A. Congestive heart failure detection using random forest classifier. <italic toggle="yes">Comput. Methods Programs Biomed.</italic><bold>130</bold>, 54&#8211;64 (2016).<pub-id pub-id-type="pmid">27208521</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.cmpb.2016.03.020</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liu</surname><given-names>X</given-names></name><etal/></person-group><article-title>A hybrid genetic algorithm with wrapper-embedded approaches for feature selection</article-title><source>IEEE Access.</source><year>2018</year><volume>6</volume><fpage>22863</fpage><lpage>22874</lpage></element-citation><mixed-citation id="mc-CR19" publication-type="journal">Liu, X. et al. A hybrid genetic algorithm with wrapper-embedded approaches for feature selection. <italic toggle="yes">IEEE Access.</italic><bold>6</bold>, 22863&#8211;22874 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mommersteeg</surname><given-names>PMC</given-names></name><etal/></person-group><article-title>Psychosocial distress and health status as risk factors for ten-year major adverse cardiac events and mortality in patients with non-obstructive coronary artery disease</article-title><source>Int. J. Cardiol.</source><year>2024</year><volume>406</volume><fpage>132062</fpage><pub-id pub-id-type="pmid">38643796</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.ijcard.2024.132062</pub-id></element-citation><mixed-citation id="mc-CR20" publication-type="journal">Mommersteeg, P. M. C. et al. Psychosocial distress and health status as risk factors for ten-year major adverse cardiac events and mortality in patients with non-obstructive coronary artery disease. <italic toggle="yes">Int. J. Cardiol.</italic><bold>406</bold>, 132062 (2024).<pub-id pub-id-type="pmid">38643796</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.ijcard.2024.132062</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nakanishi</surname><given-names>R</given-names></name><etal/></person-group><article-title>Machine learning in predicting coronary heart disease and cardiovascular disease events: results from the multi-ethnic study of atherosclerosis (MESA)</article-title><source>J. Am. Coll. Cardiol.</source><year>2018</year><volume>71</volume><issue>11</issue><fpage>1483</fpage></element-citation><mixed-citation id="mc-CR21" publication-type="journal">Nakanishi, R. et al. Machine learning in predicting coronary heart disease and cardiovascular disease events: Results from the multi-ethnic study of atherosclerosis (MESA). <italic toggle="yes">J. Am. Coll. Cardiol.</italic><bold>71</bold> (11), 1483 (2018).29598870
</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pezel</surname><given-names>T</given-names></name><etal/></person-group><article-title>Machine learning model using CMR to predict cardiovascular events in asymptomatic patients with known CAD</article-title><source>J. Cardiovasc. Magn. Reson.</source><year>2025</year><volume>27</volume><fpage>101270</fpage></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Pezel, T. et al. Machine learning model using CMR to predict cardiovascular events in asymptomatic patients with known CAD. <italic toggle="yes">J. Cardiovasc. Magn. Reson.</italic><bold>27</bold>, 101270 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ghasemi</surname><given-names>P</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J</given-names></name></person-group><article-title>Unsupervised feature selection to identify important ICD-10 and ATC codes for machine learning on a cohort of patients with coronary heart disease: retrospective study</article-title><source>JMIR Med. Inf.</source><year>2024</year><volume>12</volume><fpage>e52896</fpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.2196/52896</pub-id><pub-id pub-id-type="pmcid">PMC11295113</pub-id><pub-id pub-id-type="pmid">39087585</pub-id></element-citation><mixed-citation id="mc-CR23" publication-type="journal">Ghasemi, P. &amp; Lee, J. Unsupervised feature selection to identify important ICD-10 and ATC codes for machine learning on a cohort of patients with coronary heart disease: Retrospective study. <italic toggle="yes">JMIR Med. Inf.</italic><bold>12</bold>, e52896 (2024).<pub-id pub-id-type="doi" assigning-authority="pmc">10.2196/52896</pub-id><pub-id pub-id-type="pmcid">PMC11295113</pub-id><pub-id pub-id-type="pmid">39087585</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Majdi</surname><given-names>MM</given-names></name><name name-style="western"><surname>Seyedali</surname><given-names>M</given-names></name></person-group><article-title>Hybrid Whale optimization algorithm with simulated annealing for feature selection</article-title><source>Neurocomputing</source><year>2017</year><volume>260</volume><fpage>302</fpage><lpage>312</lpage></element-citation><mixed-citation id="mc-CR24" publication-type="journal">Majdi, M. M. &amp; Seyedali, M. Hybrid Whale optimization algorithm with simulated annealing for feature selection. <italic toggle="yes">Neurocomputing</italic><bold>260</bold>, 302&#8211;312 (2017).</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Thabtah</surname><given-names>F</given-names></name><etal/></person-group><article-title>Least loss: A simplified filter method for feature selection</article-title><source>Inf. Sci.</source><year>2020</year><volume>534</volume><fpage>1</fpage><lpage>15</lpage></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Thabtah, F. et al. Least loss: A simplified filter method for feature selection. <italic toggle="yes">Inf. Sci.</italic><bold>534</bold>, 1&#8211;15 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hassan</surname><given-names>M</given-names></name><etal/></person-group><article-title>Efficient prediction of coronary artery disease using machine learning algorithms with feature selection techniques</article-title><source>Comput. Electr. Eng.</source><year>2024</year><volume>115</volume><fpage>109130</fpage></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Hassan, M. et al. Efficient prediction of coronary artery disease using machine learning algorithms with feature selection techniques. <italic toggle="yes">Comput. Electr. Eng.</italic><bold>115</bold>, 109130 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><citation-alternatives><element-citation id="ec-CR27" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>MD</given-names></name><etal/></person-group><article-title>Risk assessment of atherosclerotic cardiovascular disease based on feature selection of L1-CBFS</article-title><source>Biomed. Signal Process. Control</source><year>2024</year><volume>91</volume><fpage>106062</fpage></element-citation><mixed-citation id="mc-CR27" publication-type="journal">Yang, M. D. et al. Risk assessment of atherosclerotic cardiovascular disease based on feature selection of L1-CBFS. <italic toggle="yes">Biomed. Signal Process. Control</italic>. <bold>91</bold>, 106062 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR28"><label>28.</label><citation-alternatives><element-citation id="ec-CR28" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Shen</surname><given-names>L</given-names></name><etal/></person-group><article-title>Machine learning-based predictive models for perioperative major adverse cardiovascular events in patients with stable coronary artery disease undergoing noncardiac surgery</article-title><source>Comput. Methods Programs Biomed.</source><year>2025</year><volume>260</volume><fpage>108561</fpage><pub-id pub-id-type="pmid">39708562</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.cmpb.2024.108561</pub-id></element-citation><mixed-citation id="mc-CR28" publication-type="journal">Shen, L. et al. Machine learning-based predictive models for perioperative major adverse cardiovascular events in patients with stable coronary artery disease undergoing noncardiac surgery. <italic toggle="yes">Comput. Methods Programs Biomed.</italic><bold>260</bold>, 108561 (2025).<pub-id pub-id-type="pmid">39708562</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1016/j.cmpb.2024.108561</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR29"><label>29.</label><citation-alternatives><element-citation id="ec-CR29" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J</given-names></name><etal/></person-group><article-title>Comparison of LASSO and random forest models for predicting the risk of premature coronary artery disease</article-title><source>BMC Med. Inf. Decis. Mak.</source><year>2023</year><volume>23</volume><fpage>297</fpage><pub-id pub-id-type="doi" assigning-authority="pmc">10.1186/s12911-023-02407-w</pub-id><pub-id pub-id-type="pmcid">PMC10734117</pub-id><pub-id pub-id-type="pmid">38124036</pub-id></element-citation><mixed-citation id="mc-CR29" publication-type="journal">Wang, J. et al. Comparison of LASSO and random forest models for predicting the risk of premature coronary artery disease. <italic toggle="yes">BMC Med. Inf. Decis. Mak.</italic><bold>23</bold>, 297 (2023).<pub-id pub-id-type="doi" assigning-authority="pmc">10.1186/s12911-023-02407-w</pub-id><pub-id pub-id-type="pmcid">PMC10734117</pub-id><pub-id pub-id-type="pmid">38124036</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR30"><label>30.</label><citation-alternatives><element-citation id="ec-CR30" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ye</surname><given-names>Z</given-names></name><etal/></person-group><article-title>The prediction of in-hospital mortality in chronic kidney disease patients with coronary artery disease using machine learning models</article-title><source>Eur. J. Med. Res.</source><year>2023</year><volume>28</volume><fpage>33</fpage><pub-id pub-id-type="pmid">36653875</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1186/s40001-023-00995-x</pub-id><pub-id pub-id-type="pmcid">PMC9847092</pub-id></element-citation><mixed-citation id="mc-CR30" publication-type="journal">Ye, Z. et al. The prediction of in-hospital mortality in chronic kidney disease patients with coronary artery disease using machine learning models. <italic toggle="yes">Eur. J. Med. Res.</italic><bold>28</bold>, 33 (2023).<pub-id pub-id-type="pmid">36653875</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1186/s40001-023-00995-x</pub-id><pub-id pub-id-type="pmcid">PMC9847092</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR31"><label>31.</label><mixed-citation publication-type="other">Satya Sree, K. P. N. V. et al. EMG controlled bionic robotic arm using artificial intelligence and machine learning. In <italic toggle="yes">Proceedings of the Fifth International Conference on IoT in Social, Mobile, Analytics and Cloud (I-SMAC)</italic>, Palladam, India, pp. 548&#8211;554 (2021).</mixed-citation></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chang</surname><given-names>D</given-names></name><etal/></person-group><article-title>Multiple strategies based grey Wolf optimizer for feature selection in performance evaluation of open-ended funds</article-title><source>Swarm Evol. Comput.</source><year>2024</year><volume>86</volume><fpage>101518</fpage></element-citation><mixed-citation id="mc-CR32" publication-type="journal">Chang, D. et al. Multiple strategies based grey Wolf optimizer for feature selection in performance evaluation of open-ended funds. <italic toggle="yes">Swarm Evol. Comput.</italic><bold>86</bold>, 101518 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mamatha</surname><given-names>B</given-names></name><name name-style="western"><surname>Terdal</surname><given-names>SP</given-names></name></person-group><article-title>An effective role-oriented binary Walrus grey Wolf approach for feature selection in early-stage chronic kidney disease detection</article-title><source>Int. Urol. Nephrol.</source><year>2024</year><volume>56</volume><fpage>3133</fpage><lpage>3154</lpage><pub-id pub-id-type="pmid">38748365</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s11255-024-04067-9</pub-id></element-citation><mixed-citation id="mc-CR33" publication-type="journal">Mamatha, B. &amp; Terdal, S. P. An effective role-oriented binary Walrus grey Wolf approach for feature selection in early-stage chronic kidney disease detection. <italic toggle="yes">Int. Urol. Nephrol.</italic><bold>56</bold>, 3133&#8211;3154 (2024).<pub-id pub-id-type="pmid">38748365</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s11255-024-04067-9</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><mixed-citation publication-type="other">Bikku, T. et al. Optimizing gene expression analysis using clustering algorithms. In <italic toggle="yes">Proceedings of the Fifth International Conference on Computer and Communication Technologies</italic> pp. 163&#8211;171 (Springer, 2024).</mixed-citation></ref><ref id="CR35"><label>35.</label><citation-alternatives><element-citation id="ec-CR35" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>CJ</given-names></name><name name-style="western"><surname>Lin</surname><given-names>H</given-names></name><name name-style="western"><surname>Liu</surname><given-names>M</given-names></name></person-group><article-title>Design of comprehensive evaluation index system for P2P credit risk of three rural borrowers</article-title><source>Soft. Comput.</source><year>2020</year><volume>24</volume><fpage>11493</fpage><lpage>11509</lpage></element-citation><mixed-citation id="mc-CR35" publication-type="journal">Rao, C. J., Lin, H. &amp; Liu, M. Design of comprehensive evaluation index system for P2P credit risk of three rural borrowers. <italic toggle="yes">Soft. Comput.</italic><bold>24</bold>, 11493&#8211;11509 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR36"><label>36.</label><citation-alternatives><element-citation id="ec-CR36" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Belouch</surname><given-names>M</given-names></name><name name-style="western"><surname>Elhadaj</surname><given-names>S</given-names></name><name name-style="western"><surname>Idhammad</surname><given-names>M</given-names></name></person-group><article-title>A hybrid filter-wrapper feature selection method for DDoS detection in cloud computing</article-title><source>Intell. Data Anal.</source><year>2018</year><volume>22</volume><issue>6</issue><fpage>1209</fpage><lpage>1226</lpage></element-citation><mixed-citation id="mc-CR36" publication-type="journal">Belouch, M., Elhadaj, S. &amp; Idhammad, M. A hybrid filter-wrapper feature selection method for DDoS detection in cloud computing. <italic toggle="yes">Intell. Data Anal.</italic><bold>22</bold> (6), 1209&#8211;1226 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR37"><label>37.</label><citation-alternatives><element-citation id="ec-CR37" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hassan</surname><given-names>A</given-names></name><etal/></person-group><article-title>A wrapper feature selection approach using Markov blankets</article-title><source>Pattern Recogn.</source><year>2025</year><volume>158</volume><fpage>111069</fpage></element-citation><mixed-citation id="mc-CR37" publication-type="journal">Hassan, A. et al. A wrapper feature selection approach using Markov blankets. <italic toggle="yes">Pattern Recogn.</italic><bold>158</bold>, 111069 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR38"><label>38.</label><citation-alternatives><element-citation id="ec-CR38" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>CJ</given-names></name><name name-style="western"><surname>Liu</surname><given-names>Y</given-names></name><name name-style="western"><surname>Goh</surname><given-names>M</given-names></name></person-group><article-title>Credit risk assessment mechanism of personal auto loan based on PSO-XGBoost model</article-title><source>Complex. Intell. Syst.</source><year>2023</year><volume>9</volume><issue>2</issue><fpage>1391</fpage><lpage>1414</lpage></element-citation><mixed-citation id="mc-CR38" publication-type="journal">Rao, C. J., Liu, Y. &amp; Goh, M. Credit risk assessment mechanism of personal auto loan based on PSO-XGBoost model. <italic toggle="yes">Complex. Intell. Syst.</italic><bold>9</bold> (2), 1391&#8211;1414 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR39"><label>39.</label><citation-alternatives><element-citation id="ec-CR39" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>C</given-names></name><etal/></person-group><article-title>Ensemble feature selection in medical datasets: combining filter, wrapper, and embedded feature selection results</article-title><source>Expert Syst.</source><year>2020</year><volume>37</volume><fpage>1</fpage><lpage>10</lpage></element-citation><mixed-citation id="mc-CR39" publication-type="journal">Chen, C. et al. Ensemble feature selection in medical datasets: Combining filter, wrapper, and embedded feature selection results. <italic toggle="yes">Expert Syst.</italic><bold>37</bold>, 1&#8211;10 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR40"><label>40.</label><citation-alternatives><element-citation id="ec-CR40" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Mohd Faizal</surname><given-names>AS</given-names></name><etal/></person-group><article-title>A biomarker discovery of acute myocardial infarction using feature selection and machine learning</article-title><source>Med. Biol. Eng. Comput.</source><year>2023</year><volume>61</volume><fpage>2527</fpage><lpage>2541</lpage><pub-id pub-id-type="pmid">37199891</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s11517-023-02841-y</pub-id><pub-id pub-id-type="pmcid">PMC10191821</pub-id></element-citation><mixed-citation id="mc-CR40" publication-type="journal">Mohd Faizal, A. S. et al. A biomarker discovery of acute myocardial infarction using feature selection and machine learning. <italic toggle="yes">Med. Biol. Eng. Comput.</italic><bold>61</bold>, 2527&#8211;2541 (2023).<pub-id pub-id-type="pmid">37199891</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1007/s11517-023-02841-y</pub-id><pub-id pub-id-type="pmcid">PMC10191821</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR41"><label>41.</label><citation-alternatives><element-citation id="ec-CR41" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rao</surname><given-names>CJ</given-names></name><etal/></person-group><article-title>Oversampling method via adaptive double weights and Gaussian kernel function for the transformation of unbalanced data in risk assessment of cardiovascular disease</article-title><source>Inf. Sci.</source><year>2024</year><volume>665</volume><fpage>120410</fpage></element-citation><mixed-citation id="mc-CR41" publication-type="journal">Rao, C. J. et al. Oversampling method via adaptive double weights and Gaussian kernel function for the transformation of unbalanced data in risk assessment of cardiovascular disease. <italic toggle="yes">Inf. Sci.</italic><bold>665</bold>, 120410 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR42"><label>42.</label><citation-alternatives><element-citation id="ec-CR42" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Meng</surname><given-names>DX</given-names></name><name name-style="western"><surname>Li</surname><given-names>YJ</given-names></name></person-group><article-title>An imbalanced learning method by combining SMOTE with center offset factor</article-title><source>Appl. Soft Comput.</source><year>2022</year><volume>120</volume><fpage>108618</fpage></element-citation><mixed-citation id="mc-CR42" publication-type="journal">Meng, D. X. &amp; Li, Y. J. An imbalanced learning method by combining SMOTE with center offset factor. <italic toggle="yes">Appl. Soft Comput.</italic><bold>120</bold>, 108618 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR43"><label>43.</label><citation-alternatives><element-citation id="ec-CR43" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Liaw</surname><given-names>LCM</given-names></name><etal/></person-group><article-title>A histogram SMOTE-based sampling algorithm with incremental learning for imbalanced data classification</article-title><source>Inf. Sci.</source><year>2025</year><volume>686</volume><fpage>121193</fpage></element-citation><mixed-citation id="mc-CR43" publication-type="journal">Liaw, L. C. M. et al. A histogram SMOTE-based sampling algorithm with incremental learning for imbalanced data classification. <italic toggle="yes">Inf. Sci.</italic><bold>686</bold>, 121193 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR44"><label>44.</label><citation-alternatives><element-citation id="ec-CR44" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Do&#287;an</surname><given-names>O</given-names></name><name name-style="western"><surname>Ta&#351;p&#305;nar</surname><given-names>S</given-names></name><name name-style="western"><surname>Bera</surname><given-names>AK</given-names></name></person-group><article-title>A bayesian robust chi-squared test for testing simple hypotheses</article-title><source>J. Econ.</source><year>2021</year><volume>222</volume><issue>2</issue><fpage>933</fpage><lpage>958</lpage></element-citation><mixed-citation id="mc-CR44" publication-type="journal">Do&#287;an, O., Ta&#351;p&#305;nar, S. &amp; Bera, A. K. A bayesian robust chi-squared test for testing simple hypotheses. <italic toggle="yes">J. Econ.</italic><bold>222</bold> (2), 933&#8211;958 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR45"><label>45.</label><citation-alternatives><element-citation id="ec-CR45" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Luna-Romera</surname><given-names>JM</given-names></name><etal/></person-group><article-title>External clustering validity index based on chi-squared statistical test</article-title><source>Inf. Sci.</source><year>2019</year><volume>487</volume><fpage>1</fpage><lpage>17</lpage></element-citation><mixed-citation id="mc-CR45" publication-type="journal">Luna-Romera, J. M. et al. External clustering validity index based on chi-squared statistical test. <italic toggle="yes">Inf. Sci.</italic><bold>487</bold>, 1&#8211;17 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR46"><label>46.</label><citation-alternatives><element-citation id="ec-CR46" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>WC</given-names></name><etal/></person-group><article-title>Mutual information-driven self-supervised point cloud pre-training</article-title><source>Knowl. Based Syst.</source><year>2025</year><volume>307</volume><fpage>112741</fpage></element-citation><mixed-citation id="mc-CR46" publication-type="journal">Xu, W. C. et al. Mutual information-driven self-supervised point cloud pre-training. <italic toggle="yes">Knowl. Based Syst.</italic><bold>307</bold>, 112741 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR47"><label>47.</label><citation-alternatives><element-citation id="ec-CR47" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Huang</surname><given-names>SL</given-names></name><etal/></person-group><article-title>A fisher score-based multi-instance learning method assisted by mixture of factor analysis</article-title><source>Neurocomputing</source><year>2022</year><volume>507</volume><fpage>358</fpage><lpage>368</lpage></element-citation><mixed-citation id="mc-CR47" publication-type="journal">Huang, S. L. et al. A fisher score-based multi-instance learning method assisted by mixture of factor analysis. <italic toggle="yes">Neurocomputing</italic><bold>507</bold>, 358&#8211;368 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR48"><label>48.</label><citation-alternatives><element-citation id="ec-CR48" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>J</given-names></name><etal/></person-group><article-title>Risk assessment of coronary heart disease based on cloud-random forest</article-title><source>Artif. Intell. Rev.</source><year>2023</year><volume>56</volume><fpage>203</fpage><lpage>232</lpage></element-citation><mixed-citation id="mc-CR48" publication-type="journal">Wang, J. et al. Risk assessment of coronary heart disease based on cloud-random forest. <italic toggle="yes">Artif. Intell. Rev.</italic><bold>56</bold>, 203&#8211;232 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR49"><label>49.</label><citation-alternatives><element-citation id="ec-CR49" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Yang</surname><given-names>XS</given-names></name><name name-style="western"><surname>Deb</surname><given-names>S</given-names></name></person-group><article-title>Engineering optimisation by cuckoo search</article-title><source>Int. J. Math. Modelling Numer. Optimisation</source><year>2010</year><volume>1</volume><issue>4</issue><fpage>330</fpage><lpage>343</lpage></element-citation><mixed-citation id="mc-CR49" publication-type="journal">Yang, X. S. &amp; Deb, S. Engineering optimisation by cuckoo search. <italic toggle="yes">Int. J. Math. Model. Numer. Optim</italic>. <bold>1</bold> (4), 330&#8211;343 (2010).</mixed-citation></citation-alternatives></ref><ref id="CR50"><label>50.</label><citation-alternatives><element-citation id="ec-CR50" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Koc</surname><given-names>I</given-names></name></person-group><article-title>A comprehensive analysis of grid-based wind turbine layout using an efficient binary invasive weed optimization algorithm with levy flight</article-title><source>Expert Syst. Appl.</source><year>2022</year><volume>198</volume><fpage>116835</fpage></element-citation><mixed-citation id="mc-CR50" publication-type="journal">Koc, I. A comprehensive analysis of grid-based wind turbine layout using an efficient binary invasive weed optimization algorithm with levy flight. <italic toggle="yes">Expert Syst. Appl.</italic><bold>198</bold>, 116835 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR51"><label>51.</label><citation-alternatives><element-citation id="ec-CR51" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Alqarni</surname><given-names>M</given-names></name><name name-style="western"><surname>Cherif</surname><given-names>A</given-names></name><name name-style="western"><surname>Alkayyal</surname><given-names>E</given-names></name></person-group><article-title>ODM-BCSA: an offloading decision-making framework based on binary cuckoo search algorithm for mobile edge computing</article-title><source>Comput. Netw.</source><year>2023</year><volume>226</volume><fpage>109647</fpage></element-citation><mixed-citation id="mc-CR51" publication-type="journal">Alqarni, M., Cherif, A. &amp; Alkayyal, E. ODM-BCSA: An offloading decision-making framework based on binary cuckoo search algorithm for mobile edge computing. <italic toggle="yes">Comput. Netw.</italic><bold>226</bold>, 109647 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR52"><label>52.</label><citation-alternatives><element-citation id="ec-CR52" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>ZY</given-names></name><etal/></person-group><article-title>A novel particle swarm optimization algorithm with l&#233;vy flight and orthogonal learning</article-title><source>Swarm Evol. Comput.</source><year>2022</year><volume>75</volume><fpage>101207</fpage></element-citation><mixed-citation id="mc-CR52" publication-type="journal">Wang, Z. Y. et al. A novel particle swarm optimization algorithm with l&#233;vy flight and orthogonal learning. <italic toggle="yes">Swarm Evol. Comput.</italic><bold>75</bold>, 101207 (2022).</mixed-citation></citation-alternatives></ref></ref-list></back></article>
        
    </metadata>
</record>
    </GetRecord>

</OAI-PMH>