


<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
    <responseDate>2025-09-09T14:00:37Z</responseDate>
    <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:12406497" metadataPrefix="pmc">https://pmc.ncbi.nlm.nih.gov/api/oai/v1/mh/</request>
    
    <GetRecord>
        <record>
    <header>
    <identifier>oai:pubmedcentral.nih.gov:12406497</identifier>
    <datestamp>2025-09-04</datestamp>
    
        
        <setSpec>frontai</setSpec>
        
    
        
        <setSpec>pmc-open</setSpec>
        
    
</header>
    <metadata>
        
        <article xmlns="https://jats.nlm.nih.gov/ns/archiving/1.4/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.4/ https://jats.nlm.nih.gov/archiving/1.4/xsd/JATS-archivearticle1-4.xsd" xml:lang="en" article-type="research-article" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Front Artif Intell</journal-id><journal-id journal-id-type="iso-abbrev">Front Artif Intell</journal-id><journal-id journal-id-type="pmc-domain-id">3981</journal-id><journal-id journal-id-type="pmc-domain">frontai</journal-id><journal-id journal-id-type="publisher-id">Front. Artif. Intell.</journal-id><journal-title-group><journal-title>Frontiers in Artificial Intelligence</journal-title></journal-title-group><issn pub-type="epub">2624-8212</issn><publisher><publisher-name>Frontiers Media SA</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12406497</article-id><article-id pub-id-type="pmcid-ver">PMC12406497.1</article-id><article-id pub-id-type="pmcaid">12406497</article-id><article-id pub-id-type="pmcaiid">12406497</article-id><article-id pub-id-type="pmid">40910114</article-id><article-id pub-id-type="doi">10.3389/frai.2025.1605539</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Artificial Intelligence</subject><subj-group><subject>Original Research</subject></subj-group></subj-group></article-categories><title-group><article-title>Graph neural networks with configuration cross-attention for tensor compilers</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name name-style="western"><surname>Khizbullin</surname><given-names initials="D">Dmitrii</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><xref rid="c001" ref-type="corresp">
<sup>*</sup>
</xref><uri xlink:href="http://loop.frontiersin.org/people/2809127/overview"/><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/><role content-type="https://credit.niso.org/contributor-roles/funding-acquisition/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-original-draft/"/></contrib><contrib contrib-type="author"><name name-style="western"><surname>de Andrade</surname><given-names initials="ER">Eduardo Rocha</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/methodology/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/></contrib><contrib contrib-type="author"><name name-style="western"><surname>Nguyen</surname><given-names initials="TH">Thanh Hau</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/></contrib><contrib contrib-type="author"><name name-style="western"><surname>Ferreira</surname><given-names initials="MP">Matheus Pedroza</given-names></name><xref rid="aff2" ref-type="aff">
<sup>2</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/conceptualization/"/><role content-type="https://credit.niso.org/contributor-roles/validation/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/><role content-type="https://credit.niso.org/contributor-roles/investigation/"/><role content-type="https://credit.niso.org/contributor-roles/software/"/><role content-type="https://credit.niso.org/contributor-roles/visualization/"/><role content-type="https://credit.niso.org/contributor-roles/data-curation/"/></contrib><contrib contrib-type="author"><name name-style="western"><surname>Pugh</surname><given-names initials="DR">David R.</given-names></name><xref rid="aff1" ref-type="aff">
<sup>1</sup>
</xref><role content-type="https://credit.niso.org/contributor-roles/supervision/"/><role content-type="https://credit.niso.org/contributor-roles/writing-review-editing/"/></contrib></contrib-group><aff id="aff1"><sup>1</sup><institution>King Abdullah University of Science and Technology (KAUST)</institution>, <addr-line>Thuwal</addr-line>, <country>Saudi Arabia</country></aff><aff id="aff2"><sup>2</sup><institution>Sprout.ai</institution>, <addr-line>London</addr-line>, <country>United Kingdom</country></aff><author-notes><fn fn-type="edited-by"><p>Edited by: Ming-Feng Ge, China University of Geosciences Wuhan, China</p></fn><fn fn-type="edited-by"><p>Reviewed by: Keith Ulmer, University of Colorado Boulder, United States</p><p>Dengke Han, Chinese Academy of Sciences (CAS), China</p></fn><corresp id="c001">*Correspondence: Dmitrii Khizbullin <email>dmitrii.khizbullin@kaust.edu.sa</email></corresp></author-notes><pub-date pub-type="epub"><day>20</day><month>8</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>8</volume><issue-id pub-id-type="pmc-issue-id">481073</issue-id><elocation-id>1605539</elocation-id><history><date date-type="received"><day>03</day><month>4</month><year>2025</year></date><date date-type="accepted"><day>17</day><month>7</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>20</day><month>08</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-05 10:25:38.007"><day>05</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>Copyright &#169; 2025 Khizbullin, de Andrade, Nguyen, Ferreira and Pugh.</copyright-statement><copyright-year>2025</copyright-year><copyright-holder>Khizbullin, de Andrade, Nguyen, Ferreira and Pugh</copyright-holder><license><ali:license_ref specific-use="textmining" content-type="ccbylicense">https://creativecommons.org/licenses/by/4.0/</ali:license_ref><license-p>This is an open-access article distributed under the terms of the Creative Commons Attribution License (CC BY). The use, distribution or reproduction in other forums is permitted, provided the original author(s) and the copyright owner(s) are credited and that the original publication in this journal is cited, in accordance with accepted academic practice. No use, distribution or reproduction is permitted which does not comply with these terms.</license-p></license></permissions><self-uri content-type="pmc-pdf" xlink:href="frai-08-1605539.pdf"/><abstract><p>With the recent popularity of neural networks comes the need for efficient serving of inference workloads. A neural network inference workload can be represented as a computational graph with nodes as operators transforming multidimensional tensors. The tensors can be transposed and/or tiled in a combinatorially large number of ways, some configurations leading to accelerated inference. We propose TGraph, a neural graph architecture that allows screening for fast configurations of the target computational graph, thus representing an artificial intelligence (AI) tensor compiler in contrast to traditional heuristic-based compilers. The proposed solution improves mean Kendall's &#964; across layout collections of TpuGraphs from 29.8% of the reliable baseline to 67.4% of TGraph. We estimate the potential CO<sub>2</sub> emission reduction associated with our work to be equivalent to over 50% of the total household emissions in the areas hosting AI-oriented data centers.</p></abstract><kwd-group><kwd>graph neural network (GNN)</kwd><kwd>tensor compilation</kwd><kwd>attention mechanism</kwd><kwd>ranking loss function</kwd><kwd>machine learning for systems</kwd></kwd-group><funding-group><funding-statement>The author(s) declare that financial support was received for the research and/or publication of this article. This work was supported by the SDAIA-KAUST Center of Excellence in Data Science and Artificial Intelligence (SDAIA-KAUST AI). This work was supported by the prizes of the 1st and the 2nd winning places of &#8220;Google - Fast or Slow? Predict AI Model Runtime&#8221; Kaggle competition. The funder was not involved in the study design, collection, analysis, interpretation of data, the writing of this article, or the decision to submit it for publication.</funding-statement></funding-group><counts><fig-count count="3"/><table-count count="3"/><equation-count count="10"/><ref-count count="35"/><page-count count="9"/><word-count count="6225"/></counts><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>section-at-acceptance</meta-name><meta-value>Machine Learning and Artificial Intelligence</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec sec-type="intro" id="s1"><title>1 Introduction</title><p>Machine learning (ML) continues to gain popularity in solving engineering tasks, including Large Language Models for natural language processing, convolutional and transformer models for computer vision, recommendation models in online services, etc. (<xref rid="B19" ref-type="bibr">Minaee et al., 2024</xref>; <xref rid="B24" ref-type="bibr">Pugliese et al., 2021</xref>). Most of the computing associated with ML is done by serving ML models for inference rather than training them (<xref rid="B8" ref-type="bibr">Desislavov et al., 2023</xref>). The need to reduce monetary costs as well as the CO<sub>2</sub> footprint of inference workloads leads to significant efforts in the optimization of computations (<xref rid="B3" ref-type="bibr">Bol&#243;n-Canedo et al., 2024</xref>; <xref rid="B29" ref-type="bibr">Tschand et al., 2025</xref>). Typically, ML workloads are launched on specialized accelerators: GPUs, TPUs (<xref rid="B14" ref-type="bibr">Jouppi et al., 2023</xref>), and others, which do not provide the same level of on-chip real-time optimization as CPUs do. Consequently, the complexity of optimization of computations for ML accelerators is shifted toward the compiler. Implementation of an enormous quantity of specialized kernels supporting the full matrix formed by a variety of accelerators times a variety of ML models seems intangible (<xref rid="B11" ref-type="bibr">Huyen, 2024</xref>). One solution to this problem is to employ ML-based tensor compilers.</p><sec><title>1.1 Related work</title><p>Several attempts have been made to build a highly efficient tensor compiler in recent years. Tensorflow (<xref rid="B1" ref-type="bibr">Abadi et al., 2016</xref>) has a rule-based XLA tensor program optimization engine <xref rid="B25" ref-type="bibr">Sabne (2020)</xref> that was studied by <xref rid="B27" ref-type="bibr">Snider and Liang (2023)</xref>. TVM (<xref rid="B6" ref-type="bibr">Chen et al., 2018a</xref>) introduces a Python-based meta-language to describe the computation and its execution schedule separately, allowing a range of automated optimizations mostly limited to one operator and avoiding operator (kernel) fusion. AutoTVM (<xref rid="B7" ref-type="bibr">Chen et al., 2018b</xref>) introduces the optimization of tensor programs based on gradient-boosted trees and TreeGRU and uses the ranking loss for model training rather than element-wise losses like MSE. PyTorch (<xref rid="B20" ref-type="bibr">Paszke et al., 2019</xref>), being a framework built with the imperative paradigm in mind, in its recent version, supports TorchScript, a just-in-time (JIT) compiled for the annotated functions and classes. JAX (<xref rid="B4" ref-type="bibr">Bradbury et al., 2018</xref>) as a functional meta-language natively supports JIT.</p><p>TASO (<xref rid="B12" ref-type="bibr">Jia et al., 2019</xref>) performs equivalent graph substitution as a way to fuse kernels. PET (<xref rid="B31" ref-type="bibr">Wang et al., 2021</xref>) then builds on top of TASO (<xref rid="B12" ref-type="bibr">Jia et al., 2019</xref>) to expand the search space to non-equivalent transformations and apply automatically generated correction kernels. DeepCuts (<xref rid="B15" ref-type="bibr">Jung et al., 2021</xref>), Ansor (<xref rid="B34" ref-type="bibr">Zheng et al., 2020</xref>), and TensorComp (<xref rid="B30" ref-type="bibr">Vasilache et al., 2018</xref>) rely on heuristics to solve the problem of efficient execution of a computational graph. NN-Meter (<xref rid="B33" ref-type="bibr">Zhang et al., 2021</xref>) presents a latency prediction model based on a combination of heuristics to account for the effects of kernel fusion and a random forest for single-operator latency prediction.</p><p>A significant fraction of the aforementioned works rely solely on heuristics and rules to compile a tensor program. Although the compilation time of a heuristic-based algorithm may be very small, it fails to achieve the absolute minimum of program runtime. In this work, we propose an algorithm based on machine learning to optimize a tensor program that is represented as a computational graph. The closest works to ours are <xref rid="B21" ref-type="bibr">Phothilimthana et al. (2020)</xref> and <xref rid="B32" ref-type="bibr">Xu et al. (2023)</xref> that use the same dataset and a benchmark TpuGraphs (<xref rid="B22" ref-type="bibr">Phothilimthana et al., 2023</xref>). Graph Segment Training (GST) (<xref rid="B5" ref-type="bibr">Cao et al., 2023</xref>) uses TpuGraphs as well but reports another metric, OPA, and does not provide a breakdown across the collections.</p><p>Apart from TpuGraphs, few datasets represent runtime measurements of computational graphs: Tenset (<xref rid="B35" ref-type="bibr">Zheng et al., 2021</xref>) and the dataset published by the authors of nn-meter <xref rid="B33" ref-type="bibr">Zhang et al. (2021)</xref>, while none of these explicitly organizes the node and edge attributes in a systematic way suitable for machine learning.</p><p>Traditional tensor compilers employ heuristic-based approaches to optimize computational graphs for hardware accelerators. The typical workflow involves several stages: graph analysis to identify optimization opportunities, operator fusion to combine adjacent operations into single kernels, memory layout optimization to improve data locality, and code generation targeting specific hardware. Heuristic compilers like XLA rely on predefined rules and patterns to make optimization decisions, such as choosing tensor layouts or tiling strategies based on operator types and tensor shapes. While these approaches offer fast compilation times and predictable behavior, they are inherently limited by the quality of hand-crafted rules and struggle to adapt to the diverse and rapidly evolving landscape of neural network architectures and hardware accelerators. Recent AI-based approaches aim to overcome these limitations by learning optimization strategies from data, using techniques such as reinforcement learning to explore the optimization space or supervised learning to predict the performance of different configurations. However, existing ML-based methods such as AutoTVM and Ansor focus primarily on single-operator optimization and fail to capture the complex inter-operator dependencies that arise in full computational graphs. Most critically, these approaches treat configuration optimization as an individual prediction problem, where each configuration is evaluated in isolation without explicit comparison to alternatives, ignoring the inherently relative nature of the optimization task. Our TGraph architecture addresses these limitations through two key innovations: (1) cross-configuration attention that enables explicit comparison between different configurations within the same batch, transforming the problem from individual prediction to learned ranking, and (2) a graph neural network architecture specifically designed for computational graphs with configurable nodes, allowing the model to capture both local operator behavior and global graph-level optimization opportunities while operating on pruned subgraphs for improved computational efficiency.</p></sec><sec><title>1.2 TpuGraphs dataset and benchmark details</title><p>The only publicly available dataset for the large-scale compiler configuration search is TpuGraphs (<xref rid="B22" ref-type="bibr">Phothilimthana et al., 2023</xref>). TpuGraphs contains execution times of an XLA's HLO graph with a specific compiler configuration on a Tensor Processing Unit (TPU v3). TpuGraphs focuses on optimizing tensor layouts and tensor tiling as compiler configurations. A tensor layout describes the order in which dimensions of a tensor are arranged or permuted in memory. Specifically, a layout is a 0-based sequence of dimension indices. A tensor tiling configuration defines how to partition a tensor into smaller sub-tensors or tiles by specifying the ranges of indices for each dimension, enabling efficient computation and memory locality. The tensor layout optimization dataset comprises 4 collections organized in a matrix shown in <xref rid="T1" ref-type="table">Table 1</xref>. The two groups of network architectures (<monospace>xla</monospace> and <monospace>nlp</monospace>) represent two distinct categories of workloads: <monospace>xla</monospace> - predominantly computer vision loads, while <monospace>nlp</monospace>&#8212;exclusively transformer-based natural language processing loads. Each architecture has up to 100,000 different tensor layout configurations and the associated runtimes recorded. The total number of unique architectures in <monospace>layout:xla</monospace> collections is 78 with the average number of configurations of over 11,000 (for <monospace>layout:xla::random</monospace>), and in <monospace>layout:nlp</monospace> collections&#8212;244 with the average number of configurations of over 66,000 (for <monospace>layout:nlp::random</monospace>). Another dimension across which the layout dataset is organized is the utilized configuration search strategy: random or genetic-algorithm-based (GA-based, denoted as Default). Even though the final goal is to be able to predict configurations' runtimes, during the dataset creation, some sort of bootstrapping search must be used. Random search gives very wide coverage across all the possible runtimes, whereas the GA-based search focuses more on sampling runtimes in the vicinity of the fastest runtime, making the task of runtime prediction harder and very challenging for the predictive model.</p><table-wrap position="float" id="T1" orientation="portrait"><label>Table 1</label><caption><p>The matrix of the 4 <monospace>Layout</monospace> collections.</p></caption><table frame="box" rules="all"><thead><tr style="background-color:#919498;color:#ffffff"><th valign="top" align="left" rowspan="2" colspan="1">
<bold>Group of graphs</bold>
</th><th valign="top" align="center" colspan="2" rowspan="1">
<bold>Configuration sampling strategy</bold>
</th></tr><tr style="background-color:#919498;color:#ffffff"><th valign="top" align="left" rowspan="1" colspan="1">
<bold>Random (uniform)</bold>
</th><th valign="top" align="left" rowspan="1" colspan="1">
<bold>Default (GA-based)</bold>
</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">XLA (CV, NLP and other)</td><td valign="top" align="left" rowspan="1" colspan="1">
<monospace>layout-xla-random</monospace>
</td><td valign="top" align="left" rowspan="1" colspan="1">
<monospace>layout-xla-default</monospace>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">NLP (Transformers)</td><td valign="top" align="left" rowspan="1" colspan="1">
<monospace>layout-nlp-random</monospace>
</td><td valign="top" align="left" rowspan="1" colspan="1">
<monospace>layout-nlp-default</monospace>
</td></tr></tbody></table></table-wrap><p>To illustrate the problem of configuration selection we provide an example on <xref rid="F1" ref-type="fig">Figure 1</xref>. Here, four elementary operations compose a computational graph, while only two of them, reshape and conv, are configurable. A tensor layout can be chosen by the compiler, and the choice results in potentially significantly different runtimes as a result of random or sequential memory access and deep specifics of a particular computational unit. More details can be found in <xref rid="B22" ref-type="bibr">Phothilimthana et al. (2023)</xref> Figure 3.</p><fig position="float" id="F1" orientation="portrait"><label>Figure 1</label><caption><p>An example of how different tensor layout configurations affect the runtime of the computational (sub-)graph. Configuration 1 is faster than and, consequently, superior to configuration 2.</p></caption><graphic position="float" orientation="portrait" xlink:href="frai-08-1605539-g0001.jpg"><alt-text>Diagram showing a data flow with operations: &#8221;add&#8221; and &#8221;max&#8221; inputs connected to &#8221;reshape&#8221; and &#8221;conv&#8221; processes. The reshape outputs 0 and the conv outputs 1,0. Below, a table shows runtime configurations comparing two setups, with the first configuration having a runtime of 1,222 milliseconds and the second 1,555 milliseconds.</alt-text></graphic></fig></sec><sec><title>1.3 Contribution summary</title><p>Our contributions can be summarized as follows:</p><list list-type="bullet"><list-item><p>We propose TGraph, a graph neural network (GNN) architecture with cross-channel and cross-configuration attention that achieves state-of-the-art on the TpuGraphs benchmark.</p></list-item><list-item><p>We show very efficient training and inference by applying non-configurable node pruning, configuration de-duplication, and compression.</p></list-item></list></sec><sec><title>1.4 Societal impact</title><p>We perform a case study to highlight the importance of data center AI workload optimization. According to our estimates the potential impact of this work can be reduction of CO<sub>2</sub> emissions equivalent to 50% (or higher) of household emissions in areas similar to North Virginia, VA. The details can be found in Section 2.6.</p></sec></sec><sec id="s2"><title>2 TGraph runtime ranking architecture</title><sec><title>2.1 Problem specification</title><p>We are looking to find the configuration <inline-formula><mml:math id="M1" overflow="scroll"><mml:mover accent="true"><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover></mml:math></inline-formula> that minimizes the tensor program runtime <italic toggle="yes">R</italic>(<italic toggle="yes">c</italic>) across the configuration space <italic toggle="yes">C</italic> for a specific computational graph.</p><disp-formula id="E1"><label>(1)</label><mml:math id="M2" overflow="scroll"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mover accent="true"><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo>~</mml:mo></mml:mover><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mrow><mml:mtext class="textrm" mathvariant="normal">argmin</mml:mtext></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>C</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>R</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>c</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>As we have only partial knowledge of R(c) in the form of benchmarked data, we are looking for a solution as an approximation <italic toggle="yes">R</italic><sub><italic toggle="yes">neural</italic></sub>(<italic toggle="yes">c</italic>) of the underlying true <italic toggle="yes">R</italic>(<italic toggle="yes">c</italic>).</p><p>The configuration space <italic toggle="yes">C</italic> can be described as &#8484;<sup><italic toggle="yes">N</italic></sup> where <italic toggle="yes">N</italic> is the number of discrete configurable variables (node and edge attributes) in a specific graph.</p></sec><sec><title>2.2 Data pre-processing</title><sec><title>2.2.1 Graph pruning</title><p>For layout collections, only Convolution, Dot, and Reshape nodes are configurable. Also, in most cases, the majority of nodes are identical across the configuration set. Thus, we adopt the following pruning strategy: for each graph, we only keep the nodes that are either configurable nodes themselves or are connected to a configurable node, i.e., input or output to a configurable node. By doing this, we transform a single graph into multiple (possibly disconnected) sub-graphs. The possibly disconnected graph does not pose a problem since TGraph has a global graph pooling layer as one of the final layers that fuses the sub-graph information. This way of graph pruning reduces the vRAM usage 4 times and speeds up training by a factor of 5 in some cases. An example of graph pruning is shown on <xref rid="F2" ref-type="fig">Figure 2</xref>.</p><fig position="float" id="F2" orientation="portrait"><label>Figure 2</label><caption><p>An example of node pruning. Nodes that are not connected to configurable nodes are removed (red nodes on the diagram). Two disconnected subgraphs are left after pruning.</p></caption><graphic position="float" orientation="portrait" xlink:href="frai-08-1605539-g0002.jpg"><alt-text>Flowchart illustrating a process with nodes labeled as &#8221;Non-configurable (pruned)&#8221; in red and &#8221;Configurable&#8221; in green. It shows pathways where some elements are retained and others pruned. The process branches into different sequences with labels indicating their configurable status.</alt-text></graphic></fig></sec><sec><title>2.2.2 Configuration deduplication</title><p>Most of the configuration sets for layout collections contain a lot of duplication. The runtime for the duplicated configuration sets can vary up to 0.4% of the mean value. Training on the same configuration sets but different runtime targets makes loss noisy and the training process less stable. Thus, we remove all the duplicated configuration sets for layout collections and leave the smallest runtime value for determinism.</p></sec><sec><title>2.2.3 Lossless configuration compression</title><p>Even with pruning and de-duplication, the RAM usage to load all configurations to the system memory for NLP collections is beyond the RAM capacity. We circumvent that issue by compressing <monospace>node_config_feat</monospace> beforehand and only decompressing it on the fly in the data loader after configuration sampling. This allows us to load all data to memory at the beginning of training, which reduces IO/CPU bottlenecks considerably and allows us to train faster. The compression is implemented based on the fact that each <monospace>node_config_feat</monospace> 6-dim vector (input, output, and kernel) can only have 7 possible values (-1, 0, 1, 2, 3, 4, 5) and, thus, can be represented by a single integer in base-7 (from 0 to 7<sup>6</sup>&#8722;1).</p></sec><sec><title>2.2.4 Changing the pad value in <monospace>node_feat</monospace></title><p>The features in <monospace>node_feat</monospace> are 0-padded. Whilst this is not a problem for most features, for others like <monospace>layout_minor_to_major_*</monospace>, this can be ambiguous since 0 is a valid axis index. Also, the <monospace>node_config_feat</monospace> are &#8722;1 padded, which makes it incompatible with <monospace>layout_minor_to_major_*</monospace> from <monospace>node_feat</monospace>. With that in mind, we re-generate <monospace>node_feat</monospace> with &#8722;1 padded, and this allows us to use a single embedding matrix for both <monospace>node_feat[134:]</monospace> and <monospace>node_config_feat</monospace>.</p></sec><sec><title>2.2.5 Data normalization, embedding and batching</title><p>For <monospace>layout</monospace>, the node features are formed as a 140-dimensional vector <monospace>node_feat</monospace> that represents various fields in an XLA's HLO instruction (a node in an HLO graph) either as they are, or as categorical values using one-hot encoding. We split <monospace>node_feat</monospace> into <monospace>node_feat[:134]</monospace> containing numerical and one-hot-encoded values and <monospace>node_feat[134:]</monospace> that contains the tensor index permutation of the output tensor layout (<monospace>layout_minor_to_major_*</monospace>). The former is normalized to element-wise 0-mean and unit standard deviation (<monospace>StandardScaler</monospace> on <xref rid="F3" ref-type="fig">Figure 3</xref>), while the latter, along with <monospace>node_config_feat</monospace>, is fed into a learned embedding matrix (4 channels). We find that the normalization is essential since <monospace>node_feat</monospace> has features like <monospace>*_sum</monospace> and <monospace>*_product</monospace> that can be very high in values compared to the rest of the features and, consequently, disrupt the optimization. Further, we find that the natural way to encode the permutation vectors is to embed them into a low-dimensional vector. For <monospace>node_opcode</monospace>, we also use a separate embedding layer with 16 channels. The input to the network is the concatenation of all aforementioned features. For each graph, we sample on the fly a batch of 64 (for <monospace>default</monospace> collections) or 128 (for <monospace>random</monospace> collections) configurations to form the input batch. For tile, on the other hand, we opt to use late fusion to integrate <monospace>config_feat</monospace> into the network.</p><fig position="float" id="F3" orientation="portrait"><label>Figure 3</label><caption><p>Architecture diagram of TGraph. <italic toggle="yes">n</italic><sub><italic toggle="yes">configs</italic></sub> is the number of configurations sampled into a batch. <italic toggle="yes">n</italic><sub><italic toggle="yes">nodes</italic></sub> is the number of nodes in the sampled graph after pruning.</p></caption><graphic position="float" orientation="portrait" xlink:href="frai-08-1605539-g0003.jpg"><alt-text>Flowchart illustrating a neural network model architecture. It starts with 'StandardScaler' for node features, followed by two embedding layers. These are concatenated, followed by a linear block, a graph convolution block repeated twice, and global average graph pooling. This leads to a linear transformation, outputting logits. A detailed view of the graph convolution block shows an instance normalization, SAGEConv, self-channel attention, cross-config attention, concatenation, GELU activation, and summation process.</alt-text></graphic></fig></sec></sec><sec><title>2.3 Architecture details</title><p>Following the reasoning laid out by (<xref rid="B21" ref-type="bibr">Phothilimthana et al. 2020</xref>), we employ GraphSAGE (<xref rid="B9" ref-type="bibr">Hamilton et al., 2017</xref>) as a basis of a graph convolutional block. GraphSage operation can be expressed as</p><disp-formula id="E2"><label>(2)</label><mml:math id="M3" overflow="scroll"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>L</mml:mi><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mtext class="textrm" mathvariant="normal">concat</mml:mtext><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:mstyle displaystyle="true"><mml:munder class="msub"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi><mml:mo>&#8712;</mml:mo><mml:mtext class="textrm" mathvariant="normal">neighbors</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:munder></mml:mstyle><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where <italic toggle="yes">i</italic> is the index of a node, <italic toggle="yes">k</italic> is the index of the layer, <inline-formula><mml:math id="M4" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mo>.</mml:mo><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula> - feedforward layers at the specific depth <italic toggle="yes">k</italic>, <italic toggle="yes">N</italic><sub><italic toggle="yes">L</italic>2</sub> - <italic toggle="yes">L</italic><sub>2</sub> normalization, <italic toggle="yes">neighbors</italic>(<italic toggle="yes">i</italic>) - a set of immediate neighbors of node <italic toggle="yes">i</italic>.</p><p>We construct the graph convolutional block that can be expressed in the following way.</p><disp-formula id="E3"><label>(3)</label><mml:math id="M5" overflow="scroll"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:msubsup><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#949;</mml:mi><mml:mo>+</mml:mo><mml:mi>a</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext class="textrm" mathvariant="normal">concat</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#951;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>&#951;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where <italic toggle="yes">a</italic> is GELU activation, <italic toggle="yes">A</italic><sub><italic toggle="yes">cross</italic></sub> - configuration cross-attention operation, and &#951;<sub><italic toggle="yes">i</italic></sub>(&#949;) is expressed as:</p><disp-formula id="E4"><label>(4)</label><mml:math id="M6" overflow="scroll"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>&#951;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>S</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>N</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi><mml:mi>s</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>n</mml:mi><mml:mi>c</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Here <italic toggle="yes">A</italic><sub><italic toggle="yes">self</italic></sub> is the self-attention operation described below, <italic toggle="yes">N</italic><sub><italic toggle="yes">instance</italic></sub> is instance normalization.</p><sec><title>2.3.1 Channel-wise self-attention</title><p>Inspired by the idea of Squeeze-and-Excitation (<xref rid="B10" ref-type="bibr">Hu et al., 2018</xref>), we add a channel-wise self-attention layer as a part of the graph convolutional block. We first apply a Linear layer to bottleneck the channel dimensions (8x reduction), followed by ReLU. Then, we apply a second linear layer to increase the channels again to the original value, followed by sigmoid. We finish by applying element-wise multiplication to the obtained feature map and the original input. The idea behind channel-wise self-attention is to capture the correlations between channels and use them to suppress less useful ones while enhancing the important ones.</p><disp-formula id="E5"><label>(5)</label><mml:math id="M7" overflow="scroll"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi><mml:mi>f</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mi>&#949;</mml:mi><mml:mo>&#9675;</mml:mo><mml:mi>&#963;</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>s</mml:mi><mml:mi>q</mml:mi><mml:mi>u</mml:mi><mml:mi>e</mml:mi><mml:mi>e</mml:mi><mml:mi>z</mml:mi><mml:mi>e</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mtext class="textrm" mathvariant="normal">ReLU</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>e</mml:mi><mml:mi>x</mml:mi><mml:mi>c</mml:mi><mml:mi>i</mml:mi><mml:mi>t</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Here &#9675; denotes element-wise multiplication.</p></sec><sec><title>2.3.2 Cross-configuration attention</title><p>Another dimension in which we apply the attention mechanism is the batch dimension: across the sampled configurations. We design the cross-configuration attention block that allows the model to explicitly compare each configuration against the others throughout the network. We find this method to be much superior to letting the model infer for each configuration individually and only compare them implicitly via the loss function (<monospace>PairwiseHingeLoss</monospace> in this paper). The cross-configuration attention expression comes as follows:</p><disp-formula id="E6"><label>(6)</label><mml:math id="M8" overflow="scroll"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>A</mml:mi></mml:mrow><mml:mrow><mml:mi>c</mml:mi><mml:mi>r</mml:mi><mml:mi>o</mml:mi><mml:mi>s</mml:mi><mml:mi>s</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msubsup><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo>&#9675;</mml:mo><mml:mstyle displaystyle="true"><mml:munder><mml:mrow><mml:mtext class="textrm" mathvariant="normal">Softmax</mml:mtext></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:msubsup><mml:mrow><mml:mi>&#949;</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>b</mml:mi></mml:mrow></mml:msubsup><mml:mo>/</mml:mo><mml:mi>T</mml:mi></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>Here <italic toggle="yes">i</italic> is the node index, <italic toggle="yes">b</italic> is the configuration index across the batch dimension, <italic toggle="yes">T</italic> is a learnable temperature parameter.</p><p>By applying the cross-configuration attention layer after the channel-wise self-attention at every block of the network, we observe a significant improvement of the target metric (Kendall's &#964;), especially for <monospace>default</monospace> collections.</p></sec><sec><title>2.3.3 Entire architecture</title><p>The full architecture of TGraph is shown in <xref rid="F3" ref-type="fig">Figure 3</xref>. After feature concatenation, we apply a fully-connected layer, then we apply a stack of 2 graph convolutional blocks <inline-formula><mml:math id="M9" overflow="scroll"><mml:msubsup><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msubsup></mml:math></inline-formula>, <italic toggle="yes">k</italic>&#8712;1..2, then we perform global average pooling over the node dimension indexed by <italic toggle="yes">i</italic>, and finally, we apply another linear layer to eliminate the feature dimension and get the vector of scores <italic toggle="yes">s</italic><sub><italic toggle="yes">c</italic></sub> where <italic toggle="yes">c</italic> is the index across the configuration dimension.</p><p>The entire network prediction can be expressed as:</p><disp-formula id="E7"><label>(7)</label><mml:math id="M10" overflow="scroll"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>R</mml:mi></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mi>e</mml:mi><mml:mi>u</mml:mi><mml:mi>r</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>o</mml:mi><mml:mi>u</mml:mi><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>P</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:msub><mml:mrow><mml:mi>l</mml:mi></mml:mrow><mml:mrow><mml:mi>g</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>b</mml:mi><mml:mi>a</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>B</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>f</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:msub><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>X</mml:mi></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where <italic toggle="yes">X</italic> is the input feature vector, <italic toggle="yes">f</italic><sub><italic toggle="yes">in</italic></sub> - a 2-layer MLP with {256, 256} features and GELU activation, <italic toggle="yes">f</italic><sub><italic toggle="yes">out</italic></sub> - linear layer with a single feature and no activation, <italic toggle="yes">Pool</italic><sub><italic toggle="yes">global</italic></sub> - global average pooling across nodes.</p></sec></sec><sec><title>2.4 Training and inference procedures</title><sec><title>2.4.1 Loss function</title><p>We use the Pairwise Hinge Loss (<monospace>PairwiseHingeLoss</monospace>, <xref rid="B13" ref-type="bibr">Joachims, 2002</xref>; <xref rid="B2" ref-type="bibr">Agarwal et al., 2019</xref>) loss function for training the model.</p><disp-formula id="E8"><label>(8)</label><mml:math id="M11" overflow="scroll"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mrow><mml:mstyle><mml:mi>L</mml:mi></mml:mstyle></mml:mrow><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow><mml:mo>,</mml:mo><mml:mrow><mml:mo>{</mml:mo><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mo>}</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mo>=</mml:mo><mml:mstyle displaystyle="true"><mml:munder class="msub"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mstyle displaystyle="true"><mml:munder class="msub"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mi>I</mml:mi><mml:mrow><mml:mo>[</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>&gt;</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>]</mml:mo></mml:mrow><mml:mo class="qopname">max</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mn>0</mml:mn><mml:mo>,</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where <italic toggle="yes">r</italic><sub><italic toggle="yes">i</italic></sub> - are the ground truth runtimes, <italic toggle="yes">s</italic><sub><italic toggle="yes">i</italic></sub> - are the scores predicted by the model.</p><p>It is important that the predicted scores <italic toggle="yes">s</italic><sub><italic toggle="yes">i</italic></sub> = <italic toggle="yes">R</italic><sub><italic toggle="yes">neural</italic></sub>(<italic toggle="yes">c</italic><sub><italic toggle="yes">i</italic></sub>) do not correspond to the absolute values of runtimes <italic toggle="yes">r</italic><sub><italic toggle="yes">i</italic></sub> = <italic toggle="yes">R</italic>(<italic toggle="yes">c</italic><sub><italic toggle="yes">i</italic></sub>). The applied loss function is a ranking loss function. It trains the model to order (rank) the predicted values in the same way as they are ordered by <italic toggle="yes">R</italic>(<italic toggle="yes">c</italic>). The correct ordering is enough to satisfy <xref rid="E1" ref-type="disp-formula">Equation 1</xref>.</p></sec><sec><title>2.4.2 Training details</title><p>We train separate model instances for all collections. We've identified that separate models perform better than a joint model trained on all collections or models that were trained on all-<monospace>xla</monospace> or all-<monospace>nlp</monospace> combinations as well as all-<monospace>random</monospace> or all-<monospace>default</monospace>.</p><p>We use Adam (<xref rid="B16" ref-type="bibr">Kingma and Ba, 2014</xref>) optimizer (specifically AdamW version) with the learning rate of 1e-3, 0.05 of the total number of epochs as linear warm-up, a single-cycle (lifted cosine) learning rate schedule, and weight decay of 1e-5 for non-bias parameters. We apply gradient norm clipping at value 1.0.</p><p>We train the <monospace>tile-xla</monospace> collection for 17.5 epochs, whereas <monospace>layout-nlp</monospace> collections for 1000 epochs and <monospace>layout-xla</monospace> collections for 750 epochs.</p><p>Training wall-clock time is 2.5 hours per fold per collection measured on RTX4090 with 24 GB RAM. Training one set of models for all collections produces 13.45 kg CO<sub>2</sub> as per <xref rid="B17" ref-type="bibr">Lacoste et al. (2019)</xref>.</p></sec><sec><title>2.4.3 Data splits</title><p>Whereas the official training/validation split is reasonably designed, we, however, employ K-fold cross-validation with <italic toggle="yes">K</italic> = 20 on the merged train/validation data splits. We train the first 5 folds to limit the training compute. We then pick the top-4 folds by the validation score to combat the instability of training. This choice comes from the slight instability of training: in rare cases, the training process for a specific fold may get stuck at a local minimum or experience partial parameter corruption due to gradient explosion. In addition, we choose not to split configurations of the same graph into train/validation since it would introduce a train-to-validation leak due to the very high correlation of configuration runtimes within the same graph.</p></sec></sec><sec><title>2.5 Benchmark results</title><sec><title>2.5.1 Evaluation splits</title><p>TpuGraphs (<xref rid="B22" ref-type="bibr">Phothilimthana et al., 2023</xref>) dataset does not provide public test data annotations. Hence, we report the cross-validation score according to the Section 2.4.3.</p></sec><sec><title>2.5.2 Evaluation metrics</title><p>Kendall's &#964; (Kendall's rank correlation coefficient) is used as the metric for <monospace>layout</monospace> collections:</p><disp-formula id="E9"><label>(9)</label><mml:math id="M12" overflow="scroll"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:mi>&#964;</mml:mi><mml:mo>=</mml:mo><mml:mfrac><mml:mrow><mml:mn>2</mml:mn></mml:mrow><mml:mrow><mml:mi>n</mml:mi><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:mi>n</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mrow></mml:mfrac><mml:mstyle displaystyle="true"><mml:munder class="msub"><mml:mrow><mml:mo>&#8721;</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&lt;</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:munder></mml:mstyle><mml:mtext class="textrm" mathvariant="normal">sgn</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>s</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow><mml:mtext class="textrm" mathvariant="normal">sgn</mml:mtext><mml:mrow><mml:mo stretchy="false">(</mml:mo><mml:mrow><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo stretchy="false">)</mml:mo></mml:mrow></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where <italic toggle="yes">s</italic> are the predicted scores, <italic toggle="yes">r</italic> are the ground truth runtimes, <italic toggle="yes">n</italic> is the batch size.</p><p>For the <monospace>tile</monospace> collection, the metric is set as:</p><disp-formula id="E10"><label>(10)</label><mml:math id="M13" overflow="scroll"><mml:mtable class="eqnarray" columnalign="left"><mml:mtr><mml:mtd><mml:msub><mml:mrow><mml:mi>M</mml:mi></mml:mrow><mml:mrow><mml:mtext class="textrm" mathvariant="normal">tile</mml:mtext></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mrow><mml:mo stretchy="true">(</mml:mo><mml:mrow><mml:mfrac><mml:mrow><mml:mtext class="textrm" mathvariant="normal">Best runtime of top-</mml:mtext><mml:mi>k</mml:mi><mml:mtext>&#160;</mml:mtext><mml:mtext class="textrm" mathvariant="normal">predictions</mml:mtext></mml:mrow><mml:mrow><mml:mtext class="textrm" mathvariant="normal">Best runtime of all configurations</mml:mtext></mml:mrow></mml:mfrac><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow><mml:mo stretchy="true">)</mml:mo></mml:mrow></mml:mtd></mml:mtr><mml:mtr><mml:mtd><mml:mtext>&#8195;&#8195;</mml:mtext><mml:mo>=</mml:mo><mml:mn>2</mml:mn><mml:mo>-</mml:mo><mml:mfrac><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mrow><mml:mo class="qopname">min</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>K</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mrow><mml:mstyle displaystyle="true"><mml:msub><mml:mrow><mml:mo class="qopname">min</mml:mo></mml:mrow><mml:mrow><mml:mi>i</mml:mi><mml:mo>&#8712;</mml:mo><mml:mi>A</mml:mi></mml:mrow></mml:msub></mml:mstyle><mml:msub><mml:mrow><mml:mi>r</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mtd></mml:mtr></mml:mtable></mml:math></disp-formula><p>where <italic toggle="yes">K</italic> = 5.</p></sec><sec><title>2.5.3 Details of the inference mode</title><p>For inference, we use the batch size of 128. However, since the prediction depends on the batch, we leverage the batch further by applying test-time augmentation (TTA) to generate N (10) permutations of the configurations and average the result after sorting it back to the original order. We average the scores of models trained on different folds.</p><p>The single-batch wall clock time is 60 ms on average for 1 fold and 240 ms on average for all 4 folds per collection.</p></sec><sec><title>2.5.4 Experimental results</title><p>Our experimental results are summarized in the <xref rid="T2" ref-type="table">Table 2</xref>. The confidence ranges are reported as 1-sigma. We demonstrate state-of-the-art performance in 4 out of 5 collections. On <monospace>xla-default</monospace>
<xref rid="B32" ref-type="bibr">Xu et al. (2023)</xref> show better results than our work; however, their results may contain an error since <monospace>xla-default</monospace> collection is harder than <monospace>xla-random</monospace> due to closer and harder-to-distinguish runtime annotations (the pattern is also followed by the results of TpuGraphs; <xref rid="B22" ref-type="bibr">Phothilimthana et al., 2023</xref>), but the score of <xref rid="B32" ref-type="bibr">Xu et al. (2023)</xref> for <monospace>xla-default</monospace> is higher than for <monospace>xla-random</monospace> which is very implausible.</p><table-wrap position="float" id="T2" orientation="portrait"><label>Table 2</label><caption><p>Experimental results.</p></caption><table frame="box" rules="all"><thead><tr style="background-color:#919498;color:#ffffff"><th valign="top" align="left" rowspan="2" colspan="1">
<bold>Collection</bold>
</th><th valign="top" align="left" rowspan="2" colspan="1">
<bold>Metric</bold>
</th><th valign="top" align="center" colspan="3" rowspan="1">
<bold>Validation score</bold>
</th></tr><tr style="background-color:#919498;color:#ffffff"><th valign="top" align="center" rowspan="1" colspan="1">
<bold>TpuGraphs (<xref rid="B22" ref-type="bibr">Phothilimthana et al., 2023</xref>)</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>(<xref rid="B32" ref-type="bibr">Xu et al., 2023</xref>)</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>TGraph (ours)</bold>
</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">layout:xla:random</td><td valign="top" align="left" rowspan="1" colspan="1">Kendall's &#964;</td><td valign="top" align="center" rowspan="1" colspan="1">0.19</td><td valign="top" align="center" rowspan="1" colspan="1">0.5285</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.6840</bold> &#177; 0.0110</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">layout:xla:default</td><td valign="top" align="left" rowspan="1" colspan="1">Kendall's &#964;</td><td valign="top" align="center" rowspan="1" colspan="1">0.12</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.5887</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">0.4785 &#177; 0.0031</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">layout:nlp:random</td><td valign="top" align="left" rowspan="1" colspan="1">Kendall's &#964;</td><td valign="top" align="center" rowspan="1" colspan="1">0.58</td><td valign="top" align="center" rowspan="1" colspan="1">0.8387</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.9713</bold> &#177; 0.0008</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">layout:nlp:default</td><td valign="top" align="left" rowspan="1" colspan="1">Kendall's &#964;</td><td valign="top" align="center" rowspan="1" colspan="1">0.30</td><td valign="top" align="center" rowspan="1" colspan="1">0.4841</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.5628</bold> &#177; 0.0027</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">mean across layout</td><td valign="top" align="left" rowspan="1" colspan="1">Kendall's &#964;</td><td valign="top" align="center" rowspan="1" colspan="1">0.298</td><td valign="top" align="center" rowspan="1" colspan="1">0.610</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.674</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">tile:xla</td><td valign="top" align="left" rowspan="1" colspan="1">
<italic toggle="yes">M</italic>
<sub>
<italic toggle="yes">tile</italic>
</sub>
</td><td valign="top" align="center" rowspan="1" colspan="1">&#8211;</td><td valign="top" align="center" rowspan="1" colspan="1">0.8622</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.9694</bold> &#177; 0.0021</td></tr></tbody></table><table-wrap-foot><p>Row-wise best is highlighted with bold.</p></table-wrap-foot></table-wrap></sec><sec><title>2.5.5 Ablation study</title><p>Ablations for channel-wise self-attention, cross-configuration attention, and edges in the graph are collected in <xref rid="T3" ref-type="table">Table 3</xref>. While the effect of channel-wise self-attention is less obvious but nevertheless noticeable, the effect of cross-configuration attention is substantial, implying that the task of comparing the configurations between each other is easier than predicting the absolute values of runtimes. Additionally, we ablate the edges of the GraphSage GNN to demonstrate how essential the connectivity between the computational nodes is. In tensor compilers the adjacent operators are often fused into a single optimized operator, the procedure commonly know as kernel fusion. For a model solving the problem of predicting computational graph runtimes it is paramount to implicitly learn the &#8220;rules&#8221; of kernel fusion from data since the early stages of tensor compilation including kernel fusion are treated as a black box.</p><boxed-text position="float" orientation="portrait"><p>&#8220;<italic toggle="yes">Data centers will use 8% of US power by 2030, compared with 3% in 2022.&#8221;</italic></p><p>&#8211; <xref rid="B26" ref-type="bibr">Sachs (2024)</xref></p></boxed-text><table-wrap position="float" id="T3" orientation="portrait"><label>Table 3</label><caption><p>Ablation study.</p></caption><table frame="box" rules="all"><thead><tr style="background-color:#919498;color:#ffffff"><th valign="top" align="left" rowspan="2" colspan="1">
<bold>Configuration</bold>
</th><th valign="top" align="center" colspan="4" rowspan="1"><bold>Validation score, Kendall's</bold> &#964;</th></tr><tr style="background-color:#919498;color:#ffffff"><th valign="top" align="center" rowspan="1" colspan="1">
<bold>layout:xla:random</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>layout:xla:default</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>layout:nlp:random</bold>
</th><th valign="top" align="center" rowspan="1" colspan="1">
<bold>layout:nlp:default</bold>
</th></tr></thead><tbody><tr><td valign="top" align="left" rowspan="1" colspan="1">Final, all features</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.6840</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">0.4785</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.9713</bold>
</td><td valign="top" align="center" rowspan="1" colspan="1">
<bold>0.5628</bold>
</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">- Channel-wise self-attention</td><td valign="top" align="center" rowspan="1" colspan="1">0.6737 (<inline-formula><mml:math id="M14" overflow="scroll"><mml:mrow><mml:mstyle mathcolor="#ff0000"><mml:mo>&#8722;</mml:mo><mml:mn>0.0103</mml:mn></mml:mstyle></mml:mrow></mml:math></inline-formula>)</td><td valign="top" align="center" rowspan="1" colspan="1"><bold>0.4787</bold> (<inline-formula><mml:math id="M15" overflow="scroll"><mml:mrow><mml:mstyle mathcolor="#539653"><mml:mo>+</mml:mo><mml:mn>0.0002</mml:mn></mml:mstyle></mml:mrow></mml:math></inline-formula>)</td><td valign="top" align="center" rowspan="1" colspan="1">0.9680 (<inline-formula><mml:math id="M16" overflow="scroll"><mml:mrow><mml:mstyle mathcolor="#ff0000"><mml:mo>&#8722;</mml:mo><mml:mn>0.0033</mml:mn></mml:mstyle></mml:mrow></mml:math></inline-formula>)</td><td valign="top" align="center" rowspan="1" colspan="1">0.5555 (<inline-formula><mml:math id="M17" overflow="scroll"><mml:mrow><mml:mstyle mathcolor="#ff0000"><mml:mo>&#8722;</mml:mo><mml:mn>0.0073</mml:mn></mml:mstyle></mml:mrow></mml:math></inline-formula>)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">- Cross-configuration attention</td><td valign="top" align="center" rowspan="1" colspan="1">0.6539 (<inline-formula><mml:math id="M18" overflow="scroll"><mml:mrow><mml:mstyle mathcolor="#ff0000"><mml:mo>&#8722;</mml:mo><mml:mn>0.0301</mml:mn></mml:mstyle></mml:mrow></mml:math></inline-formula>)</td><td valign="top" align="center" rowspan="1" colspan="1">0.4518 (<inline-formula><mml:math id="M19" overflow="scroll"><mml:mrow><mml:mstyle mathcolor="#ff0000"><mml:mo>&#8722;</mml:mo><mml:mn>0.0267</mml:mn></mml:mstyle></mml:mrow></mml:math></inline-formula>)</td><td valign="top" align="center" rowspan="1" colspan="1">0.9387 (<inline-formula><mml:math id="M20" overflow="scroll"><mml:mrow><mml:mstyle mathcolor="#ff0000"><mml:mo>&#8722;</mml:mo><mml:mn>0.0326</mml:mn></mml:mstyle></mml:mrow></mml:math></inline-formula>)</td><td valign="top" align="center" rowspan="1" colspan="1">0.5436 (<inline-formula><mml:math id="M21" overflow="scroll"><mml:mrow><mml:mstyle mathcolor="#ff0000"><mml:mo>&#8722;</mml:mo><mml:mn>0.0192</mml:mn></mml:mstyle></mml:mrow></mml:math></inline-formula>)</td></tr><tr><td valign="top" align="left" rowspan="1" colspan="1">- Graph edges</td><td valign="top" align="center" rowspan="1" colspan="1">0.5022 (<inline-formula><mml:math id="M22" overflow="scroll"><mml:mrow><mml:mstyle mathcolor="#ff0000"><mml:mo>&#8722;</mml:mo><mml:mn>0.1818</mml:mn></mml:mstyle></mml:mrow></mml:math></inline-formula>)</td><td valign="top" align="center" rowspan="1" colspan="1">0.3631 (<inline-formula><mml:math id="M23" overflow="scroll"><mml:mrow><mml:mstyle mathcolor="#ff0000"><mml:mo>&#8722;</mml:mo><mml:mn>0.1154</mml:mn></mml:mstyle></mml:mrow></mml:math></inline-formula>)</td><td valign="top" align="center" rowspan="1" colspan="1">0.7751 (<inline-formula><mml:math id="M24" overflow="scroll"><mml:mrow><mml:mstyle mathcolor="#ff0000"><mml:mo>&#8722;</mml:mo><mml:mn>0.1962</mml:mn></mml:mstyle></mml:mrow></mml:math></inline-formula>)</td><td valign="top" align="center" rowspan="1" colspan="1">0.3349 (<inline-formula><mml:math id="M25" overflow="scroll"><mml:mrow><mml:mstyle mathcolor="#ff0000"><mml:mo>&#8722;</mml:mo><mml:mn>0.2279</mml:mn></mml:mstyle></mml:mrow></mml:math></inline-formula>)</td></tr></tbody></table><table-wrap-foot><p>Column-wise best is highlighted with bold. Red indicates a decrease in a metric, green indicates an increase relative to our final result (&#8220;Final, all features&#8221;).</p></table-wrap-foot></table-wrap></sec></sec><sec><title>2.6 Environmental impact case study</title><p>According to <xref rid="B18" ref-type="bibr">Loten (2023)</xref> the total data center AI workload consumption in Northern Virginia (NV), VA, the US was 2132 MW in 2023. Thus, the annual data center energy consumption can be estimated as 18.6 million MWh. Considering the carbon footprint of energy production in NV of 0.3 tonne CO<sub>2</sub> per MWh as per (<xref rid="B28" ref-type="bibr">Statista, 2022</xref>) the total annual CO<sub>2</sub> emissions of NV data centers can be assessed as 5.58 mln tonnes CO<sub>2</sub>. From the authors of XTAT (<xref rid="B23" ref-type="bibr">Phothilimthana et al., 2021</xref>) we take 5% as a reference number for the runtime speed-up across a diverse dataset of 150 neural architectures. Speeding up AI workloads by 5% with the more efficient execution would reduce CO<sub>2</sub> emissions by 275'000 tonnes CO<sub>2</sub> yearly in NV alone. This is equivalent to the annual emissions of 36'000 households (approximately 50% of all NV households). Even though it is yet to be determined how to estimate the real acceleration of computation based on the values of Kendall's &#964;, we expect the effect to be similar or superior to XTAT (<xref rid="B23" ref-type="bibr">Phothilimthana et al., 2021</xref>).</p></sec></sec><sec sec-type="conclusions" id="s3"><title>3 Conclusion</title><p>The proposed novel TGraph neural network architecture establishes a state-of-the-art on the TpuGraphs dataset. A significant contribution to the performance comes from channel-wise self-attention and cross-configuration attention operations. The latter acts as one of the batch normalization techniques, allowing the exchange of information between individual samples, which improves performance in ranking problems.</p><p>In general, more efficient ML-based tensor compilation methods have a very positive societal impact. Firstly, they decrease energy consumption and CO<sub>2</sub> emissions of data centers, consequently helping to fight climate change. Secondly, they help to free software engineers from the tedious labor of re-implementing lots of highly specialized computational kernels for the constant flow of hardware releases. Even though it may seem that it is a case of &#8220;AI taking over people's jobs&#8221;, in fact, the achieved extreme efficiency of digital infrastructure like data centers may cover the needs of people to the extent that they do not need to work or can opt to dedicate themselves to more human-centered activities.</p></sec><sec id="s4"><title>4 Limitations and future work</title><p>The proposed neural network architecture is limited to predicting the runtimes of a static tensor program that can be represented as a computational graph. Another limitation is that the proposed method is not able to learn the behavior of the tensor program if the behavior is dependent on the values of input or intermediate data. As a machine learning algorithm, the proposed method requires a substantial amount of training data. In the absence of a diverse sample of benchmarked architectures, the domain gap between the training graphs and the unknown test graphs may be big enough, and the model is not able to generalize to it. The proposed method does not provide any guidance on how to choose the graphs for the creation of the training dataset. The proposed method does not generalize to unknown operators. New graphs with the new operator must be added to the training data in order for the model to learn the information about its contribution to the runtime. An ML model trained on one hardware (TPU) does not necessarily generalize to other hardware (GPU, CPU, etc) and must be re-trained for other hardware. Lastly, the proposed solution addresses two compilation sub-problems: tensor layout selection and tensor tiling selection, whereas there are more sub-problems to be solved by tensor compilers.</p><p>A potential future direction of research could be to transition from a predictive model to a generative model that is capable of directly proposing efficient configurations. This could involve training a variational autoencoder or diffusion model on the configuration space to generate novel tensor layouts and tiling strategies that may outperform configurations found through traditional search methods.</p></sec></body><back><sec sec-type="data-availability" id="s5"><title>Data availability statement</title><p>Publicly available datasets were analyzed in this study. This data can be found here: <ext-link xlink:href="https://www.kaggle.com/competitions/predict-ai-model-runtime/data" ext-link-type="uri">https://www.kaggle.com/competitions/predict-ai-model-runtime/data</ext-link>.</p></sec><sec sec-type="author-contributions" id="s6"><title>Author contributions</title><p>DK: Supervision, Software, Investigation, Writing &#8211; review &amp; editing, Funding acquisition, Visualization, Writing &#8211; original draft. EA: Conceptualization, Data curation, Methodology, Visualization, Writing &#8211; review &amp; editing, Software, Investigation. TN: Writing &#8211; review &amp; editing, Software, Investigation, Data curation, Visualization, Validation, Conceptualization. MF: Conceptualization, Validation, Writing &#8211; review &amp; editing, Investigation, Software, Visualization, Data curation. DP: Supervision, Writing &#8211; review &amp; editing.</p></sec><sec sec-type="COI-statement" id="conf1"><title>Conflict of interest</title><p>EA, TN and MF were employed at Sprout.ai. The remaining authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></sec><sec sec-type="ai-statement" id="s8"><title>Generative AI statement</title><p>The author(s) declare that no Gen AI was used in the creation of this manuscript.</p></sec><sec sec-type="disclaimer" id="s9"><title>Publisher's note</title><p>All claims expressed in this article are solely those of the authors and do not necessarily represent those of their affiliated organizations, or those of the publisher, the editors and the reviewers. Any product that may be evaluated in this article, or claim that may be made by its manufacturer, is not guaranteed or endorsed by the publisher.</p></sec><sec sec-type="supplementary-material" id="s10"><title>Supplementary material</title><p>The Supplementary Material for this article can be found online at: <ext-link xlink:href="https://www.frontiersin.org/articles/10.3389/frai.2025.1605539/full#supplementary-material" ext-link-type="uri">https://www.frontiersin.org/articles/10.3389/frai.2025.1605539/full#supplementary-material</ext-link></p><supplementary-material id="SM1" position="float" content-type="local-data" orientation="portrait"><media xlink:href="Data_Sheet_1.zip" position="float" orientation="portrait"/></supplementary-material></sec><ref-list><title>References</title><ref id="B1"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Abadi</surname><given-names>M.</given-names></name><name name-style="western"><surname>Barham</surname><given-names>P.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Davis</surname><given-names>A.</given-names></name><name name-style="western"><surname>Dean</surname><given-names>J.</given-names></name><etal/></person-group>. (<year>2016</year>). <article-title>&#8220;Tensorflow: A system for large-scale machine learning,&#8221;</article-title> in <source>12th USENIX Symposium on Operating Systems Design and Implementation (OSDI '16</source>).</mixed-citation></ref><ref id="B2"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Agarwal</surname><given-names>A.</given-names></name><name name-style="western"><surname>Takatsu</surname><given-names>K.</given-names></name><name name-style="western"><surname>Zaitsev</surname><given-names>I.</given-names></name><name name-style="western"><surname>Joachims</surname><given-names>T.</given-names></name></person-group> (<year>2019</year>). <article-title>&#8220;A general framework for counterfactual learning-to-rank,&#8221;</article-title> in <source>Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR'19</source> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>).</mixed-citation></ref><ref id="B3"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bol&#243;n-Canedo</surname><given-names>V.</given-names></name><name name-style="western"><surname>Mor&#225;n-Fern&#225;ndez</surname><given-names>L.</given-names></name><name name-style="western"><surname>Cancela</surname><given-names>B.</given-names></name><name name-style="western"><surname>Alonso-Betanzos</surname><given-names>A.</given-names></name></person-group> (<year>2024</year>). <article-title>A review of green artificial intelligence: towards a more sustainable future</article-title>. <source>Neurocomputing</source><volume>599</volume>:<fpage>128096</fpage>. <pub-id pub-id-type="doi">10.1016/j.neucom.2024.128096</pub-id></mixed-citation></ref><ref id="B4"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bradbury</surname><given-names>J.</given-names></name><name name-style="western"><surname>Frostig</surname><given-names>R.</given-names></name><name name-style="western"><surname>Hawkins</surname><given-names>P.</given-names></name><name name-style="western"><surname>Johnson</surname><given-names>M. J.</given-names></name><name name-style="western"><surname>Leary</surname><given-names>C.</given-names></name><name name-style="western"><surname>Maclaurin</surname><given-names>D.</given-names></name><etal/></person-group>. (<year>2018</year>). <source>JAX: Composable Transformations of Python</source>+<italic toggle="yes">NumPy Programs</italic>.</mixed-citation></ref><ref id="B5"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cao</surname><given-names>K.</given-names></name><name name-style="western"><surname>Phothilimthana</surname><given-names>P. M.</given-names></name><name name-style="western"><surname>Abu-El-Haija</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zelle</surname><given-names>D.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Mendis</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2023</year>). <article-title>Learning large graph property prediction via graph segment training</article-title>. <source>arXiv</source> [preprint] arXiv:2305.12322. <pub-id pub-id-type="doi">10.48550/arXiv.2305.12322</pub-id>27534393
</mixed-citation></ref><ref id="B6"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>T.</given-names></name><name name-style="western"><surname>Moreau</surname><given-names>T.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>E.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>H.</given-names></name><etal/></person-group>. (<year>2018a</year>). <article-title>&#8220;TVM: An automated End-to-End optimizing compiler for deep learning,&#8221;</article-title> in <source>13th USENIX Symposium on Operating Systems Design and Implementation (OSDI 18)</source> (<publisher-loc>Carlsbad, CA</publisher-loc>: <publisher-name>USENIX Association</publisher-name>), <fpage>578</fpage>&#8211;<lpage>594</lpage>.</mixed-citation></ref><ref id="B7"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Yan</surname><given-names>E.</given-names></name><name name-style="western"><surname>Jiang</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Moreau</surname><given-names>T.</given-names></name><name name-style="western"><surname>Ceze</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2018b</year>). <article-title>&#8220;Learning to optimize tensor programs,&#8221;</article-title> in <source>Advances in Neural Information Processing Systems</source>, eds. S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett (New York: Curran Associates, Inc.).</mixed-citation></ref><ref id="B8"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Desislavov</surname><given-names>R.</given-names></name><name name-style="western"><surname>Mart&#237;nez-Plumed</surname><given-names>F.</given-names></name><name name-style="western"><surname>Hern&#225;ndez-Orallo</surname><given-names>J.</given-names></name></person-group> (<year>2023</year>). <article-title>Trends in AI inference energy consumption: Beyond the performance-vs-parameter laws of deep learning</article-title>. <source>Sustain. Comput.: Inform. Syst</source>. <volume>38</volume>:<fpage>100857</fpage>. <pub-id pub-id-type="doi">10.1016/j.suscom.2023.100857</pub-id></mixed-citation></ref><ref id="B9"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hamilton</surname><given-names>W.</given-names></name><name name-style="western"><surname>Ying</surname><given-names>R.</given-names></name><name name-style="western"><surname>Leskovec</surname><given-names>J.</given-names></name></person-group> (<year>2017</year>). <article-title>&#8220;Inductive representation learning on large graphs,&#8221;</article-title> in <source>31st Conference on Neural Information Processing Systems (NIPS 2017)</source>.</mixed-citation></ref><ref id="B10"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Hu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Shen</surname><given-names>L.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>G.</given-names></name></person-group> (<year>2018</year>). <article-title>&#8220;Squeeze-and-excitation networks,&#8221;</article-title> in <source>2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</source> (<publisher-loc>Salt Lake City, UT</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>7132</fpage>&#8211;<lpage>7141</lpage>.</mixed-citation></ref><ref id="B11"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Huyen</surname><given-names>C.</given-names></name></person-group> (<year>2024</year>). <source>AI Engineering</source>. <publisher-loc>Sebastopol, CA</publisher-loc>: <publisher-name>O'Reilly Media</publisher-name>.</mixed-citation></ref><ref id="B12"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Jia</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Padon</surname><given-names>O.</given-names></name><name name-style="western"><surname>Thomas</surname><given-names>J.</given-names></name><name name-style="western"><surname>Warszawski</surname><given-names>T.</given-names></name><name name-style="western"><surname>Zaharia</surname><given-names>M.</given-names></name><name name-style="western"><surname>Aiken</surname><given-names>A.</given-names></name></person-group> (<year>2019</year>). <article-title>&#8220;TASO: optimizing deep learning computation with automatic generation of graph substitutions,&#8221;</article-title> in <source>SOSP '19</source> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>), <fpage>47</fpage>&#8211;<lpage>62</lpage>.</mixed-citation></ref><ref id="B13"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Joachims</surname><given-names>T.</given-names></name></person-group> (<year>2002</year>). <article-title>&#8220;Optimizing search engines using clickthrough data,&#8221;</article-title> in <source>Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '02</source> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>), <fpage>133</fpage>&#8211;<lpage>142</lpage>.</mixed-citation></ref><ref id="B14"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Jouppi</surname><given-names>N.</given-names></name><name name-style="western"><surname>Kurian</surname><given-names>G.</given-names></name><name name-style="western"><surname>Li</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>P.</given-names></name><name name-style="western"><surname>Nagarajan</surname><given-names>R.</given-names></name><name name-style="western"><surname>Nai</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2023</year>). <article-title>&#8220;TPU v4: An optically reconfigurable supercomputer for machine learning with hardware support for embeddings,&#8221;</article-title> in <source>Proceedings of the 50th Annual International Symposium on Computer Architecture, ISCA '23</source> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>).</mixed-citation></ref><ref id="B15"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Jung</surname><given-names>W.</given-names></name><name name-style="western"><surname>Dao</surname><given-names>T. T.</given-names></name><name name-style="western"><surname>Lee</surname><given-names>J.</given-names></name></person-group> (<year>2021</year>). <article-title>&#8220;Deepcuts: a deep learning optimization framework for versatile gpu workloads,&#8221;</article-title> in <source>Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, PLDI 2021</source> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>Association for Computing Machinery</publisher-name>), <fpage>190</fpage>&#8211;<lpage>205</lpage>.</mixed-citation></ref><ref id="B16"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kingma</surname><given-names>D. P.</given-names></name><name name-style="western"><surname>Ba</surname><given-names>J.</given-names></name></person-group> (<year>2014</year>). <article-title>Adam: A method for stochastic optimization</article-title>. <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1412.6980</pub-id></mixed-citation></ref><ref id="B17"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lacoste</surname><given-names>A.</given-names></name><name name-style="western"><surname>Luccioni</surname><given-names>A. S.</given-names></name><name name-style="western"><surname>Schmidt</surname><given-names>V.</given-names></name><name name-style="western"><surname>Dandres</surname><given-names>T.</given-names></name></person-group> (<year>2019</year>). <article-title>Quantifying the carbon emissions of machine learning</article-title>. <source>arXiv</source>. <pub-id pub-id-type="doi">10.48550/arXiv.1910.09700</pub-id></mixed-citation></ref><ref id="B18"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Loten</surname><given-names>A.</given-names></name></person-group> (<year>2023</year>). <source>Rising Data Center Costs Linked to AI Demands</source>. Available online at: <ext-link xlink:href="https://www.wsj.com/articles/rising-data-center-costs-linked-to-ai-demands-fc6adc0ewww.wsj.com" ext-link-type="uri">https://www.wsj.com/articles/rising-data-center-costs-linked-to-ai-demands-fc6adc0ewww.wsj.com</ext-link> (Accessed October 21, 2024).</mixed-citation></ref><ref id="B19"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Minaee</surname><given-names>S.</given-names></name><name name-style="western"><surname>Mikolov</surname><given-names>T.</given-names></name><name name-style="western"><surname>Nikzad</surname><given-names>N.</given-names></name><name name-style="western"><surname>Chenaghlu</surname><given-names>M.</given-names></name><name name-style="western"><surname>Socher</surname><given-names>R.</given-names></name><name name-style="western"><surname>Amatriain</surname><given-names>X.</given-names></name><etal/></person-group>. (<year>2024</year>). <source>Large Language Models: A Survey</source>.</mixed-citation></ref><ref id="B20"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Paszke</surname><given-names>A.</given-names></name><name name-style="western"><surname>Gross</surname><given-names>S.</given-names></name><name name-style="western"><surname>Massa</surname><given-names>F.</given-names></name><name name-style="western"><surname>Lerer</surname><given-names>A.</given-names></name><name name-style="western"><surname>Bradbury</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chanan</surname><given-names>G.</given-names></name><etal/></person-group>. (<year>2019</year>). <article-title>&#8220;Pytorch: an imperative style, high-performance deep learning library,&#8221;</article-title> in Advances in Neural Information Processing Systems, eds. H. Wallach, A. Larochelle, A. Beygelzimer, F. d' Alch&#233;-Buc, E. Fox, and R. Garnett (New York: Curran Associates, Inc).</mixed-citation></ref><ref id="B21"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Phothilimthana</surname><given-names>M.</given-names></name><name name-style="western"><surname>Burrows</surname><given-names>M.</given-names></name><name name-style="western"><surname>Kaufman</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name></person-group> (<year>2020</year>). <article-title>A learned performance model for the tensor processing unit</article-title>. <source>arXiv</source>. Available online at: <ext-link xlink:href="https://arxiv.org/abs/2008.01040" ext-link-type="uri">https://arxiv.org/abs/2008.01040</ext-link></mixed-citation></ref><ref id="B22"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Phothilimthana</surname><given-names>P. M.</given-names></name><name name-style="western"><surname>Abu-El-Haija</surname><given-names>S.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>K.</given-names></name><name name-style="western"><surname>Fatemi</surname><given-names>B.</given-names></name><name name-style="western"><surname>Burrows</surname><given-names>M.</given-names></name><name name-style="western"><surname>Mendis</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2023</year>). <article-title>&#8220;TpuGraphs: a performance prediction dataset on large tensor computational graphs,&#8221;</article-title> in <source>Thirty-seventh Conference on Neural Information Processing Systems Datasets and Bench-marks Track</source>.33118351
</mixed-citation></ref><ref id="B23"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Phothilimthana</surname><given-names>P. M.</given-names></name><name name-style="western"><surname>Sabne</surname><given-names>A.</given-names></name><name name-style="western"><surname>Sarda</surname><given-names>N.</given-names></name><name name-style="western"><surname>Murthy</surname><given-names>K. S.</given-names></name><name name-style="western"><surname>Zhou</surname><given-names>Y.</given-names></name><name name-style="western"><surname>Angermueller</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>&#8220;A flexible approach to autotuning multi-pass machine learning compilers,&#8221;</article-title> in <source>2021 30th International Conference on Parallel Architectures and Compilation Techniques (PACT)</source>, <fpage>1</fpage>&#8211;<lpage>16</lpage>.</mixed-citation></ref><ref id="B24"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Pugliese</surname><given-names>R.</given-names></name><name name-style="western"><surname>Regondi</surname><given-names>S.</given-names></name><name name-style="western"><surname>Marini</surname><given-names>R.</given-names></name></person-group> (<year>2021</year>). <article-title>Machine learning-based approach: global trends, research directions, and regulatory standpoints</article-title>. <source>Data Sci. Managem</source>. <volume>4</volume>, <fpage>19</fpage>&#8211;<lpage>29</lpage>. <pub-id pub-id-type="doi">10.1016/j.dsm.2021.12.002</pub-id></mixed-citation></ref><ref id="B25"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Sabne</surname><given-names>A.</given-names></name></person-group> (<year>2020</year>). <source>XLA: Compiling Machine Learning for Peak Performance</source>. Available online at: <ext-link xlink:href="https://research.google/pubs/xla-compiling-machine-learning-for-peak-performance/" ext-link-type="uri">https://research.google/pubs/xla-compiling-machine-learning-for-peak-performance/</ext-link></mixed-citation></ref><ref id="B26"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><name name-style="western"><surname>Sachs</surname><given-names>G.</given-names></name></person-group> (<year>2024</year>). <source>AI Poised to Drive 160</source>% <italic toggle="yes">Increase in Power Demand</italic>. Available online at: <ext-link xlink:href="https://www.goldmansachs.com/insights/articles/AI-poised-to-drive-160-increase-in-power-demandwww.goldmansachs.com" ext-link-type="uri">https://www.goldmansachs.com/insights/articles/AI-poised-to-drive-160-increase-in-power-demandwww.goldmansachs.com</ext-link> (accessed 25 October, 2024).</mixed-citation></ref><ref id="B27"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Snider</surname><given-names>D.</given-names></name><name name-style="western"><surname>Liang</surname><given-names>R.</given-names></name></person-group> (<year>2023</year>). <article-title>Operator fusion in XLA: analysis and evaluation</article-title>. <source>arXiv</source> [preprint] arXiv.2301.13062. <pub-id pub-id-type="doi">10.48550/arXiv.2301.13062</pub-id></mixed-citation></ref><ref id="B28"><mixed-citation publication-type="webpage"><person-group person-group-type="author"><collab>Statista</collab></person-group> (<year>2022</year>). <source>Power Sector Carbon Intensity in the United States in 2022, by State</source>. Available online at: <ext-link xlink:href="https://www.statista.com/statistics/1133295/electric-sector-carbon-dioxide-emission-rate-by-state-united-states/www.statista.com" ext-link-type="uri">https://www.statista.com/statistics/1133295/electric-sector-carbon-dioxide-emission-rate-by-state-united-states/www.statista.com</ext-link> (Accessed October 21, 2024).</mixed-citation></ref><ref id="B29"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Tschand</surname><given-names>A.</given-names></name><name name-style="western"><surname>Rajan</surname><given-names>A. T. R.</given-names></name><name name-style="western"><surname>Idgunji</surname><given-names>S.</given-names></name><name name-style="western"><surname>Ghosh</surname><given-names>A.</given-names></name><name name-style="western"><surname>Holleman</surname><given-names>J.</given-names></name><name name-style="western"><surname>Kir&#225;ly</surname><given-names>C.</given-names></name><etal/></person-group>. (<year>2025</year>). <article-title>&#8220;MLPerf power: Benchmarking the energy efficiency of machine learning systems from &#956;watts to mwatts for sustainable AI,&#8221;</article-title> in <source>IEEE International Symposium on High Performance Computer Architecture, HPCA 2025</source> (<publisher-loc>Las Vegas, NV</publisher-loc>: <publisher-name>IEEE</publisher-name>), <fpage>1201</fpage>&#8211;<lpage>1216</lpage>.</mixed-citation></ref><ref id="B30"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Vasilache</surname><given-names>N.</given-names></name><name name-style="western"><surname>Zinenko</surname><given-names>O.</given-names></name><name name-style="western"><surname>Theodoridis</surname><given-names>T.</given-names></name><name name-style="western"><surname>Goyal</surname><given-names>P.</given-names></name><name name-style="western"><surname>DeVito</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Moses</surname><given-names>W.</given-names></name><etal/></person-group>. (<year>2018</year>). <article-title>Tensor comprehensions: framework-agnostic high-performance machine learning abstractions</article-title>. <source>arXiv.</source><pub-id pub-id-type="doi">10.48550/arXiv.1802.04730</pub-id></mixed-citation></ref><ref id="B31"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>H.</given-names></name><name name-style="western"><surname>Zhai</surname><given-names>J.</given-names></name><name name-style="western"><surname>Gao</surname><given-names>M.</given-names></name><name name-style="western"><surname>Ma</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Tang</surname><given-names>S.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>L.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>&#8220;PET: Optimizing tensor programs with partially equivalent transformations and automated corrections,&#8221;</article-title> in <source>15th USENIX Symposium on Operating Systems Design and Implementation (OSDI 21)</source> (<publisher-loc>Berkeley, CA</publisher-loc>: <publisher-name>USENIX Association</publisher-name>), <fpage>37</fpage>&#8211;<lpage>54</lpage>.</mixed-citation></ref><ref id="B32"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>J.</given-names></name><name name-style="western"><surname>Pan</surname><given-names>L.</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>Q.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>W.</given-names></name><name name-style="western"><surname>Wan</surname><given-names>W.</given-names></name></person-group> (<year>2023</year>). <article-title>Based on tpugraphs predicting model runtimes using graph neural networks</article-title>. <source>Front. Comp. Intellig. Syst</source>. <volume>6</volume>, <fpage>66</fpage>&#8211;<lpage>69</lpage>. <pub-id pub-id-type="doi">10.54097/fcis.v6i1.13</pub-id></mixed-citation></ref><ref id="B33"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zhang</surname><given-names>L. L.</given-names></name><name name-style="western"><surname>Han</surname><given-names>S.</given-names></name><name name-style="western"><surname>Wei</surname><given-names>J.</given-names></name><name name-style="western"><surname>Zheng</surname><given-names>N.</given-names></name><name name-style="western"><surname>Cao</surname><given-names>T.</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Y.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>&#8220;nn-Meter: towards accurate latency prediction of deep-learning model inference on diverse edge devices,&#8221;</article-title> in <source>Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services</source> (<publisher-loc>New York, NY</publisher-loc>: <publisher-name>ACM</publisher-name>), <fpage>81</fpage>&#8211;<lpage>93</lpage>.</mixed-citation></ref><ref id="B34"><mixed-citation publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Jia</surname><given-names>C.</given-names></name><name name-style="western"><surname>Sun</surname><given-names>M.</given-names></name><name name-style="western"><surname>Wu</surname><given-names>Z.</given-names></name><name name-style="western"><surname>Yu</surname><given-names>C. H.</given-names></name><name name-style="western"><surname>Haj-Ali</surname><given-names>A.</given-names></name><etal/></person-group>. (<year>2020</year>). <article-title>&#8220;Ansor: generating high-performance tensor programs for deep learning,&#8221;</article-title> in <source>Proceedings of the 14th USENIX Conference on Operating Systems Design and Implementation, OSDI'20, USA</source> (<publisher-loc>Berkeley, CA</publisher-loc>: <publisher-name>USENIX Association</publisher-name>).</mixed-citation></ref><ref id="B35"><mixed-citation publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Zheng</surname><given-names>L.</given-names></name><name name-style="western"><surname>Liu</surname><given-names>R.</given-names></name><name name-style="western"><surname>Shao</surname><given-names>J.</given-names></name><name name-style="western"><surname>Chen</surname><given-names>T.</given-names></name><name name-style="western"><surname>Gonzalez</surname><given-names>J. E.</given-names></name><name name-style="western"><surname>Stoica</surname><given-names>I.</given-names></name><etal/></person-group>. (<year>2021</year>). <article-title>&#8220;Tenset: a large-scale program performance dataset for learned tensor compilers,&#8221;</article-title> in <source>Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1)</source>.</mixed-citation></ref></ref-list></back></article>
        
    </metadata>
</record>
    </GetRecord>

</OAI-PMH>