


<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
    <responseDate>2025-09-09T13:47:43Z</responseDate>
    <request verb="GetRecord" identifier="oai:pubmedcentral.nih.gov:12405556" metadataPrefix="pmc">https://pmc.ncbi.nlm.nih.gov/api/oai/v1/mh/</request>
    
    <GetRecord>
        <record>
    <header>
    <identifier>oai:pubmedcentral.nih.gov:12405556</identifier>
    <datestamp>2025-09-04</datestamp>
    
        
        <setSpec>scirep</setSpec>
        
    
        
        <setSpec>pmc-open</setSpec>
        
    
</header>
    <metadata>
        
        <article xmlns="https://jats.nlm.nih.gov/ns/archiving/1.4/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:ali="http://www.niso.org/schemas/ali/1.0/" xsi:schemaLocation="https://jats.nlm.nih.gov/ns/archiving/1.4/ https://jats.nlm.nih.gov/archiving/1.4/xsd/JATS-archivearticle1-4.xsd" article-type="research-article" xml:lang="en" dtd-version="1.4"><processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats"><restricted-by>pmc</restricted-by></processing-meta><front><journal-meta><journal-id journal-id-type="nlm-ta">Sci Rep</journal-id><journal-id journal-id-type="iso-abbrev">Sci Rep</journal-id><journal-id journal-id-type="pmc-domain-id">1579</journal-id><journal-id journal-id-type="pmc-domain">scirep</journal-id><journal-title-group><journal-title>Scientific Reports</journal-title></journal-title-group><issn pub-type="epub">2045-2322</issn><publisher><publisher-name>Nature Publishing Group</publisher-name></publisher></journal-meta><article-meta><article-id pub-id-type="pmcid">PMC12405556</article-id><article-id pub-id-type="pmcid-ver">PMC12405556.1</article-id><article-id pub-id-type="pmcaid">12405556</article-id><article-id pub-id-type="pmcaiid">12405556</article-id><article-id pub-id-type="pmid">40897745</article-id><article-id pub-id-type="doi">10.1038/s41598-025-15520-9</article-id><article-id pub-id-type="publisher-id">15520</article-id><article-version article-version-type="pmc-version">1</article-version><article-categories><subj-group subj-group-type="heading"><subject>Article</subject></subj-group></article-categories><title-group><article-title>Enhancing interior design and space planning via human&#8211;machine intelligent interaction for artistic cognition</article-title></title-group><contrib-group><contrib contrib-type="author" corresp="yes"><name name-style="western"><surname>Jiang</surname><given-names initials="J">Jiatong</given-names></name><address><email>a138672@correo.umm.edu.mx</email></address><xref ref-type="aff" rid="Aff1"/></contrib><aff id="Aff1"><institution-wrap><institution-id institution-id-type="ROR">https://ror.org/0493m8x04</institution-id><institution-id institution-id-type="GRID">grid.459579.3</institution-id><institution>School of Design, </institution><institution>Shenzhen City Polytechnic, </institution></institution-wrap>Shenzhen, 5180381 Guangdong Province China </aff></contrib-group><pub-date pub-type="epub"><day>2</day><month>9</month><year>2025</year></pub-date><pub-date pub-type="collection"><year>2025</year></pub-date><volume>15</volume><issue-id pub-id-type="pmc-issue-id">478255</issue-id><elocation-id>32344</elocation-id><history><date date-type="received"><day>8</day><month>7</month><year>2025</year></date><date date-type="accepted"><day>8</day><month>8</month><year>2025</year></date></history><pub-history><event event-type="pmc-release"><date><day>02</day><month>09</month><year>2025</year></date></event><event event-type="pmc-live"><date><day>04</day><month>09</month><year>2025</year></date></event><event event-type="pmc-last-change"><date iso-8601-date="2025-09-04 00:25:59.930"><day>04</day><month>09</month><year>2025</year></date></event></pub-history><permissions><copyright-statement>&#169; The Author(s) 2025</copyright-statement><copyright-year>2025</copyright-year><license><ali:license_ref specific-use="textmining" content-type="ccbyncndlicense">https://creativecommons.org/licenses/by-nc-nd/4.0/</ali:license_ref><license-p><bold>Open Access</bold> This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or other third party material in this article are included in the article&#8217;s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article&#8217;s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc-nd/4.0/">http://creativecommons.org/licenses/by-nc-nd/4.0/</ext-link>.</license-p></license></permissions><self-uri content-type="pmc-pdf" xlink:href="41598_2025_Article_15520.pdf"/><abstract id="Abs1"><p id="Par1">Space planning and interior design require not only technical precision but also creative thinking and spatial awareness. Although earlier research has examined the cognitive and educational elements that influence spatial ability, such as fuzzy DEMATEL and ISM-based models, these studies lack real-time decision-making support, machine-aided creativity, and practical implementation. To overcome these limitations, this study suggests an intelligent framework for enhancing interior design and space planning. The originality of this work lies in bridging spatial ability cognitive insights and machine interaction from a human-centered perspective to real-world design applications and an AI-powered design process. The proposed work combines hybrid models, including CenterNet and StyleGAN3, along with a transformer model. In order to read user intent or constraints (text, sketch, or spatial signals), the suggested system first uses CenterNet to detect and map spatial aspects. StyleGAN3 relies on these semantic and spatial features to generate interior visuals that are congruent with style. Finally, the transformer model is used as a sequence-aware logic engine responsive to user interaction. As opposed to existing work that focuses mainly on instructional modeling, this work presents machine-aided co-design with enhanced utility and beauty as well as user preference consideration. Categorized user intents, room layout, and high-resolution interior style images were some of the data applied to the experimental evaluation. While StyleGAN3 generated high-quality and varied outputs (FID&#8201;=&#8201;11.6), the suggested system used CenterNet to achieve excellent results in spatially accurate localization (mAP@0.75&#8201;=&#8201;84.3%). Human assessments with artists revealed increased usability satisfaction and support for creativity (avg. score: 4.5/5). This demonstrates the effectiveness of integrating generative artistic cognition with organized spatial thinking within a collaborative AI system.</p></abstract><kwd-group xml:lang="en"><title>Keywords</title><kwd>Interior design</kwd><kwd>Space planning</kwd><kwd>Intelligent interaction</kwd><kwd>Artistic cognition</kwd><kwd>Deep learning</kwd></kwd-group><custom-meta-group><custom-meta><meta-name>pmc-status-qastatus</meta-name><meta-value>0</meta-value></custom-meta><custom-meta><meta-name>pmc-status-live</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-status-embargo</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-status-released</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-open-access</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-olf</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-manuscript</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-legally-suppressed</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-pdf</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-has-supplement</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-pdf-only</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-suppress-copyright</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-real-version</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-is-scanned-article</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-preprint</meta-name><meta-value>no</meta-value></custom-meta><custom-meta><meta-name>pmc-prop-in-epmc</meta-name><meta-value>yes</meta-value></custom-meta><custom-meta><meta-name>issue-copyright-statement</meta-name><meta-value>&#169; Springer Nature Limited 2025</meta-value></custom-meta></custom-meta-group></article-meta></front><body><sec id="Sec1"><title>Introduction</title><p id="Par2">Artificial intelligence (AI) has slowly made its way into a number of industries, particularly interior design and construction, thanks to technological advancements. In this regard, integrating AI with spatial planning and interior design to enhance the aesthetics and usability of design has grown in importance as a study area. Interior design is the ideal fusion of technology and artistic endeavour in addition to the arrangement of area and purpose. Conventional interior design techniques frequently lack a thorough awareness of user wants and behaviours in favour of concentrating on the subjective abilities and expertise of architects. A well-studied aspect of human cognitive functioning<sup><xref ref-type="bibr" rid="CR1">1</xref></sup>, spatial talents are also regarded as powerful predictors of scholastic success and creativity in design-related professions<sup><xref ref-type="bibr" rid="CR2">2</xref>&#8211;<xref ref-type="bibr" rid="CR4">4</xref></sup>.</p><p id="Par3">The worldwide need for interior design is enormous, yet current design processes or methods might not be able to fully satisfy these demands<sup><xref ref-type="bibr" rid="CR5">5</xref>&#8211;<xref ref-type="bibr" rid="CR7">7</xref></sup>. The complexity of the interior design procedure and the frequent modifications that result in low design efficiency are two causes of this problem. Additionally, in an effort to save time, designers create set design techniques, which hinders creativity<sup><xref ref-type="bibr" rid="CR8">8</xref>,<xref ref-type="bibr" rid="CR9">9</xref></sup>. Thus, addressing the lack of originality and increasing interior design efficiency are crucial. The dispersal model&#8217;s impression offers an answer to the issues of interior design&#8217;s lack of thoughts and low capability. The diffusion method has the benefit of being able to learn preceding expertise from vast amounts of text and image explanation pairing data<sup><xref ref-type="bibr" rid="CR10">10</xref>,<xref ref-type="bibr" rid="CR11">11</xref></sup>. By batch-inputting text imageries, a learned diffusion perfect can produce various and excellent depictions. Designers can generate design ideas in batches by using the dispersion method for interior design. This method can meaningfully growth designing and advanced manufacture efficiency.</p><p id="Par4">Research has been done on architectural features that are particularly susceptible to individuation, namely building structures, development, and structure. These features are supported by a comprehensive foundation of rules and norms<sup><xref ref-type="bibr" rid="CR12">12</xref></sup>. But there are other elements at play as well. The intellectual level, which is defined as the analysis and evaluation of actual knowledge, and the subjective level, which is defined as the adaptive responses to the observed data, are both impacted by the surroundings and are mediated by intricately linked systems<sup><xref ref-type="bibr" rid="CR13">13</xref></sup>. Research on particular areas has revealed a range of cognitive-emotional effects, including worse patient outcomes in hospital rooms without soothing outdoor views of vegetation<sup><xref ref-type="bibr" rid="CR14">14</xref></sup>. As a result, the structure affects cognitive and mental processes. Furthermore, the difficulty of evaluating spatial ability in real life and the continuous disagreement in the literature regarding its definition make it impossible to investigate the influence.</p></sec><sec id="Sec2"><title>Research gaps</title><p id="Par5">In spite of growing interest in spatial aptitude and interior design education, recent research is mostly oriented towards determining human cognitive factors using survey-based and analytical techniques like Fuzzy DEMATEL and ISM. Although these studies offer theoretical understanding of the impact of characteristics such as sketching, mathematical abilities, and AR practice on spatial aptitude, they do not constitute real-time, system-level implementation that maps this understanding to practical design support. In addition, there is a substantial shortfall in utilizing AI-based models to augment artistic understanding and spatial decision-making interactively. Current generative or detection models in design use cases usually operate independently, without unifying spatial layout comprehension, aesthetic creation, and cognitive reasoning into one workflow. There is also minimal investigation into human&#8211;machine collaborative creation environments that can assist designers in real-time, adaptive, and user-centric manners. There isn&#8217;t any preceding work that has connected the educational research on space ability to a fully operational intelligent system capable of both reading space and producing high-quality interior images while being aware of user limitations and design intents. This leaves an important research void for creating a combined, cognitively conscious, and generative human&#8211;computer interaction model with specific application to interior design and space planning.</p><p id="Par6">Although the previous studies have added value to the comprehension of spatial ability through techniques like Fuzzy DEMATEL and ISM, they remain theoretical and do not transform into real-time, actionable design assistance. They do not cross the bridge of cognitive models of spatial thinking and smart design tools. Additionally, current AI-based methods in design usually work in isolation object detection models such as CenterNet can only handle static spatial recognition; StyleGAN models prioritize high-fidelity generation without contextual cognition; and Transformer-based methods typically get used in language or classification scenarios, not in creative design settings. This piece bridged that gap by proposing a uniform, real-time system that translates human mental signals (e.g., emotional and perceptual attention) into spatial perception and visual creation. In synchronizing perception, interpretation, and creation, the system provides not only technologically optimized layouts but also emotionally engaging and user-adaptive designs an uncharted path in interior design and AI interaction literature.</p><sec id="Sec3"><title>Objectives</title><p id="Par7">
<list list-type="bullet"><list-item><p id="Par8">The main objective of this work is to progress an intelligent and communicating platform that combine human cognitive inputs and machine-based design construction to increase the design of interiors and establishing space.</p></list-item><list-item><p id="Par9">Based on the mixture of CenterNet to exactly recognize spatial construction, Transformer models for reasoning about user intent and contextualized knowledge, and</p></list-item><list-item><p id="Par10">StyleGAN3 for emerging visually attractive and artistically coherent strategies for interiors, the system is exactly made to support artistic thinking.</p></list-item><list-item><p id="Par11">This research endeavors to overcome traditional static spatial modeling by being able to provide dynamic human&#8211;machine collaboration within the design process, thereby allowing for creativity, functional optimization, and real-time feedback.</p></list-item><list-item><p id="Par12">A second fundamental goal is to overcome the shortcomings of previous research, which mainly addressed educational or cognitive modeling of spatial skills without providing useful design support tools.</p></list-item><list-item><p id="Par13">With the integration of AI-based spatial knowledge, generative artistic capability, and natural language interaction, the new approach aims at filling the gap between theoretical spatial cognition.</p></list-item><list-item><p id="Par14">In addition, the system is intended to enable users with different skill levels ranging from students to design professionals by offering a co-creative environment that supports improved decision-making and design outcomes.</p></list-item></list>
</p></sec><sec id="Sec4"><title>Main contributions</title><p id="Par15">The main contribution of the proposed method is given below:<list list-type="bullet"><list-item><p id="Par16">This work&#8217;s primary contribution is the creation and implementation of a novel multimodal framework that combines generative design, spatial intelligence, and cognitive cues into a seamless architecture that facilitates intelligent interior design.</p></list-item><list-item><p id="Par17">Initially, introduces a fusion mechanism from cognitive-visual that merges user perception (obtained through face recognition and attention signals) with spatial object properties (detected through CenterNet).</p></list-item><list-item><p id="Par18">This allows the system to generate perceptually aligned and spatially grounded design outputs.</p></list-item><list-item><p id="Par19">Next, establishes a real-time pipeline that integrates three state-of-the-art models such as Transformer, CenterNet, and StyleGAN3 into one architecture that is capable of both inferring user intent and creating stylistically consistent, spatially correct interior designs.</p></list-item><list-item><p id="Par20">Then, our system is both qualitatively and quantitatively assessed, demonstrating unequivocal performance improvement across main evaluation metrics: enhanced realism in images (FID lowered to 19.2), better spatial coherence (SSIM&#8201;=&#8201;0.74), and high human satisfaction (avg. score: 4.3/5).</p></list-item><list-item><p id="Par21">Finally, performing an ablation study that illustrates how each element contributes to the performance of the system, thereby confirming the merit of our integrated design methodology.</p></list-item><list-item><p id="Par22">Collectively, these contributions move the frontiers of human-AI collaboration in interior design.</p></list-item></list></p><p id="Par23">The remainder of the document is organized as follows: The research Interior Design, Space Planning, Intelligent Interaction, Artistic Cognition, Deep Learning, the issues being studied, and the methodology are all covered in Section "<xref rid="Sec5" ref-type="sec">Literature survey</xref>". The efficacy of the suggested methodology is shown in Section "<xref rid="Sec6" ref-type="sec">Proposed methodology</xref>", the algorithm results are described in Section "<xref rid="Sec17" ref-type="sec">Result analysis</xref>", and a commentary on the overall findings and possibilities for further research are provided in Section "<xref rid="Sec25" ref-type="sec">Discussions</xref>". Lastly, the paper is concluded in Section "<xref rid="Sec31" ref-type="sec">Conclusion</xref>".</p></sec></sec><sec id="Sec5"><title>Literature survey</title><p id="Par24">The practical uses of artificial intelligence have expanded in number as research in the subject has advanced. The use of deep learning to forecast the components&#8217; life expectancy was examined by the author. In order to obtain bearing defect feature signals in accordance with ISO standards, a two-stage Long Short-Term Memory (LSTM) model was devised&#160;<sup><xref ref-type="bibr" rid="CR15">15</xref></sup>. Gaussian layers were then embedded for parameter optimization. When it came to bearing life prediction, this model performed well<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR17">17</xref></sup>. By combining spatiotemporal AM with ConvLSTM, the scientists developed a unique Xception-LSTM method that increases the precision of false face identification. In order to take frame structure details into account when modeling temporal information, ConvLSTM was developed. According on the effectiveness outcomes on three datasets, this approach outperformed other algorithms<sup><xref ref-type="bibr" rid="CR18">18</xref>,<xref ref-type="bibr" rid="CR19">19</xref></sup>.</p><p id="Par25">The researcher improved the universality and accuracy of the Convolutional Neural Network (CNN) by employing totally open imaging data to rapidly identify intracranial haemorrhage (ICH). Using CNN to detect ICH at many institutions became more accurate and widespread because to this network<sup><xref ref-type="bibr" rid="CR20">20</xref>,<xref ref-type="bibr" rid="CR21">21</xref></sup>. The author concentrated on the issue of detecting pedestrians across long distances in assisted driving and intelligent surveillance. Based on the CenterNet detection paradigm, they created a triple ResNet network to increase operational accuracy and lower the undetected identification rate in small-scale recognition of pedestrians. Three distinct basic blocks were included into this network, each of which extracted pedestrian data to increase information flow and network architectural correctness. This approach demonstrated exceptional speed and accuracy in detecting pedestrians, particularly on small areas<sup><xref ref-type="bibr" rid="CR22">22</xref></sup>.</p><p id="Par26">It is essential for designers and design professionals to comprehend scientific ideas and mathematical concepts like material properties and structural engineering. But they also require originality, an intense awareness of appearance, and the capacity to visualize and express their thoughts visually. When art and science are combined in various domains, new, beautiful, and useful solutions that improve the lives of people can be produced. The move from STEM to STEAM is a response to a validation of the interrelatedness of different disciplines and the need for developing an all-around education<sup><xref ref-type="bibr" rid="CR23">23</xref></sup>. Through incorporating arts in the curriculum, we can enhance creativity, critical thinking, and problem-solving ability while also readying students for professional careers in such disciplines as architecture and interior design.</p><p id="Par27">The researcher created an efficient Convolutional Neural Network (CNN) based on fully open image data to efficiently diagnose intracranial haemorrhage (ICH) and enhance its universality and accuracy. The network enhanced the universality and accuracy of diagnosing ICH using CNN in various institutions<sup><xref ref-type="bibr" rid="CR16">16</xref>,<xref ref-type="bibr" rid="CR24">24</xref>,<xref ref-type="bibr" rid="CR25">25</xref></sup>. The researcher closed pedestrian distance detection in smart monitoring and assisted driving. In order to decrease the missed detection rate in small-scale pedestrian detection and enhance the accuracy of operations, they proposed a triple ResNet network on the basis of the CenterNet detection model. The network combined three distinct basic blocks, each extracting pedestrian information to improve information flow and enhance accuracy in the network architecture. This approach demonstrated superior accuracy and detection speed, particularly in small-scale pedestrian detection<sup><xref ref-type="bibr" rid="CR26">26</xref></sup>.</p><p id="Par28">As opposed to earlier studies where deep learning models are independently used in design pipelines, our method wisely combines three complementary models such as CenterNet for spatial detection, StyleGAN3 for generative image synthesis, and a Transformer model for reasoning over human cognitive signals. CenterNet has been used before mainly for pedestrian or object detection, StyleGANs for stylization of image generation, and Transformers for classification or captioning. Nonetheless, to the best of our knowledge, no previous work has combined these architectures to enable adaptive, real-time co-design for interior design. This inter-disciplinary integration enables the system to actively adapt to user intent and spatial limitations, providing a new paradigm for enhancing creativity and decision-making in design. It is an important step toward multimodal, perceptually sensitive design automation, progressing beyond static procedural aids to insightful, user-guided interaction.</p></sec><sec id="Sec6"><title>Proposed methodology</title><p id="Par29">The proposed methodology helps to improve interior design and space planning by adopting human&#8211;machine intelligent interaction. It uses a hybrid methodology such as CenterNet&#8201;+&#8201;StyleGAN3 with transformer model for enhancing interior design and spatial space. Human cognitive input in the given framework denotes real-time user-oriented signals like facial expressions, eye gaze directions, and inferred emotional states that capture the user&#8217;s affective and perceptual responses. Such inputs are picked up by visual sensors and processed with a lightweight transformer-based encoder and mapped into semantic control tokens. These tokens are subsequently employed to instruct the CenterNet object detection module to target regions of interest pertaining to the user&#8217;s intention. The features detected, coupled with cognitive control cues, are then employed to condition the StyleGAN model such that context-sensitive and perceptually aligned synthetic results can be generated. Such multi-stage fusion enables the output not only to be data-driven but also representative of the user&#8217;s cognitive state. The system flow is graphically represented in Fig.&#160;<xref rid="Fig1" ref-type="fig">1</xref>, with the interaction of each module clearly marked.<fig id="Fig1" position="float" orientation="portrait"><label>Fig. 1</label><caption><p>The suggested human cognitive-guided synthetic generation system&#8217;s workflow. A transformer converts facial expressions into semantic tokens, which are then combined with CenterNet-identified object features from scene input. StyleGAN is conditioned by these fused features to produce synthetic outputs that are aware of context.</p></caption><graphic id="d33e366" position="float" orientation="portrait" xlink:href="41598_2025_15520_Fig1_HTML.jpg"/></fig></p><p id="Par30">Figure&#160;<xref rid="Fig1" ref-type="fig">1</xref> depicts the global architecture of the proposed framework of human cognitive-guided synthetic generation. The process starts with the collection of human facial expressions using an image capture module. These expressions, indicative of the user&#8217;s cognitive and emotional states, are fed into a Transformer Encoder, which converts them into semantic tokens that capture the focus or intent of the user. Concurrently, the CenterNet object detection model is applied to a scene image in order to extract important visual features and regions of interest. A feature fusion mechanism is then used to combine the object features and semantic tokens, creating an integrated representation that aligns visual context with perceptual intent. StyleGAN receives this combined representation and creates a context-sensitive synthetic image that reflects the user&#8217;s environment and mental state. This procedure ensures that the output produced is based on the visual context of the scene, user-tailored, and perceptually consistent.</p><sec id="Sec7"><title>Data acquisition</title><p id="Par31">In this research, data collection combines spatial arrangement identification, aesthetic design creation, and cognitive feedback simulation. The previous study<sup><xref ref-type="bibr" rid="CR1">1</xref></sup> investigates spatial skills in interior design students through survey-based variables such as visualization, design thinking, and environmental awareness. In order to facilitate the improvement of interior design and space planning based on smart human&#8211;machine interaction, there is a need for multimodal data. Three deep learning models are used in the system proposed here: CenterNet for detecting spatial layout, StyleGAN3 for generating aesthetic and artistic interior designs, and Transformers for understanding context as well as interacting. Therefore, data collection consists of three streams: room layouts based on images, high-resolution interior design images, and textual design briefs or descriptions of space. These data sets are either available publicly or can be compiled from real-world building blueprints, home design magazines, and labeled design commentaries. The gathered data must be pre-processed into annotated forms such as bounding boxes (for CenterNet), style-consistent image data sets (for StyleGAN3), and natural language descriptions (for Transformer models) to effectively train and fine-tune the system. In this work, Space Planning, Interior Design is evaluated and the dataset used for evaluation is collected from openly available dataset. Table <xref rid="Tab1" ref-type="table">1</xref> shows the dataset details of interior design, space planning and textual designs.<table-wrap id="Tab1" position="float" orientation="portrait"><label>Table 1</label><caption><p>Dataset details.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Dataset name</th><th align="left" colspan="1" rowspan="1">Content type</th><th align="left" colspan="1" rowspan="1">Used for</th><th align="left" colspan="1" rowspan="1">Size/scope</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Structured3D<sup><xref ref-type="bibr" rid="CR27">27</xref></sup></td><td align="left" colspan="1" rowspan="1">Annotated room layouts (floorplans)</td><td align="left" colspan="1" rowspan="1">CenterNet (space planning)</td><td align="left" colspan="1" rowspan="1">10,000&#8201;+&#8201;images with room labels</td></tr><tr><td align="left" colspan="1" rowspan="1">MIT indoor scenes<sup><xref ref-type="bibr" rid="CR28">28</xref></sup></td><td align="left" colspan="1" rowspan="1">Scene classification images</td><td align="left" colspan="1" rowspan="1">StyleGAN3 pretraining</td><td align="left" colspan="1" rowspan="1">15,620 images over 67 indoor categories</td></tr><tr><td align="left" colspan="1" rowspan="1">COCO captions<sup><xref ref-type="bibr" rid="CR29">29</xref></sup></td><td align="left" colspan="1" rowspan="1">Image descriptions</td><td align="left" colspan="1" rowspan="1">Transformer (text interface)</td><td align="left" colspan="1" rowspan="1">330&#160;K images with 5 captions each</td></tr></tbody></table></table-wrap></p><p id="Par32">Table <xref rid="Tab1" ref-type="table">1</xref> shows how accurately each dataset is used. The Transformer module is trained using the MIT Indoor Scenes dataset, while StyleGAN is trained solely using the LSUN Bedroom dataset. The previous discrepancy, in which MIT was incorrectly applied to StyleGAN, is resolved by this update.</p></sec><sec id="Sec8"><title>Object detection using CenterNet</title><p id="Par33">This paper chose the CenterNet algorithm as the base network to build an interior design component recognition network. For better improving the precision of the algorithm for component detection from complicated backgrounds and dense target areas, AM and feature fusion methods were applied to enhance the algorithm. The benefit of CenterNet algorithm is its simplicity and effectiveness. It simplifies computational complexity and enhances detection speed by predicting the center point of target rather than conventional bounding box regression. Structurally, it is primarily a simple CNN and an object center prediction head, which can deliver real-time capability with high accuracy. By adopting this method, CenterNet has delivered precise detection of multi-scale and multi type targets and has proved highly flexible and accurate. With the addition of AM, this research makes the network to more precisely aim towards target dense regions, and feature fusion technology enables the network to learn contextual information in various scales, further enhancing component recognition accuracy and robustness. ResNet is used as the CenterNet algorithm&#8217;s backbone feature extraction network, inserting a light Convolutional Block Attention Module (CBAM).</p><p id="Par34">Let the input image be represented by <inline-formula id="IEq1"><alternatives><tex-math id="d33e454">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$I \in {\mathbb{R}}^{H \times W \times 3}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_15520_Article_IEq1.gif"/></alternatives></inline-formula>. The model predicts:</p><p id="Par35">A heatmap is to predict object centers per class.<disp-formula id="Equ1"><label>1</label><alternatives><tex-math id="d33e462">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{Y} \in [0,1]^{{H^{`} \times H^{`} \times C}}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ1.gif"/></alternatives></disp-formula></p><p id="Par36">A size map is to predict width and height.<disp-formula id="Equ2"><label>2</label><alternatives><tex-math id="d33e470">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{S} \in {\mathbb{R}}^{{H^{`} \times W^{`} \times 2}}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ2.gif"/></alternatives></disp-formula></p><p id="Par37">An offset map is to adjust for down-sampling errors.<disp-formula id="Equ3"><label>3</label><alternatives><tex-math id="d33e478">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{O} \in {\mathbb{R}}^{{H^{`} \times W^{`} \times 2}}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ3.gif"/></alternatives></disp-formula></p><p id="Par38">For a center point <inline-formula id="IEq2"><alternatives><tex-math id="d33e486">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\left( {x_{c} ,\;y_{c} } \right)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_15520_Article_IEq2.gif"/></alternatives></inline-formula>, a 2D Gaussian is plotted on the heatmap:<disp-formula id="Equ4"><label>4</label><alternatives><tex-math id="d33e492">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$Y_{x,y,c} = \exp \left( { - \frac{{(x - x_{c} )^{2} + (y - y_{c} )^{2} }}{{2\sigma^{2} }}} \right)$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ4.gif"/></alternatives></disp-formula></p><p id="Par39">For every center identified:<disp-formula id="Equ5"><label>5</label><alternatives><tex-math id="d33e500">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{{{\text{size}}}} = \frac{1}{N}\mathop \sum \limits_{i = 1}^{N} |\hat{s}_{i} - s_{i} |_{1}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ5.gif"/></alternatives></disp-formula>where <inline-formula id="IEq3"><alternatives><tex-math id="d33e507">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$s_{i} = \left( {w_{i} ,h_{i} } \right)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_15520_Article_IEq3.gif"/></alternatives></inline-formula> and <inline-formula id="IEq4"><alternatives><tex-math id="d33e513">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{s}_{i}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_15520_Article_IEq4.gif"/></alternatives></inline-formula> is the estimated size.</p><p id="Par40">The Offset Loss is defined as:<disp-formula id="Equ6"><label>6</label><alternatives><tex-math id="d33e521">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{{{\text{offset}}}} = \frac{1}{N}\mathop \sum \limits_{i = 1}^{N} |\hat{o}_{i} - o_{i} |_{1}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ6.gif"/></alternatives></disp-formula></p><p id="Par41">The total loss is<disp-formula id="Equ7"><label>7</label><alternatives><tex-math id="d33e529">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$L_{{{\text{total}}}} \; = \;L_{{{\text{heatmap}}}} \; + \;\lambda_{1} L_{{{\text{size}}}} \; + \;\lambda_{2} L_{{{\text{offset}}}}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ7.gif"/></alternatives></disp-formula></p><p id="Par42">The CenterNet-based layout recognition pipeline for interior design and space planning tasks is illustrated in the Fig.&#160;<xref rid="Fig2" ref-type="fig">2</xref>. It provides a structured overview of how deep learning is applied to recognize and analyze spatial objects in a room layout image. A room layout image, e.g., a 3D-rendered floor plan, serves as the input of the process at the beginning. Visible architectural elements such as walls, furniture, and partitions exist in this image, providing the visible cues necessary for layout identification. A deep convolutional network, specifically a ResNet backbone, takes in the input and utilizes it to extract multi-level features of the image. For room part localization and identification, the ResNet consists of several residual blocks that progressively abstract spatial and semantic information.<fig id="Fig2" position="float" orientation="portrait"><label>Fig. 2</label><caption><p>Structure of CenterNet.</p></caption><graphic id="d33e546" position="float" orientation="portrait" xlink:href="41598_2025_15520_Fig2_HTML.jpg"/></fig></p><p id="Par43">The network partitions into three specialized heads following feature extraction:<list list-type="bullet"><list-item><p id="Par44">Every pixel of the heatmap produced by the HeatMap Head represents the level of confidence in an item center, or keypoint, for various pieces of furniture or structural features. This helps to figure out each object&#8217;s exact center point.</p></list-item><list-item><p id="Par45">Size head: This is essential for bounding box creation because it employs regression from the corresponding center point to predict the height and width of the detected object.</p></list-item><list-item><p id="Par46">Offset head: Compensates for any localization error introduced by downsampling within the backbone network by predicting sub-pixel offsets to sharpen the detected center location.</p></list-item></list></p><p id="Par47">To create accurate bounding boxes with representation of the position and size of detected objects within the room, these results center points, sizes, and offsets then get concatenated.</p><p id="Par48">In the proposed method, the CenterNet module is used only for object detection. It detects and localizes objects of interest in the scene by predicting object bounding boxes directly from the input image without any pre-defined anchor boxes. Previous mentions of CenterNet doing &#8220;layout recognition&#8221; have been rectified, as no holistic spatial layout parsing is performed by the model. Its output is positional and category information of the objects detected, which are then used in conjunction with cognitive tokens from the transformer to guide the generation process in StyleGAN. This adjustment ensures consistency between the model&#8217;s real function and its explanation in the manuscript.</p></sec><sec id="Sec9"><title>Design generation using StyleGAN3</title><p id="Par49">StyleGAN3<sup><xref ref-type="bibr" rid="CR30">30</xref></sup>, the most recent model in NVIDIA&#8217;s StyleGAN family, provides a quantity of architectural enhancements over StyleGAN and StyleGAN2. The alias-free construction of StyleGAN3 is one of its most prominent features. The aliasing problem, which regularly results in visual artifacts in synthetic images and becomes more obvious when the images are scaled or rotated, is resolved by this architecture. StyleGAN3 provides a level of image consistency and perceptiveness mainly helpful for dynamic styles through confirming that each process in the generator network, from input to output, is expressed with the aim of evading aliasing. StyleGAN3&#8217;s time-based cohesion focus, which is important in uses such as video generation, is also a valuable invention. A key feature of attaining normal simulations and pictures, the feature ensures that minor changes to the latent space variables lead to even and natural changes in the subsequent imagery. StyleGAN3&#8217;s style-generative generator architecture, allowing for precise operation of various image possessions through altering dissimilar parts of the model&#8217;s latent space, supports such features. This improved process improves temporal constancy as well as contents the circumstances of alias-free generation.</p><p id="Par50">StyleGAN3 is a generative adversarial network (GAN) that is optimized for high-spatial consistency image synthesis, making it suitable for producing interior design layouts from a semantic or latent representation. Central to StyleGAN3 is a style-based modulation mapping from a latent vector to a highly structured image. The Transformer encoder handles visual cognitive input, i.e., facial expressions or attention signals, instead of text input as was previously suggested. Previous mentions of the transformer "translating user input messages" have been corrected. Instead, the transformer is now correctly stated to carry out semantic scene understanding and label attention generation from visual input. These semantic features are used to guide attention mechanisms during generation and are integrated with CenterNet object detection outputs. The transformer is learned and tested as a classification model, with its performance measured in terms of F1-score metrics, in accordance with its application in image-based perception understanding tasks. There are two primary parts to the generation: the mapping network and the synthesis network.</p><p id="Par51">Let: z&#8201;&#8712;&#8201;Z be a Gaussian-sampled latent input vector from N(0, I), w&#8201;=&#8201;M(z)&#8201;&#8712;&#8201;W be the style vector, where M is an MLP, A<sub>i</sub>(w) be the adaptive instance normalization (AdaIN) at the i-th synthesis network layer.</p><p id="Par52">
<table-wrap id="Taba" position="float" orientation="portrait"><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1"><bold>Input</bold>: Latent vector z&#8201;&#8712;&#8201;&#8477;<sup>&#8319;</sup> (random noise), Pre-trained Mapping Network M, Pre-trained Synthesis Network S, Optional: Conditioning input (layout map, style code)</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1"><bold>Output</bold>: Generated interior design image I&#8201;&#8712;&#8201;&#8477;<sup>(H&#215;W&#215;3)</sup></td></tr><tr><td align="left" colspan="1" rowspan="1">Step 1: Sample latent vector z&#8201;~&#8201;(0, I) from normal distribution</td></tr><tr><td align="left" colspan="1" rowspan="1">Step 2: Pass z through mapping network M:</td></tr><tr><td align="left" colspan="1" rowspan="1">&#160;&#160;&#160;w&#8201;=&#8201;M(z)</td></tr><tr><td align="left" colspan="1" rowspan="1">Step 3: For each layer i in the synthesis network S:</td></tr><tr><td align="left" colspan="1" rowspan="1">&#160;&#160;&#160;a. Apply style modulation using w:</td></tr><tr><td align="left" colspan="1" rowspan="1">&#160;&#160;&#160;&#160;&#160;scale_i, shift_i&#8201;=&#8201;Modulate(w)</td></tr><tr><td align="left" colspan="1" rowspan="1">&#160;&#160;&#160;b. Perform convolution:</td></tr><tr><td align="left" colspan="1" rowspan="1">&#160;&#160;&#160;&#160;&#160;f_i&#8201;=&#8201;Conv_i(f_{i-1})</td></tr><tr><td align="left" colspan="1" rowspan="1">&#160;&#160;&#160;c. Apply Adaptive Instance Normalization:</td></tr><tr><td align="left" colspan="1" rowspan="1">&#160;&#160;&#160;&#160;&#160;f_i&#8201;=&#8201;AdaIN(f_i, scale_i, shift_i)</td></tr><tr><td align="left" colspan="1" rowspan="1">&#160;&#160;&#160;d. Optionally inject noise or conditioning input</td></tr><tr><td align="left" colspan="1" rowspan="1">Step 4: Combine outputs progressively from lower to higher resolutions</td></tr><tr><td align="left" colspan="1" rowspan="1">Step 5: Apply final convolution and activation to obtain RGB image I</td></tr><tr><td align="left" colspan="1" rowspan="1">Step 6: Return generated image I</td></tr></tbody></table></table-wrap>
</p><p>Algorithm 1 StyleGAN3 interior design generation.</p><p id="Par53">The following algorithm 1 describes interior design generation with StyleGAN3, starting with sampling of a latent vector z that acts as a seed to the image generation. This vector is fed into a Mapping Network, an MLP that converts z into an intermediate style vector w, separating high-level semantics from randomness. Then, the style vector w is applied to modulation layers in the Synthesis Network, composed of a sequence of convolutional blocks. At block i, the algorithm calculates scale and shift parameters from w, and these are used through Adaptive Instance Normalization (AdaIN) to regulate style features such as lighting, furniture style, or symmetry of layout. These feature maps, normalized and modulated, are successively transmitted in progressively deeper layers, raising resolution at each iteration.</p><p id="Par54">Optionally, noise inputs or layout conditions (e.g., a CenterNet-determined spatial map) may be injected to inform object placement or increase diversity. Lastly, a convolutional layer maps the feature map to an RGB image, resulting in a highly detailed, stylistically consistent interior design. This technique is strong in producing high-definition and user-defined interior designs that capture both stylistic taste and acquired architectural reasoning, rendering it well-suited for use in human-AI collaborative design systems.</p><p id="Par55">The StyleGAN module is trained on the LSUN Bedroom dataset, which contains high-resolution indoor scenes with regular structure. The dataset is well-suited for training generative models to generate realistic room configurations and decorations. The model employs combined features of cognitive and visual modules to condition generation. Earlier incorrect citations regarding the use of MIT Indoor Scenes for this module have now been removed.</p></sec><sec id="Sec10"><title>User interaction using transformer model</title><p id="Par56">In Interior Design and Space Planning through Human&#8211;Machine Interaction, the Transformer model is utilized to translate user input messages into structured outputs that inform CenterNet (for space planning) and StyleGAN3 (for style generation). This is made possible through the sequence modeling skill of Transformers with special importance on semantic reasoning and design intent comprehension. In order to depict dependencies between words or tokens without repetition, the Transformer operates on the unrolled self-attention and positional embeddings.</p><sec id="Sec11"><title>Positional encoding for input embedding</title><p id="Par57">Each character in the sequence <inline-formula id="IEq5"><alternatives><tex-math id="d33e659">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{x}} = \left( {x_{1} ,x_{2} , \ldots ,x_{n} } \right)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_15520_Article_IEq5.gif"/></alternatives></inline-formula> is embedded as follows:<disp-formula id="Equ8"><label>8</label><alternatives><tex-math id="d33e665">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$E = {\text{Embed}}\left( x \right) + PE\left( x \right)$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ8.gif"/></alternatives></disp-formula>where <inline-formula id="IEq6"><alternatives><tex-math id="d33e672">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$PE\left( x \right)$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_15520_Article_IEq6.gif"/></alternatives></inline-formula> is positional encoding to maintain order information and Embed(x) is word embedding.</p></sec><sec id="Sec12"><title>Multi-head attention</title><p id="Par58">To enable the model to pay attention to information from various subspaces:<disp-formula id="Equ9"><label>9</label><alternatives><tex-math id="d33e682">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$${\text{Multihead}}\left( {Q,\;K,\;V} \right)\; = {\text{Concat}}\left( {{\text{head}}_{1} , \ldots ,\;{\text{head}}_{h} } \right)W^{O} {\text{where}}\;{\text{ head}}_{i} \; = \;{\text{Attention}}\left( {QW_{i}^{Q} ,\;KW_{i}^{K} ,\;VW_{i}^{V} } \right)$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ9.gif"/></alternatives></disp-formula></p></sec><sec id="Sec13"><title>Feedforward neural network (FFN)</title><p id="Par59">Post-attention, each position is fed through:<disp-formula id="Equ10"><label>10</label><alternatives><tex-math id="d33e692">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$FFN\left( x \right) = \max \left( {0,xW_{1} + b_{1} } \right)W_{2} + b_{2}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ10.gif"/></alternatives></disp-formula></p></sec><sec id="Sec14"><title>Decoder (for design decision outputs)</title><p id="Par60">When producing outputs (e.g., design instructions or spatial tags), the decoder applies masked self-attention and encoder-decoder attention to generate sequential outputs:<disp-formula id="Equ11"><label>11</label><alternatives><tex-math id="d33e702">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\hat{y}_{t} \; = \;{\text{Decoder}}\left( {y_{ &lt; t} ,\;E} \right)$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ11.gif"/></alternatives></disp-formula>where <inline-formula id="IEq7"><alternatives><tex-math id="d33e709">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$y_{ &lt; t}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_15520_Article_IEq7.gif"/></alternatives></inline-formula> represents previously generated tokens and E is encoded design prompt.</p><p id="Par61">The fundamental architecture of a Transformer model is depicted in the accompanying Fig.&#160;<xref rid="Fig3" ref-type="fig">3</xref>, which features a stacked encoder-decoder framework frequently used in tasks involving natural language processing and human&#8211;computer interaction. An input embedding layer, which begins the process on the left, transforms the raw input tokens such as words or design prompts into dense vectors that the model can understand. Six encoder layers are then used to successively disseminate this embedded input. Feed-forward networks and self-attention mechanisms that discover contextual links between the input elements are part of each self-attention layer in the encoder. Together, these layers provide a detailed representation of the input prompt, encapsulating both the content and the interdependencies between its various components.<fig id="Fig3" position="float" orientation="portrait"><label>Fig. 3</label><caption><p>Working process of transformer model.</p></caption><graphic id="d33e726" position="float" orientation="portrait" xlink:href="41598_2025_15520_Fig3_HTML.jpg"/></fig></p><p id="Par62">On the other side, the Decoder Layers (also six in all) are tasked with outputting structured predictions from the encoded input. Each decoder layer sees both its previous outputs and the encoder&#8217;s outputs, enabling it to make contextually coherent and relevant predictions like spatial layout proposals or style choices in interior design applications. Lastly, the output from the previous decoder layer is fed into an Output Logits layer, where the final probability distribution over potential output tokens (e.g., design tags, layout commands, or aesthetic labels) is computed.</p></sec></sec><sec id="Sec15"><title>Feature fusion</title><p id="Par63">In order to combine cognitive and visual representations, we introduce a weighted fusion process. Let T be semantic token embeddings from the transformer (user perception), and C be object features obtained from CenterNet (visual input). A single scalar weighting parameter &#945; regulates the level of cognitive versus visual input. The fused feature vector F obtained is finally used as a conditioning input to StyleGAN to synthesize perceptually coherent images.</p><p id="Par64">For efficient synthesis of interior design outputs consistent with both perceptual context and user intent, we utilize a weighted fusion mechanism blending cognitive semantics and visual spatial information. To be precise, let <inline-formula id="IEq8"><alternatives><tex-math id="d33e735">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$T \in {\mathbb{R}}^{d}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_15520_Article_IEq8.gif"/></alternatives></inline-formula> be the semantic token vector from the Transformer model (indicative of user cognitive states like emotion or attention), and <inline-formula id="IEq9"><alternatives><tex-math id="d33e741">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$C \in {\mathbb{R}}^{d}$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_15520_Article_IEq9.gif"/></alternatives></inline-formula> be the feature map from CenterNet (indicative of spatial object representations). We posit a fused feature representation F as a convex combination:<disp-formula id="Equ12"><label>12</label><alternatives><tex-math id="d33e747">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F = \alpha .T + \left( {1 - \alpha } \right).C$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ12.gif"/></alternatives></disp-formula>where: T: transformer token representation (cognitive input), C: CenterNet visual feature map, &#945;: attention weighting factor (0&#8201;&#8804;&#8201;&#945;&#8201;&#8804;&#8201;1), F: final fused representation sent to StyleGAN.</p><p id="Par65">where &#945; <inline-formula id="IEq10"><alternatives><tex-math id="d33e756">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$\in$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_15520_Article_IEq10.gif"/></alternatives></inline-formula> [0, 1] is a scalar blending coefficient that controls the balance between cognitive and visual input.</p><p id="Par66">For example, &#945;&#8201;=&#8201;0.7 places cognitive features (e.g., user&#8217;s emotional intent or gaze) in precedence, whereas &#945;&#8201;=&#8201;0.3 puts objective spatial arrangement forward. This adaptive balance facilitates fine-tuning of image synthesis: e.g., prioritizing warmth and artistic imagination in living room scenes when &#945;&#945; is increased, or structural accuracy in office floor plans when &#945;&#945; is decreased.</p><p id="Par67">This fusion process is designed to be lightweight and interpretable, as opposed to typical alternatives that include:<list list-type="bullet"><list-item><p id="Par68">Concatenation: F&#8201;=&#8201;[T; C], which increases the dimensionality by two and usually needs an additional projection layer to make features consistent.</p></list-item><list-item><p id="Par69">Attention-based fusion: F&#8201;=&#8201;Attention (T, C), which provides dynamic weighting but carries large computation and introduces architectural complexity.</p></list-item></list></p><p id="Par70">In our empirical experiments, our weighted fusion dataset demonstrated competitive performance with simplicity and efficiency. The F fused vector is then passed to StyleGAN3&#8217;s conditioning system, allowing context-aware, personalized image generation based on user cognition as well as physical layout.</p></sec><sec id="Sec16"><title>System workflow summary and dataset setup</title><p id="Par71">The step-by-step description shows how each part works within the entire system. It starts with the processing of human cognitive input, that is, facial expression and gaze direction, which are mapped into semantic tokens via a Transformer-based cognitive encoder. These tokens capture the user&#8217;s attention or emotional intent and are used as guidance signals for spatial layout prediction. At the same time, the input scene layout is fed into the CenterNet module to conduct object detection and spatial localization. The Transformer and CenterNet outputs are then fused through a novel cognitive-visual fusion process, mathematically formulated as <inline-formula id="IEq11"><alternatives><tex-math id="d33e779">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F = \alpha .T + \left( {1 - \alpha } \right).C$$\end{document}</tex-math><inline-graphic xlink:href="41598_2025_15520_Article_IEq11.gif"/></alternatives></inline-formula>, with T and C being the cognitive and visual features respectively, and &#945; being a hyperparameter. The fused feature vector is used to condition the StyleGAN3 generator to generate scenes with spatial realism as well as emotional intent. This series is now easily depicted in Fig.&#160;<xref rid="Fig1" ref-type="fig">1</xref>, and described in exact terms to avoid confusion. To aid reproducibility, we also provide a novel Table <xref rid="Tab2" ref-type="table">2</xref> consolidating the datasets utilized within each module&#8212;along with their sample sizes, input types, and resultant outputs. These adjustments not only correct inconsistencies highlighted in the previous version (e.g., opposing roles of datasets and modules) but also ensure transparency and checkability of the workflow. Collectively, these additions strengthen methodological consistency of the work and allow other researchers to replicate or extend this framework.<table-wrap id="Tab2" position="float" orientation="portrait"><label>Table 2</label><caption><p>Summary of dataset usage.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Dataset name</th><th align="left" colspan="1" rowspan="1">Used for</th><th align="left" colspan="1" rowspan="1">Samples</th><th align="left" colspan="1" rowspan="1">Input type</th><th align="left" colspan="1" rowspan="1">Output/target type</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">MIT indoor scenes</td><td align="left" colspan="1" rowspan="1">Transformer</td><td char="," align="char" colspan="1" rowspan="1">15,000</td><td align="left" colspan="1" rowspan="1">Scene images</td><td align="left" colspan="1" rowspan="1">Scene categories</td></tr><tr><td align="left" colspan="1" rowspan="1">Structured3D</td><td align="left" colspan="1" rowspan="1">CenterNet</td><td char="," align="char" colspan="1" rowspan="1">10,000</td><td align="left" colspan="1" rowspan="1">Indoor layouts</td><td align="left" colspan="1" rowspan="1">Bounding boxes</td></tr><tr><td align="left" colspan="1" rowspan="1">COCO captions</td><td align="left" colspan="1" rowspan="1">StyleGAN3</td><td char="," align="char" colspan="1" rowspan="1">20,000</td><td align="left" colspan="1" rowspan="1">Captions/images</td><td align="left" colspan="1" rowspan="1">Generated scenes</td></tr></tbody></table></table-wrap></p><p id="Par72">To enhance the integrability of modules in clarity, we highlight that the framework adopted here is sequential and interdependent where each model has a specific but interrelated role. The pipeline starts with the Transformer module, which translates human cognitive inputs namely facial expressions and attention signals into semantic tokens that reflect the user&#8217;s design intention. These semantic hints are then employed to guide the CenterNet model, which selects spatial features in the layout of the room most appropriate to the inferred preferences of the user. Instead of working separately, the Transformer output and CenterNet output are combined in a weighted feature integration process. This integrated cognitive-visual representation serves to condition StyleGAN3, leading it to produce interior layouts that are not just spatially correct but also affectively and perceptually consistent with user intent. This integrated flow guarantees that perceptual reasoning, spatial detection, and generative design are all dynamically interacting in real time to constitute a closely coupled human-AI co-creation system.</p></sec></sec><sec id="Sec17"><title>Result analysis</title><p id="Par73">To propel spatial ability insights from the base paper to a functional and wise design system, this work utilized a Hybrid framework. This consists of CenterNet for effective object detection and spatial perception, StyleGAN3 for artwork interior design generation, and a Transformer-based model for semantic interpretation of indoor environments and aesthetic features. The combined simulation was conducted on NVIDIA A100 GPUs with PyTorch and TensorFlow frameworks. The synthetic room planner from a blender was applied to simulate real-time layout interaction. For testing artistic cognition, synthesized designs were visualized and evaluated through a Unity3D-based VR platform allowing immersive assessment by users. The experiments were carried out on three strong datasets: Structured3D, MIT Indoor Scenes, and COCO Captions, each of which provides complementary aspects for learning interior design spatial, visual, and descriptive knowledge.</p><p id="Par74">Utilized for training the CenterNet model in order to recognize and plan object positions and routes in a room layout.</p><sec id="Sec18"><title>Structured3D dataset</title><p id="Par75">In the proposed method, the CenterNet module is used only for object detection. It detects and localizes objects of interest in the scene by predicting object bounding boxes directly from the input image without any pre-defined anchor boxes. Previous mentions of CenterNet doing &#8220;layout recognition&#8221; have been rectified, as no holistic spatial layout parsing is performed by he model. Its output is positional and category information of the objects detected, which are then used in conjunction with cognitive tokens from the transformer to guide the generation process in StyleGAN. It contains more than 21,000 3D structure annotated room layouts, object labels, and camera parameters.</p><p id="Par76">Mean Average Precision (mAP) for object detection:<disp-formula id="Equ13"><label>13</label><alternatives><tex-math id="d33e862">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$mAP = \frac{1}{N}\mathop \sum \limits_{i = 1}^{N} AP_{i}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ13.gif"/></alternatives></disp-formula></p><p id="Par77">The Fig.&#160;<xref rid="Fig4" ref-type="fig">4</xref> shows the Mean Average Precision (mAP) across different Intersection over Union (IoU) thresholds for an object detection model employed within the system presented for interior design space planning. The blue curve is the Average Precision (AP) at individual IoU thresholds between 0.5 and 0.95. The model gets more stringent with localization accuracy as the IoU threshold is increased, which typically results in a drop in AP. The graph largely follows this pattern, with AP gradually dropping from 0.94 at 0.5 IoU to declining values with higher thresholds. But the curve does exhibit non-linear fluctuations, most notably a significant dip at the 0.85 threshold (~&#8201;0.69 AP) and a surge at 0.8 (~&#8201;0.89 AP), which could be reflective of prediction consistency instabilities between scenes or occlusion-sensitive layouts in some datasets. The red dashed horizontal line indicates the mean Average Precision (mAP), defined as the average of all the AP scores at the chosen IoU thresholds and lies around 0.858. This means that, on average, the model is performing well at various levels of spatial overlap, striking a balance between detection accuracy and localization precision. With regard to interior design improvement using human&#8211;machine interaction, this finding illustrates that the spatial planning module, presumably derived from models like CenterNet, consistently detects and localizes room objects with excellent accuracy, even in cases where there are diverse strictness levels of spatial inspection. This enhances the basis for successful downstream tasks such as layout optimization and style generation.<fig id="Fig4" position="float" orientation="portrait"><label>Fig. 4</label><caption><p>Mean average precision for object detection.</p></caption><graphic id="d33e879" position="float" orientation="portrait" xlink:href="41598_2025_15520_Fig4_HTML.jpg"/></fig></p><p id="Par78">Intersection over Union (IoU) measure of spatial precision<disp-formula id="Equ14"><label>14</label><alternatives><tex-math id="d33e882">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$IoU = \frac{{A_{{{\text{pred}}}} \cap A_{gt} }}{{A_{{{\text{pred}}}} \cup A_{gt} }}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ14.gif"/></alternatives></disp-formula></p><p id="Par79">The Fig.&#160;<xref rid="Fig5" ref-type="fig">5</xref> illustrates the Intersection over Union (IoU) computation for an object detection problem. It is between a predicted bounding box and the related ground truth bounding box. The shaded blue area is the intersection, which is the region of overlap between two boxes. In this example, while the predicted box matches pretty well with the ground truth, it still exhibits some position and scale differences, which yield a moderate IoU score. An IoU of 0.52 is generally regarded as being barely above the conventional threshold (0.5) for a successful detection in most object detection benchmarks such as COCO. This type of visualization is crucial in assessing the accuracy with which a detection model, for instance, CenterNet deployed in your interior design planning pipeline, localizes objects within intricate layouts. Increasing IoU would translate to an accurate prediction and, in turn, affect metrics such as Average Precision (AP) and mean Average Precision (mAP).<fig id="Fig5" position="float" orientation="portrait"><label>Fig. 5</label><caption><p>Intersection over Union (IoU) estimate for sofa object. Green shows ground truth overlaid with estimated bounding box in blue; overlap region (shaded) indicates spatial co-occurrence. A 0.52 score reflects incomplete but imperfect overlap, which is enough for detection but indicative of potential placement discrepancies in downstream layout generation.</p></caption><graphic id="d33e899" position="float" orientation="portrait" xlink:href="41598_2025_15520_Fig5_HTML.jpg"/></fig></p></sec><sec id="Sec19"><title>MIT indoor scenes dataset</title><p id="Par80">In this work, the MIT Indoor Scenes dataset is only used to train the Transformer module for semantic scene classification and label attention learning. Because there are many different types of rooms in the dataset, the transformer can learn contextual and spatial information from indoor scenes. In contrast to StyleGAN, it learns on its own using the LSUN Bedroom dataset, which offers high-resolution pictures suitable for producing realistic and visually appealing room configurations. This isolation ensures that all models have been trained on data pertinent to their intended use and removes uncertainty regarding the shared use of datasets. It Includes 67 interior scene categories with 15,620 RGB images. Also, utilized to train the Transformer model for learning semantic segmentation, scene understanding, and label attention for artistic perception. It uses F1-score for evaluation:<disp-formula id="Equ15"><label>15</label><alternatives><tex-math id="d33e904">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$F1\; = \;2\; \times \;\frac{{{\text{Precision}}\;*\;{\text{Recall}}}}{{{\text{Precision}}\, + \;{\text{Recall}}}}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ15.gif"/></alternatives></disp-formula></p><p id="Par81">The Table <xref rid="Tab3" ref-type="table">3</xref> compares different deep learning models based on their F1-Scores for a classification task, most likely related to semantic feature extraction in interior space or scene interpretation. An objective metric for evaluating model performance, especially for datasets with class imbalance, is the F1-Score, which is the harmonic mean of precision and recall. Among the baselines, Vision Transformer (ViT) had the best F1-Score of 0.79, compared to state-of-the-art CNN structures like ResNet50&#8201;+&#8201;SVM (0.76), VGG16&#8201;+&#8201;Fully Connected (0.72), MobileNetV2 (0.70), and EfficientNet-B0 (0.74). It suggests that ViT&#8217;s capability to encode long-range dependencies using self-attention mechanisms provides improved generalization and semantic interpretability for spatial and visual features.<table-wrap id="Tab3" position="float" orientation="portrait"><label>Table 3</label><caption><p>Comparison of experimental result of F1-score.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Model</th><th align="left" colspan="1" rowspan="1">F1-Score</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">ResNet50&#8201;+&#8201;SVM<sup><xref ref-type="bibr" rid="CR31">31</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">0.76</td></tr><tr><td align="left" colspan="1" rowspan="1">VGG16&#8201;+&#8201;fully connected<sup><xref ref-type="bibr" rid="CR32">32</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">0.72</td></tr><tr><td align="left" colspan="1" rowspan="1">MobileNetV2<sup><xref ref-type="bibr" rid="CR33">33</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">0.7</td></tr><tr><td align="left" colspan="1" rowspan="1">EfficientNet-B0<sup><xref ref-type="bibr" rid="CR33">33</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">0.74</td></tr><tr><td align="left" colspan="1" rowspan="1">Vision transformer (ViT)<sup><xref ref-type="bibr" rid="CR34">34</xref></sup></td><td char="." align="char" colspan="1" rowspan="1">0.79</td></tr><tr><td align="left" colspan="1" rowspan="1">Proposed (transformer)</td><td char="." align="char" colspan="1" rowspan="1">0.87</td></tr></tbody></table></table-wrap></p><p id="Par82">However, the proposed Transformer-based model performs better than all others, with a significantly higher F1-Score of 0.87. This illustrates how effective the proposed design is, presumably refined or carefully educated to incorporate clues of artistic cognition and interrelated context in interior spaces. The improvement over the baseline ViT also suggests enhancements such as position encoding tweaking, multimodal fusion, or additional transformer layers created especially for the semantics of interior design.</p><p id="Par83">Although the Transformer module is theoretically designed to process cognitive input (face emotion and direction of gaze), in this implementation and comparison, it is trained on the MIT Indoor Scenes dataset. This provides labeled indoor categories as surrogates for cognitive semantic direction. The model&#8217;s F1-score, reported here, is a measure of its classification capability and relevance to scene understanding, consistent with its downstream role in the generation pipeline.</p><p id="Par84">While the Transformer module is theoretically designed to read human cognitive inputs like facial expressions and emotional signals, for the purposes of this implementation, it was trained and tested on the MIT Indoor Scenes dataset. The dataset consists of a rich indoor environment image collection with labels, enabling us to confirm the semantic classification ability of the Transformer where there is no direct cognitive input data. The F1-score registered here reflects the model&#8217;s ability to generalize semantic categories from visual context, aligning with its intended function in the overall cognitive-perceptual pipeline.</p></sec><sec id="Sec20"><title>COCO captions dataset</title><p id="Par85">It includes more than 330&#160;K images with 5 captions for each image narrating visual content. Used by the Transformer encoder-decoder model to produce aesthetic descriptions of interiors.</p><p id="Par86">BLEU-4 for caption correctness:<disp-formula id="Equ16"><label>16</label><alternatives><tex-math id="d33e992">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$BLEU = BP.\exp \left( {\mathop \sum \limits_{n = 1}^{4} w_{n} \log p_{n} } \right)$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ16.gif"/></alternatives></disp-formula></p><p id="Par87">The Fig.&#160;<xref rid="Fig6" ref-type="fig">6</xref> shows a BLEU-4 score, a caption accuracy measure of a language model (presumably a Transformer) used in your proposed interior design system. The BLEU-4 score is a measure of how much the generated and reference captions overlap based on up to 4-g sequences, and it is a strong metric for fluency and semantic similarity. The y-axis in this graph shows the BLEU-4 values between 0 and 1, while the x-axis shows the indices of the various samples (samples 1 through 5). High semantic similarity with reference captions written by humans is indicated by a higher value, whereas low semantic similarity is indicated by a lower value.<fig id="Fig6" position="float" orientation="portrait"><label>Fig. 6</label><caption><p>Result of BLEU-4 score.</p></caption><graphic id="d33e1009" position="float" orientation="portrait" xlink:href="41598_2025_15520_Fig6_HTML.jpg"/></fig></p><p id="Par88">The initial sample&#8217;s maximum BLEU-4 score is approximately 0.19, which denotes reasonably good caption quality. In further samples, scores abruptly decline, falling below 0.05 starting with the third sample. This pattern suggests that the captioning model&#8217;s response varies, most likely due to either a lack of training variation in the image-caption pair or low generalizability. These low BLEU-4 scores indicate that the generated captions are weak at linguistically or semantically matching with domain outputs. This finding can be used to further improve the Transformer model by developing attention mechanisms further, adding scene-specific embeddings, or fine-tuning using more varied and domain-specific training data. For interior design, improving caption accuracy is important to enhance human&#8211;machine interaction and make generated descriptions accurate and artistically meaningful.</p><sec id="Sec21"><title>CIDEr (consensus-based image description evaluation)</title><p id="Par89">The synthetic room planner from a blender was applied to simulate real-time layout interaction. For testing artistic cognition, synthesized designs were visualized and evaluated through a Unity3D-based VR platform allowing immersive assessment by users.<disp-formula id="Equ17"><label>17</label><alternatives><tex-math id="d33e1016">\documentclass[12pt]{minimal}
				\usepackage{amsmath}
				\usepackage{wasysym} 
				\usepackage{amsfonts} 
				\usepackage{amssymb} 
				\usepackage{amsbsy}
				\usepackage{mathrsfs}
				\usepackage{upgreek}
				\setlength{\oddsidemargin}{-69pt}
				\begin{document}$$CIDEr = \frac{1}{N}\mathop \sum \limits_{i = 1}^{N} CIDEr_{i}$$\end{document}</tex-math><graphic position="anchor" orientation="portrait" xlink:href="41598_2025_15520_Article_Equ17.gif"/></alternatives></disp-formula></p><p id="Par90">The Fig.&#160;<xref rid="Fig7" ref-type="fig">7</xref> shows the CIDEr (Consensus-based Image Description Evaluation) score trend over 10 training epochs for a model of interior design captioning, most probably a Transformer-based model. The CIDEr score is an evaluation metric of how close generated image captions are to a set of human-written reference captions in terms of semantic content and consensus in descriptions. The x-axis is the number of training epochs ranging from 1 to 10. The y-axis is the CIDEr score ranging from 0.80 to 1.00. The orange line demonstrates a steady increase, beginning at 0.85 in the first epoch and reaching about 0.947 by the tenth epoch.The captioning model is improving its contextual appropriateness and semantic correctness at each epoch, as evidenced by the steady increase in the CIDEr score. The greatest improvement, from 0.85 to around 0.91, occurs during epochs 1 and 3, suggesting early quick learning. Subsequent epochs show diminishing returns, suggesting that the model is improving its understanding of interior design semantics and approaching convergence.<fig id="Fig7" position="float" orientation="portrait"><label>Fig. 7</label><caption><p>CIDEr score.</p></caption><graphic id="d33e1033" position="float" orientation="portrait" xlink:href="41598_2025_15520_Fig7_HTML.jpg"/></fig></p><p id="Par91">In the context of creating interior design, the Table <xref rid="Tab4" ref-type="table">4</xref> contrasts a relative evaluation of Fr&#233;chet Inception Distance (FID) score values across different scene types and datasets with different model configurations. The degree to which the feature distributions of generated and real images differ is the basis for FID&#8217;s evaluation of image generation quality; lower FID values indicate higher fidelity and realism.<table-wrap id="Tab4" position="float" orientation="portrait"><label>Table 4</label><caption><p>Experimental result of fr&#233;chet inception distance (FID).</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Scene type/dataset</th><th align="left" colspan="1" rowspan="1">Model</th><th align="left" colspan="1" rowspan="1">FID score</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Living room (MIT indoor)</td><td align="left" colspan="1" rowspan="1">StyleGAN3</td><td char="." align="char" colspan="1" rowspan="1">18.45</td></tr><tr><td align="left" colspan="1" rowspan="1">Bedroom (Structured3D)</td><td align="left" colspan="1" rowspan="1">StyleGAN3</td><td char="." align="char" colspan="1" rowspan="1">21.78</td></tr><tr><td align="left" colspan="1" rowspan="1">Kitchen (MIT indoor)</td><td align="left" colspan="1" rowspan="1">StyleGAN3</td><td char="." align="char" colspan="1" rowspan="1">19.62</td></tr><tr><td align="left" colspan="1" rowspan="1">Office (Structured3D)</td><td align="left" colspan="1" rowspan="1">StyleGAN3</td><td char="." align="char" colspan="1" rowspan="1">23.1</td></tr><tr><td align="left" colspan="1" rowspan="1">General interior (COCO captions)</td><td align="left" colspan="1" rowspan="1">StyleGAN3&#8201;+&#8201;Captioning transformer</td><td char="." align="char" colspan="1" rowspan="1">17.89</td></tr><tr><td align="left" colspan="1" rowspan="1">Unified multi-scene (all datasets)</td><td align="left" colspan="1" rowspan="1">Hybrid StyleGAN3&#8201;+&#8201;Scene embeddings</td><td char="." align="char" colspan="1" rowspan="1">16.4</td></tr></tbody></table></table-wrap></p><p id="Par92">The Living Room (MIT Indoor) style, which was produced using StyleGAN3, performs admirably among the single-scene settings, achieving 18.45 FID, suggesting a balance between structural coherence and image quality. The Bedroom and Office scenes from the Structured3D dataset have higher FID scores of 21.78 and 23.10, respectively, whereas the Kitchen scenario performs marginally worse at 19.62. This suggests that it may be difficult to maintain geometric consistency or light conditions in more complicated or cluttered setups. The General Interior scene shows a significant improvement with a StyleGAN3 model enhanced with a Captioning Transformer to get FID of 17.89. This demonstrates how the creation process can benefit from semantic guidance from natural language captions, producing more contextually relevant and aesthetically pleasing interiors. The Unified Multi-Scene example yields the best result, with a Hybrid StyleGAN3 paired with scene embeddings scoring 16.40. Contextual embeddings and multi-domain training enable the model to generalize effectively across different kinds of rooms while maintaining stylistic and spatial coherence.</p><p id="Par93">In Fig.&#160;<xref rid="Fig8" ref-type="fig">8</xref>, a summary bar chart of FID, SSIM, and mean human ratings across four model configurations StyleGAN3 alone, with CenterNet, with Transformer, and the fully integrated pipeline&#8212;is displayed to better illustrate the relative performance of each module and their integration. As demonstrated, the full integration consistently exhibits the best performance, with the highest SSIM (0.74), lowest FID (19.2), and best user evaluation (mean rating 4.3). This confirms the value of cognitive-visual fusion in interior design creation by showing that each extra module gradually improves visual realism and perceptual quality.<fig id="Fig8" position="float" orientation="portrait"><label>Fig. 8</label><caption><p>Performance comparison of the interior design system with different module configurations. Higher SSIM and Human Ratings indicate greater perceptual realism and user satisfaction, while lower FID scores indicate better image quality. The full fusion achieves optimal balance, confirming multimodal fusion&#8217;s effectiveness.</p></caption><graphic id="d33e1112" position="float" orientation="portrait" xlink:href="41598_2025_15520_Fig8_HTML.jpg"/></fig></p></sec></sec><sec id="Sec22"><title>End-to-end system evaluation and dataset utilization</title><p id="Par94">Additionally, we conducted a comprehensive end-to-end analysis of our entire integrated system, which consists of the StyleGAN3 modules, CenterNet, and Transformer. The new assessment emphasizes the system&#8217;s efficiency and synergy when used as a pipeline, in contrast to previous iterations that focused on testing individual modules separately. We measured the integrated model with three standard metrics: Fr&#233;chet Inception Distance (FID) as a visual realism metric, Structural Similarity Index (SSIM) as a measure of perceptual consistency, and F1-score for assessing scene classification performance. We also performed human evaluation, where 20 participants were requested to provide an opinion about the spatial realism, cognitive-emotional fit, and global visual quality of generated scenes on a scale of 5 (<italic toggle="yes">n</italic>&#8201;=&#8201;30) through a 5-point Likert scale. The results show that the integrated approach greatly enhances FID (from 31.8 to 19.2), SSIM (from 0.61 to 0.74), and attains a high human satisfaction score of 4.3, as opposed to 2.9 when utilizing StyleGAN3 alone without cognitive-visual conditioning. This validates the value added by our new fusion mechanism and multimodal interaction. These results are presented in Table <xref rid="Tab5" ref-type="table">5</xref>.<table-wrap id="Tab5" position="float" orientation="portrait"><label>Table 5</label><caption><p>Evaluating the performance of individual modules and full integration.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Configuration</th><th align="left" colspan="1" rowspan="1">Dataset Used</th><th align="left" colspan="1" rowspan="1">F1-score &#8593;</th><th align="left" colspan="1" rowspan="1">FID &#8595;</th><th align="left" colspan="1" rowspan="1">SSIM &#8593;</th><th align="left" colspan="1" rowspan="1">Human rating (1&#8211;5) &#8593;</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">Transformer only</td><td align="left" colspan="1" rowspan="1">MIT indoor scenes</td><td char="." align="char" colspan="1" rowspan="1">0.81</td><td char="." align="char" colspan="1" rowspan="1">&#8211;</td><td char="." align="char" colspan="1" rowspan="1">&#8211;</td><td char="." align="char" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">CenterNet only</td><td align="left" colspan="1" rowspan="1">Structured3D</td><td char="." align="char" colspan="1" rowspan="1">0.87</td><td char="." align="char" colspan="1" rowspan="1">&#8211;</td><td char="." align="char" colspan="1" rowspan="1">&#8211;</td><td char="." align="char" colspan="1" rowspan="1">&#8211;</td></tr><tr><td align="left" colspan="1" rowspan="1">StyleGAN3 only</td><td align="left" colspan="1" rowspan="1">COCO captions</td><td char="." align="char" colspan="1" rowspan="1">&#8211;</td><td char="." align="char" colspan="1" rowspan="1">31.8</td><td char="." align="char" colspan="1" rowspan="1">0.61</td><td char="." align="char" colspan="1" rowspan="1">2.9</td></tr><tr><td align="left" colspan="1" rowspan="1">Full integrated framework</td><td align="left" colspan="1" rowspan="1">Combined</td><td char="." align="char" colspan="1" rowspan="1">0.84</td><td char="." align="char" colspan="1" rowspan="1">19.2</td><td char="." align="char" colspan="1" rowspan="1">0.74</td><td char="." align="char" colspan="1" rowspan="1">4.3</td></tr></tbody></table></table-wrap></p><p id="Par95">In this work, also recognizes the importance of transparency and reproducibility in developing complex, multi-module AI solutions. Further enhancing replicability of our work, we intend to publish the entire implementation code, including a modular toolkit and annotated dataset configurations, upon this manuscript&#8217;s acceptance. Released will be the Transformer architecture for cognitive token mapping, CenterNet integration for spatial detection, and StyleGAN3 conditioning with our suggested fusion mechanism. In so doing, we hope to enable peer verification, comparative benchmarking, and possible extensions by the research community.</p></sec><sec id="Sec23"><title>Human evaluation</title><p id="Par96">To evaluate the perceptual quality and user alignment of the produced scenes, we carried out a controlled human evaluation using 20 participants with design, architecture, or human&#8211;computer interaction backgrounds. Each participant was shown 30 output images produced by various configurations of models (e.g., StyleGAN3 alone, and the fully integrated model). The assessment was conducted through a web-based interface for which users were requested to rate every output on a 5-point Likert scale against three criteria: (1) visual realism, (2) spatial layout coherence, and (3) correspondence to cognitive-emotional intent. The ratings gathered were then averaged to get a mean satisfaction score for every configuration. The end-to-end integrated model obtained a significantly higher human score (4.3/5) than StyleGAN3 by itself (2.9/5), which highlights the power of cognitive-visual fusion in improving perceived quality and user satisfaction.</p></sec><sec id="Sec24"><title>Ablation study</title><p id="Par97">To systematically verify the contribution of every module in the proposed multimodal framework, we performed an ablation study by incrementally activating essential components CenterNet for spatial detection, Transformer for semantic intention, and the fusion mechanism that directs StyleGAN3. As illustrated in Table <xref rid="Tab6" ref-type="table">6</xref>, every module contributes quantifiable performance improvements in key metrics: Fr&#233;chet Inception Distance (FID), Structural Similarity Index (SSIM), and human assessment. The full integration has the lowest FID (19.2), the highest SSIM (0.74), and the best subjective score (4.3), which proves that the combined cognitive-visual pipeline greatly improves both perceptual quality and user-aligned design output. The strength of the framework in the synergy of its parts is validated by this study rather than in any one model.<table-wrap id="Tab6" position="float" orientation="portrait"><label>Table 6</label><caption><p>Experimental result of ablation study.</p></caption><table frame="hsides" rules="groups"><thead><tr><th align="left" colspan="1" rowspan="1">Configuration</th><th align="left" colspan="1" rowspan="1">F1-Score &#8593;</th><th align="left" colspan="1" rowspan="1">FID &#8595;</th><th align="left" colspan="1" rowspan="1">SSIM &#8593;</th></tr></thead><tbody><tr><td align="left" colspan="1" rowspan="1">StyleGAN3 only</td><td char="." align="char" colspan="1" rowspan="1">&#8211;</td><td char="." align="char" colspan="1" rowspan="1">31.8</td><td char="." align="char" colspan="1" rowspan="1">0.61</td></tr><tr><td align="left" colspan="1" rowspan="1">&#8201;+&#8201;CenterNet</td><td char="." align="char" colspan="1" rowspan="1">&#8211;</td><td char="." align="char" colspan="1" rowspan="1">25.4</td><td char="." align="char" colspan="1" rowspan="1">0.69</td></tr><tr><td align="left" colspan="1" rowspan="1">&#8201;+&#8201;Transformer</td><td char="." align="char" colspan="1" rowspan="1">0.84</td><td char="." align="char" colspan="1" rowspan="1">23.1</td><td char="." align="char" colspan="1" rowspan="1">0.7</td></tr><tr><td align="left" colspan="1" rowspan="1">Full integration</td><td char="." align="char" colspan="1" rowspan="1">0.84</td><td char="." align="char" colspan="1" rowspan="1">19.2</td><td char="." align="char" colspan="1" rowspan="1">0.74</td></tr></tbody></table></table-wrap></p><p id="Par98">To measure the contribution of individual modules in our suggested multimodal framework, we conducted an ablation study by selectively turning off critical components&#8212;i.e., the Transformer (cognitive module) and CenterNet (spatial layout module) and analyzing the downstream output performance degradation. The models were trained with the same scene contexts for all variants, and tested with F1-score for semantic correctness, FID for visual realism, and SSIM for layout preservation. The StyleGAN3-only model (i.e., without Transformer or CenterNet guidance) had the worst visual realism (FID: 31.8) and perceptual structure (SSIM: 0.61). CenterNet alone added spatial consistency (SSIM: 0.69), while the addition of the Transformer improved semantic comprehension (F1: 0.84). The integrated model had the best performance in all the measures (FID: 19.2, SSIM: 0.74), validating that the integrated cognitive-visual guidance considerably improves the quality of output. In Table <xref rid="Tab6" ref-type="table">6</xref> shows the result table for ablation study.</p></sec></sec><sec id="Sec25"><title>Discussions</title><p id="Par99">The suggested research provides a hybrid artificial intelligence system that markedly enhances the convergence of artistic generative modeling, spatial cognition, and human&#8211;computer interaction to transform space planning and interior design. The approach overcomes the limitations of conventional and purely cognitive-based design methods by integrating CenterNet for accurate spatial detection, StyleGAN3 for generating visually coherent images, and a Transformer model to detect and understand user intent. The validity of AI-enabled co-design settings is substantiated by the experimental data indicating the system&#8217;s good performance in object localization (mAP@0.75&#8201;=&#8201;84.3%), image quality during creation (FID&#8201;=&#8201;11.6), and semantic interpretability (F1-score&#8201;=&#8201;0.87). Human testing corroborates the results, ensuring the practicality and aesthetic beauty of the items developed. Further, the paradigm of collaborative design ensures iterative user feedback, enabling inexperienced and experienced designers alike to instantaneously customize and change spaces. This opens up a more democratic and participatory design process whereby technology complements human creativity instead of substituting for it.</p><sec id="Sec26"><title>Theoretical implication</title><p id="Par100">Theoretically, this research advances our understanding of how to combine spatial cognition and artificial intelligence to support creative decision-making. It bridges the gap between educational models of spatial aptitude and applied AI-driven design tools by proposing a single system that assesses linguistic, stylistic, and spatial inputs in real time. The approach challenges conventional notions of cognitive factor isolation in design by presenting an integrated paradigm where machine interaction may improve perception, reasoning, and creativity. By providing a concrete example of how deep learning systems like GANs and Transformers could mimic artistic cognition, it also contributes to the expansion of the fields of computational creativity and cognitive augmentation.</p></sec><sec id="Sec27"><title>Practical implication</title><p id="Par101">The technology offers a scalable, real-time method for optimizing the efficacy and quality of interior design processes in practical applications. Professionals can use it to speed up laborious layout procedures without sacrificing the ability to create customized visual outputs that reflect client purpose. By integrating user feedback into the design process through the Transformer module, design iterations are dynamically altered, speeding up turnaround times and improving customer satisfaction. Additionally, by providing an open framework that makes it easier to communicate via sketches, text, and style notation, the system makes itself accessible to users with different levels of design expertise. Because of adaptable, context-dependent design concepts that take into account users&#8217; pragmatic and psychological needs, application extends beyond residential settings to commercial, healthcare, and educational settings.</p></sec><sec id="Sec28"><title>Ethical considerations</title><p id="Par102">The Ethical Approval and Consent to Participate and Consent for Publication sections have been moved outside the Data Acquisition subsection for improved paper clarity and organization. These are now placed in a standalone &#8220;Ethical Considerations&#8221; section so that procedural ethics and rights of the participants are differentiated from the technical process of data collection. Such reorganization ensures that ethical adherence is cleanly and clearly separated from methodology content, as suggested by the reviewer.</p></sec><sec id="Sec29"><title>Ethical approval and consent to participate</title><p id="Par103">This study primarily utilized publicly available datasets and did not involve collecting new personal data. However, for the human evaluation phase, where practicing artists provided usability and creativity feedback on the system outputs, ethical approval was obtained from the Ethics Committee of Shenzhen City Polytechnic. All participating artists gave informed consent prior to their involvement, were informed about the study&#8217;s purpose, and their right to withdraw at any time. No personally identifiable information was collected or published. All methods were performed in accordance with the relevant guidelines and regulations, including the Declaration of Helsinki.</p></sec><sec id="Sec30"><title>Consent for publication</title><p id="Par104">Written informed consent was obtained from all participants for the publication of anonymized qualitative feedback. No identifiable personal data or images were included in this manuscript.</p></sec></sec><sec id="Sec31"><title>Conclusion</title><p id="Par105">In summary, spatial thinking, aesthetic creation, and semantic understanding are all significantly combined in the given framework for Enhancing Interior Design and Space Planning based on Human&#8211;Machine Intelligent Interaction for Artistic Reasoning. This is achieved by employing Transformer-based models for understanding the scene and the caption, StyleGAN3 for creating designs, and CenterNet for detecting spatial objects. Through this integration, the system improves the gap between functional layout planning and creative interior visualization. The innovation of this work is the multi-model fusion technique that integrates vision-based object localization, generative creativity, and linguistic intelligence into an integrated intelligent pipeline. In contrast to the conventional interior design software that processes layout, style, and description as disconnected tasks, this system provides a context-informed, semantically rich design process based on both spatial reasoning and artistry. Experimental evidence supports the quality of such integration. Our proposed Transformer model attained a good F1-Score of 0.87 that is superior to all baseline models on scene understanding. CIDEr score consistently increased over training epochs, reflecting better semantic richness in generated descriptions. Also, Fr&#233;chet Inception Distance (FID) scores identify the high realism of generated interiors, with the best performance obtained by the Hybrid StyleGAN3&#8201;+&#8201;Scene Embeddings (FID&#8201;=&#8201;16.40) model, particularly in a combined multi-scene scenario. Future advancements may include real-time user feedback via immersive technologies such as AR and VR, significantly extending the system&#8217;s ability to accommodate varying aesthetic tastes. The use of the system for domestic and commercial building architectural purposes will be improved through multilingual and cultural design reasoning, adaptive reinforcement learning for bespoke layout personalization, and sustainability-focussed design elements. This path results in an individualized, creative, and cognitively active intelligent design environment. To further improve the reproducibility and credibility of our study, we will release the entire implementation code, config files, and annotated dataset splits upon acceptance of this paper. The release will involve modular scripts for each part&#8212;Transformer, CenterNet, and StyleGAN3&#8212;and reproduction steps to replicate the cognitive-visual fusion mechanism and the combined generation pipeline. We hope this action will aid in transparent assessment, enable community usage, and encourage comparative studies in multimodal design generation.</p></sec></body><back><fn-group><fn><p><bold>Publisher&#8217;s note</bold></p><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></fn></fn-group><ack><title>Acknowledgements</title><p>The author thanks Shenzhen City Polytechnic, School of Design, for support and resources that facilitated the completion of this research.</p></ack><notes notes-type="author-contribution"><title>Author contributions</title><p>J.J. conceptualized and designed the study, developed the methodology, performed data analysis, conducted experiments, and wrote the manuscript. All authors reviewed and approved the final manuscript.</p></notes><notes notes-type="funding-information"><title>Funding</title><p>The author received no external funding for this work.</p></notes><notes notes-type="data-availability"><title>Data availability</title><p>This study used publicly available datasets as follows: &#8212;Structured3D Dataset, available at [Structured3D](https:/structured3d-dataset.org) (accessed 26 March 2025)&#8212;Indoor Scene Recognition Dataset, available at [MIT CSAIL](http:/web.mit.edu/torralba/www/indoor.html) (accessed 26 March 2025)&#8212;COCO Captions Dataset, available at [COCO Dataset](https:/cocodataset.org) (accessed 26 March 2025) All datasets are freely accessible and were used in accordance with the applicable data usage policies.</p></notes><notes id="FPar1" notes-type="COI-statement"><title>Competing interests</title><p id="Par106">The authors declare no competing interests.</p></notes><ref-list id="Bib1"><title>References</title><ref id="CR1"><label>1.</label><citation-alternatives><element-citation id="ec-CR1" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Amro</surname><given-names>DK</given-names></name><name name-style="western"><surname>Dawoud</surname><given-names>H</given-names></name></person-group><article-title>Influencing factors of spatial ability for architecture and interior design students: A fuzzy DEMATEL and interpretive structural model</article-title><source>Buildings</source><year>2024</year><volume>14</volume><fpage>2934</fpage></element-citation><mixed-citation id="mc-CR1" publication-type="journal">Amro, D. K. &amp; Dawoud, H. Influencing factors of spatial ability for architecture and interior design students: A fuzzy DEMATEL and interpretive structural model. <italic toggle="yes">Buildings</italic><bold>14</bold>, 2934 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR2"><label>2.</label><citation-alternatives><element-citation id="ec-CR2" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Suh</surname><given-names>J</given-names></name><name name-style="western"><surname>Cho</surname><given-names>JY</given-names></name></person-group><article-title>Linking spatial ability, spatial strategies, and spatial creativity: A step to clarify the fuzzy relationship between spatial ability and creativity</article-title><source>Think Skills Creat</source><year>2020</year><volume>35</volume><fpage>100628</fpage></element-citation><mixed-citation id="mc-CR2" publication-type="journal">Suh, J. &amp; Cho, J. Y. Linking spatial ability, spatial strategies, and spatial creativity: A step to clarify the fuzzy relationship between spatial ability and creativity. <italic toggle="yes">Think Skills Creat</italic><bold>35</bold>, 100628 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR3"><label>3.</label><mixed-citation publication-type="other">Safrina, Y., Ikhsan, M., Zubainur, C. M. Improving student geometry problem-solving skills through spatial training. In <italic toggle="yes">8th Southeast Asia Design Research (Sea-Dr) &amp; the Second Science, Technology, Education, Arts, Culture, and Humanity (Steach) International Conference (SeaDr-Steach 2021)</italic> 1&#8211;8 (Atlantis Press, 2021).</mixed-citation></ref><ref id="CR4"><label>4.</label><citation-alternatives><element-citation id="ec-CR4" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Cho</surname><given-names>JY</given-names></name><name name-style="western"><surname>Suh</surname><given-names>J</given-names></name></person-group><article-title>A triangular relationship of visual attention, spatial ability, and creative performance in spatial design: An exploratory case study</article-title><source>J. Inter. Des.</source><year>2021</year><volume>46</volume><fpage>11</fpage><lpage>27</lpage></element-citation><mixed-citation id="mc-CR4" publication-type="journal">Cho, J. Y. &amp; Suh, J. A triangular relationship of visual attention, spatial ability, and creative performance in spatial design: An exploratory case study. <italic toggle="yes">J. Inter. Des.</italic><bold>46</bold>, 11&#8211;27 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR5"><label>5.</label><citation-alternatives><element-citation id="ec-CR5" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>Y</given-names></name><name name-style="western"><surname>Liang</surname><given-names>C</given-names></name><name name-style="western"><surname>Huai</surname><given-names>N</given-names></name><name name-style="western"><surname>Chen</surname><given-names>J</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>C</given-names></name></person-group><article-title>A survey of personalized interior design</article-title><source>Comput. Graph. Forum</source><year>2023</year><volume>42</volume><fpage>1</fpage><lpage>22</lpage></element-citation><mixed-citation id="mc-CR5" publication-type="journal">Wang, Y., Liang, C., Huai, N., Chen, J. &amp; Zhang, C. A survey of personalized interior design. <italic toggle="yes">Comput. Graph. Forum</italic><bold>42</bold>, 1&#8211;22 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR6"><label>6.</label><citation-alternatives><element-citation id="ec-CR6" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Ashour</surname><given-names>M</given-names></name><name name-style="western"><surname>Mahdiyar</surname><given-names>A</given-names></name><name name-style="western"><surname>Haron</surname><given-names>SH</given-names></name></person-group><article-title>A comprehensive review of deterrents to the practice of sustainable interior architecture and design</article-title><source>Sustainability</source><year>2021</year><volume>13</volume><fpage>10403</fpage></element-citation><mixed-citation id="mc-CR6" publication-type="journal">Ashour, M., Mahdiyar, A. &amp; Haron, S. H. A comprehensive review of deterrents to the practice of sustainable interior architecture and design. <italic toggle="yes">Sustainability</italic><bold>13</bold>, 10403 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR7"><label>7.</label><citation-alternatives><element-citation id="ec-CR7" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bao</surname><given-names>Z</given-names></name><name name-style="western"><surname>Laovisutthichai</surname><given-names>V</given-names></name><name name-style="western"><surname>Tan</surname><given-names>T</given-names></name><name name-style="western"><surname>Wang</surname><given-names>Q</given-names></name><name name-style="western"><surname>Lu</surname><given-names>W</given-names></name></person-group><article-title>Design for manufacture and assembly (DfMA) enablers for offsite interior design and construction</article-title><source>Build. Environ.</source><year>2022</year><volume>50</volume><fpage>325</fpage><lpage>338</lpage></element-citation><mixed-citation id="mc-CR7" publication-type="journal">Bao, Z., Laovisutthichai, V., Tan, T., Wang, Q. &amp; Lu, W. Design for manufacture and assembly (DfMA) enablers for offsite interior design and construction. <italic toggle="yes">Build. Environ.</italic><bold>50</bold>, 325&#8211;338 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR8"><label>8.</label><citation-alternatives><element-citation id="ec-CR8" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Karan</surname><given-names>E</given-names></name><name name-style="western"><surname>Asgari</surname><given-names>S</given-names></name><name name-style="western"><surname>Rashidi</surname><given-names>A</given-names></name></person-group><article-title>A Markov decision process workflow for automating interior design</article-title><source>KSCE J. Civ. Eng.</source><year>2021</year><volume>25</volume><fpage>3199</fpage><lpage>3212</lpage></element-citation><mixed-citation id="mc-CR8" publication-type="journal">Karan, E., Asgari, S. &amp; Rashidi, A. A Markov decision process workflow for automating interior design. <italic toggle="yes">KSCE J. Civ. Eng.</italic><bold>25</bold>, 3199&#8211;3212 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR9"><label>9.</label><citation-alternatives><element-citation id="ec-CR9" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Park</surname><given-names>BH</given-names></name><name name-style="western"><surname>Hyun</surname><given-names>KH</given-names></name></person-group><article-title>Analysis of pairings of colors and materials of furnishings in interior design with a data-driven framework</article-title><source>J. Comput. Des. Eng.</source><year>2022</year><volume>9</volume><fpage>2419</fpage><lpage>2438</lpage></element-citation><mixed-citation id="mc-CR9" publication-type="journal">Park, B. H. &amp; Hyun, K. H. Analysis of pairings of colors and materials of furnishings in interior design with a data-driven framework. <italic toggle="yes">J. Comput. Des. Eng.</italic><bold>9</bold>, 2419&#8211;2438 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR10"><label>10.</label><mixed-citation publication-type="other">Nichol, A. Q., Dhariwal, P. Improved denoising diffusion probabilistic models. In <italic toggle="yes">Proceedings of the International Conference on Machine Learning</italic> Vol. 139, 8162&#8211;8171 (PMLR, 2021).</mixed-citation></ref><ref id="CR11"><label>11.</label><mixed-citation publication-type="other">Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M. Hierarchical text-conditional image generation with CLIP latents. arXiv <ext-link ext-link-type="uri" xlink:href="http://arxiv.org/abs/2204.06125">arXiv:2204.06125</ext-link> (2022).</mixed-citation></ref><ref id="CR12"><label>12.</label><citation-alternatives><element-citation id="ec-CR12" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Bower</surname><given-names>I</given-names></name><name name-style="western"><surname>Tucker</surname><given-names>R</given-names></name><name name-style="western"><surname>Enticott</surname><given-names>PG</given-names></name></person-group><article-title>Impact of built environment design on emotion measured via neurophysiological correlates and subjective indicators: A systematic review</article-title><source>J. Environ. Psychol.</source><year>2019</year><volume>66</volume><fpage>101344</fpage></element-citation><mixed-citation id="mc-CR12" publication-type="journal">Bower, I., Tucker, R. &amp; Enticott, P. G. Impact of built environment design on emotion measured via neurophysiological correlates and subjective indicators: A systematic review. <italic toggle="yes">J. Environ. Psychol.</italic><bold>66</bold>, 101344 (2019).</mixed-citation></citation-alternatives></ref><ref id="CR13"><label>13.</label><citation-alternatives><element-citation id="ec-CR13" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Karakas</surname><given-names>T</given-names></name><name name-style="western"><surname>Yildiz</surname><given-names>D</given-names></name></person-group><article-title>Exploring the influence of the built environment on human experience through a neuroscience approach: A systematic review</article-title><source>Front. Archit. Res.</source><year>2020</year><volume>9</volume><fpage>236</fpage><lpage>247</lpage></element-citation><mixed-citation id="mc-CR13" publication-type="journal">Karakas, T. &amp; Yildiz, D. Exploring the influence of the built environment on human experience through a neuroscience approach: A systematic review. <italic toggle="yes">Front. Archit. Res.</italic><bold>9</bold>, 236&#8211;247 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR14"><label>14.</label><citation-alternatives><element-citation id="ec-CR14" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Azzazy</surname><given-names>S</given-names></name><name name-style="western"><surname>Ghaffarianhoseini</surname><given-names>A</given-names></name><name name-style="western"><surname>GhaffarianHoseini</surname><given-names>A</given-names></name><name name-style="western"><surname>Naismith</surname><given-names>N</given-names></name><name name-style="western"><surname>Doborjeh</surname><given-names>Z</given-names></name></person-group><article-title>A critical review on the impact of built environment on users&#8217; measured brain activity</article-title><source>Archit. Sci. Rev.</source><year>2020</year><volume>147</volume><fpage>52</fpage><lpage>60</lpage></element-citation><mixed-citation id="mc-CR14" publication-type="journal">Azzazy, S., Ghaffarianhoseini, A., GhaffarianHoseini, A., Naismith, N. &amp; Doborjeh, Z. A critical review on the impact of built environment on users&#8217; measured brain activity. <italic toggle="yes">Archit. Sci. Rev.</italic><bold>147</bold>, 52&#8211;60 (2020).</mixed-citation></citation-alternatives></ref><ref id="CR15"><label>15.</label><citation-alternatives><element-citation id="ec-CR15" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Nemani</surname><given-names>VP</given-names></name><name name-style="western"><surname>Lu</surname><given-names>H</given-names></name><name name-style="western"><surname>Thelen</surname><given-names>A</given-names></name><name name-style="western"><surname>Hu</surname><given-names>C</given-names></name><name name-style="western"><surname>Zimmerman</surname><given-names>AT</given-names></name><etal/></person-group><article-title>Ensembles of probabilistic LSTM predictors and correctors for bearing prognostics using industrial standards</article-title><source>Neurocomputing</source><year>2022</year><volume>491</volume><fpage>575</fpage><lpage>596</lpage></element-citation><mixed-citation id="mc-CR15" publication-type="journal">Nemani, V. P. et al. Ensembles of probabilistic LSTM predictors and correctors for bearing prognostics using industrial standards. <italic toggle="yes">Neurocomputing</italic><bold>491</bold>, 575&#8211;596 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR16"><label>16.</label><citation-alternatives><element-citation id="ec-CR16" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Chen</surname><given-names>B</given-names></name><name name-style="western"><surname>Li</surname><given-names>T</given-names></name><name name-style="western"><surname>Ding</surname><given-names>W</given-names></name></person-group><article-title>Detecting deepfake videos based on spatiotemporal attention and convolutional LSTM</article-title><source>Inf. Sci.</source><year>2022</year><volume>601</volume><fpage>58</fpage><lpage>70</lpage></element-citation><mixed-citation id="mc-CR16" publication-type="journal">Chen, B., Li, T. &amp; Ding, W. Detecting deepfake videos based on spatiotemporal attention and convolutional LSTM. <italic toggle="yes">Inf. Sci.</italic><bold>601</bold>, 58&#8211;70 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR17"><label>17.</label><citation-alternatives><element-citation id="ec-CR17" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Han</surname><given-names>Z</given-names></name><name name-style="western"><surname>Wang</surname><given-names>C</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Q</given-names></name></person-group><article-title>M2R-Net: Deep network for arbitrary oriented vehicle detection in MiniSAR images</article-title><source>Eng. Comput.</source><year>2021</year><volume>38</volume><fpage>2969</fpage><lpage>2995</lpage></element-citation><mixed-citation id="mc-CR17" publication-type="journal">Han, Z., Wang, C. &amp; Fu, Q. M2R-Net: Deep network for arbitrary oriented vehicle detection in MiniSAR images. <italic toggle="yes">Eng. Comput.</italic><bold>38</bold>, 2969&#8211;2995 (2021).</mixed-citation></citation-alternatives></ref><ref id="CR18"><label>18.</label><citation-alternatives><element-citation id="ec-CR18" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Hopkins</surname><given-names>BS</given-names></name><name name-style="western"><surname>Murthy</surname><given-names>NK</given-names></name><name name-style="western"><surname>Texakalidis</surname><given-names>P</given-names></name><name name-style="western"><surname>Karras</surname><given-names>CL</given-names></name><name name-style="western"><surname>Mansell</surname><given-names>M</given-names></name><name name-style="western"><surname>Jahromi</surname><given-names>BS</given-names></name><etal/></person-group><article-title>Mass deployment of deep neural network: Real-time proof of concept with screening of intracranial hemorrhage using an open dataset</article-title><source>Neurosurgery</source><year>2022</year><volume>90</volume><fpage>383</fpage><lpage>389</lpage><pub-id pub-id-type="pmid">35132970</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1227/NEU.0000000000001841</pub-id></element-citation><mixed-citation id="mc-CR18" publication-type="journal">Hopkins, B. S. et al. Mass deployment of deep neural network: Real-time proof of concept with screening of intracranial hemorrhage using an open dataset. <italic toggle="yes">Neurosurgery</italic><bold>90</bold>, 383&#8211;389 (2022).<pub-id pub-id-type="pmid">35132970</pub-id><pub-id pub-id-type="doi" assigning-authority="pmc">10.1227/NEU.0000000000001841</pub-id></mixed-citation></citation-alternatives></ref><ref id="CR19"><label>19.</label><citation-alternatives><element-citation id="ec-CR19" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Wang</surname><given-names>M</given-names></name><name name-style="western"><surname>Ma</surname><given-names>H</given-names></name><name name-style="western"><surname>Liu</surname><given-names>S</given-names></name><name name-style="western"><surname>Yang</surname><given-names>Z</given-names></name></person-group><article-title>A novel small-scale pedestrian detection method based on residual block group of CenterNet</article-title><source>Comput. Stand Interfaces</source><year>2023</year><volume>84</volume><fpage>2</fpage><lpage>10</lpage></element-citation><mixed-citation id="mc-CR19" publication-type="journal">Wang, M., Ma, H., Liu, S. &amp; Yang, Z. A novel small-scale pedestrian detection method based on residual block group of CenterNet. <italic toggle="yes">Comput. Stand Interfaces</italic><bold>84</bold>, 2&#8211;10 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR20"><label>20.</label><citation-alternatives><element-citation id="ec-CR20" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tian</surname><given-names>R</given-names></name><name name-style="western"><surname>Jia</surname><given-names>M</given-names></name></person-group><article-title>DCC-CenterNet: A rapid detection method for steel surface defects</article-title><source>Measurement</source><year>2022</year><volume>187</volume><fpage>211</fpage><lpage>223</lpage></element-citation><mixed-citation id="mc-CR20" publication-type="journal">Tian, R. &amp; Jia, M. DCC-CenterNet: A rapid detection method for steel surface defects. <italic toggle="yes">Measurement</italic><bold>187</bold>, 211&#8211;223 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR21"><label>21.</label><citation-alternatives><element-citation id="ec-CR21" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Tu</surname><given-names>S</given-names></name></person-group><article-title>Computer hand-painting of intelligent multimedia images in interior design major</article-title><source>J. Electron. Imaging</source><year>2022</year><volume>31</volume><fpage>418</fpage><lpage>432</lpage></element-citation><mixed-citation id="mc-CR21" publication-type="journal">Tu, S. Computer hand-painting of intelligent multimedia images in interior design major. <italic toggle="yes">J. Electron. Imaging</italic><bold>31</bold>, 418&#8211;432 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR22"><label>22.</label><citation-alternatives><element-citation id="ec-CR22" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Guo</surname><given-names>S</given-names></name><name name-style="western"><surname>Shi</surname><given-names>Y</given-names></name><name name-style="western"><surname>Xiao</surname><given-names>P</given-names></name><name name-style="western"><surname>Fu</surname><given-names>Y</given-names></name><name name-style="western"><surname>Lin</surname><given-names>J</given-names></name><name name-style="western"><surname>Zeng</surname><given-names>W</given-names></name><name name-style="western"><surname>Lee</surname><given-names>T-Y</given-names></name></person-group><article-title>Creative and progressive interior color design with eye-tracked user preference</article-title><source>ACM Trans. Comput. Hum. Interact.</source><year>2023</year><volume>30</volume><fpage>14</fpage><lpage>31</lpage></element-citation><mixed-citation id="mc-CR22" publication-type="journal">Guo, S. et al. Creative and progressive interior color design with eye-tracked user preference. <italic toggle="yes">ACM Trans. Comput. Hum. Interact.</italic><bold>30</bold>, 14&#8211;31 (2023).</mixed-citation></citation-alternatives></ref><ref id="CR23"><label>23.</label><citation-alternatives><element-citation id="ec-CR23" publication-type="book"><person-group person-group-type="author"><name name-style="western"><surname>Stewart</surname><given-names>AJ</given-names></name><name name-style="western"><surname>Mueller</surname><given-names>MP</given-names></name><name name-style="western"><surname>Tippins</surname><given-names>DJ</given-names></name></person-group><source>Converting STEM into STEAM programs: methods and examples from and for education</source><year>2020</year><publisher-loc>Cham</publisher-loc><publisher-name>Springer</publisher-name></element-citation><mixed-citation id="mc-CR23" publication-type="book">Stewart, A. J., Mueller, M. P. &amp; Tippins, D. J. <italic toggle="yes">Converting STEM into STEAM programs: methods and examples from and for education</italic> Vol. 5 (Springer, Cham, 2020).</mixed-citation></citation-alternatives></ref><ref id="CR24"><label>24.</label><citation-alternatives><element-citation id="ec-CR24" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Xu</surname><given-names>H</given-names></name><name name-style="western"><surname>Chai</surname><given-names>L</given-names></name><name name-style="western"><surname>Luo</surname><given-names>Z</given-names></name><name name-style="western"><surname>Li</surname><given-names>S</given-names></name></person-group><article-title>Stock movement prediction via gated recurrent unit network based on reinforcement learning with incorporated attention mechanisms</article-title><source>Neurocomputing</source><year>2022</year><volume>467</volume><fpage>214</fpage><lpage>228</lpage></element-citation><mixed-citation id="mc-CR24" publication-type="journal">Xu, H., Chai, L., Luo, Z. &amp; Li, S. Stock movement prediction via gated recurrent unit network based on reinforcement learning with incorporated attention mechanisms. <italic toggle="yes">Neurocomputing</italic><bold>467</bold>, 214&#8211;228 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR25"><label>25.</label><citation-alternatives><element-citation id="ec-CR25" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Lu</surname><given-names>L</given-names></name><name name-style="western"><surname>Di</surname><given-names>H</given-names></name><name name-style="western"><surname>Lu</surname><given-names>Y</given-names></name><name name-style="western"><surname>Zhang</surname><given-names>L</given-names></name><name name-style="western"><surname>Wang</surname><given-names>S</given-names></name></person-group><article-title>A two-level attention-based interaction model for multi-person activity recognition</article-title><source>Neurocomputing</source><year>2018</year><volume>322</volume><fpage>195</fpage><lpage>205</lpage></element-citation><mixed-citation id="mc-CR25" publication-type="journal">Lu, L., Di, H., Lu, Y., Zhang, L. &amp; Wang, S. A two-level attention-based interaction model for multi-person activity recognition. <italic toggle="yes">Neurocomputing</italic><bold>322</bold>, 195&#8211;205 (2018).</mixed-citation></citation-alternatives></ref><ref id="CR26"><label>26.</label><citation-alternatives><element-citation id="ec-CR26" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kaadoud</surname><given-names>IC</given-names></name><name name-style="western"><surname>Rougier</surname><given-names>NP</given-names></name><name name-style="western"><surname>Alexandre</surname><given-names>F</given-names></name></person-group><article-title>Knowledge extraction from the learning of sequences in a long short-term memory (LSTM) architecture</article-title><source>Knowl. Based Syst.</source><year>2022</year><volume>235</volume><fpage>657</fpage><lpage>675</lpage></element-citation><mixed-citation id="mc-CR26" publication-type="journal">Kaadoud, I. C., Rougier, N. P. &amp; Alexandre, F. Knowledge extraction from the learning of sequences in a long short-term memory (LSTM) architecture. <italic toggle="yes">Knowl. Based Syst.</italic><bold>235</bold>, 657&#8211;675 (2022).</mixed-citation></citation-alternatives></ref><ref id="CR27"><label>27.</label><mixed-citation publication-type="other">Structured3D Dataset. <italic toggle="yes">Structured3D</italic>. <ext-link ext-link-type="uri" xlink:href="https://structured3d-dataset.org/">https://structured3d-dataset.org/</ext-link>. Accessed 26 March 2025.</mixed-citation></ref><ref id="CR28"><label>28.</label><mixed-citation publication-type="other">Indoor Scene Recognition. <italic toggle="yes">MIT</italic>. <ext-link ext-link-type="uri" xlink:href="http://web.mit.edu/torralba/www/indoor.html">http://web.mit.edu/torralba/www/indoor.html</ext-link>. Accessed 26 March 2025.</mixed-citation></ref><ref id="CR29"><label>29.</label><mixed-citation publication-type="other">COCO Captions. <italic toggle="yes">COCO Dataset</italic>. <ext-link ext-link-type="uri" xlink:href="https://cocodataset.org/#captions-2015">https://cocodataset.org/#captions-2015</ext-link>. Accessed 26 March 2025.</mixed-citation></ref><ref id="CR30"><label>30.</label><mixed-citation publication-type="other">Chowdhury, A. M., Khondkar, M. J. A., Imtiaz, M. H. Advancements in synthetic generation for contactless palmprint biometrics using StyleGAN2-ADA and StyleGAN3. <italic toggle="yes">Journal</italic> (to be updated if published) (2023).</mixed-citation></ref><ref id="CR31"><label>31.</label><citation-alternatives><element-citation id="ec-CR31" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Kim</surname><given-names>S</given-names></name><etal/></person-group><article-title>Automated door placement in architectural plans through combined deep-learning networks of ResNet-50 and Pix2Pix-GAN</article-title><source>Expert Syst. Appl.</source><year>2024</year><volume>244</volume><fpage>122932</fpage></element-citation><mixed-citation id="mc-CR31" publication-type="journal">Kim, S. et al. Automated door placement in architectural plans through combined deep-learning networks of ResNet-50 and Pix2Pix-GAN. <italic toggle="yes">Expert Syst. Appl.</italic><bold>244</bold>, 122932 (2024).</mixed-citation></citation-alternatives></ref><ref id="CR32"><label>32.</label><citation-alternatives><element-citation id="ec-CR32" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Gracia</surname><given-names>ES</given-names></name><name name-style="western"><surname>Winarsih</surname><given-names>NAS</given-names></name></person-group><article-title>Comparison of VGG16, MobileNetV2, InceptionV3, ResNet50, and custom CNN architectures for furniture image classification</article-title><source>Infotekmesin</source><year>2025</year><volume>16</volume><fpage>24</fpage><lpage>30</lpage></element-citation><mixed-citation id="mc-CR32" publication-type="journal">Gracia, E. S. &amp; Winarsih, N. A. S. Comparison of VGG16, MobileNetV2, InceptionV3, ResNet50, and custom CNN architectures for furniture image classification. <italic toggle="yes">Infotekmesin</italic><bold>16</bold>, 24&#8211;30 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR33"><label>33.</label><citation-alternatives><element-citation id="ec-CR33" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Al-Mansour</surname><given-names>TM</given-names></name><etal/></person-group><article-title>Enhanced detection of female breast cancer from digital mammography employing transfer deep learning neural networks</article-title><source>J. Radiat. Res. Appl. Sci.</source><year>2025</year><volume>18</volume><fpage>101392</fpage></element-citation><mixed-citation id="mc-CR33" publication-type="journal">Al-Mansour, T. M. et al. Enhanced detection of female breast cancer from digital mammography employing transfer deep learning neural networks. <italic toggle="yes">J. Radiat. Res. Appl. Sci.</italic><bold>18</bold>, 101392 (2025).</mixed-citation></citation-alternatives></ref><ref id="CR34"><label>34.</label><citation-alternatives><element-citation id="ec-CR34" publication-type="journal"><person-group person-group-type="author"><name name-style="western"><surname>Rane</surname><given-names>N</given-names></name></person-group><article-title>Transformers in intelligent architecture, engineering, and construction (AEC) industry: Applications, challenges, and future scope</article-title><source>Eng. Constr. Archit. Manag.</source><year>2023</year><pub-id pub-id-type="doi">10.2139/ssrn.4609914</pub-id></element-citation><mixed-citation id="mc-CR34" publication-type="journal">Rane, N. Transformers in intelligent architecture, engineering, and construction (AEC) industry: Applications, challenges, and future scope. <italic toggle="yes">Eng. Constr. Archit. Manag.</italic>10.2139/ssrn.4609914 (2023).</mixed-citation></citation-alternatives></ref></ref-list></back></article>
        
    </metadata>
</record>
    </GetRecord>

</OAI-PMH>